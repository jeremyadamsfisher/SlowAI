{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ce5939-77ef-4fc8-9dfd-6d68ca008190",
   "metadata": {},
   "source": [
    "# Convolutions\n",
    "\n",
    "> Convolutions and convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77db186",
   "metadata": {},
   "source": [
    "Adapted from:\n",
    "\n",
    "- [https://youtu.be/0Hi2r4CaHvk?si=Adfv4DL1LfoNr0nC](https://youtu.be/0Hi2r4CaHvk?si=Adfv4DL1LfoNr0nC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8511f0-53c5-402a-aa71-56fa2041955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b53d5d-1502-4b49-9c3a-c0248826c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import os\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from typing import Mapping\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as T\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch import nn, optim, tensor\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from slowai.datasets import show_image, show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163f42c9-3755-40d6-8a46-2aaf17d03460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd = load_dataset(\"mnist\")\n",
    "trn, tst = dsd[\"train\"], dsd[\"test\"]\n",
    "xb = T.to_tensor(trn[0][\"image\"])[None, ...]\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc075b8-862e-4351-8378-d5019d2fb518",
   "metadata": {},
   "source": [
    "This is a [nice resource](https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d860c-1f5c-41f2-97ff-08e8888b165e",
   "metadata": {},
   "source": [
    "Convolutions encode **position equivariance** like the probability that there is a bird at any particular location in an image. It's implemented as a sliding matrix multiplication, followed by a sum like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4bb342-d854-4240-b984-f216737b2453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel = tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
    "\n",
    "\n",
    "def apply_kernel(row, col, img, kernel):\n",
    "    # Normally, you would need to check if the kernel has\n",
    "    # an odd or even size\n",
    "    width, height = kernel.shape\n",
    "    receptive_field = img[\n",
    "        row - (height // 2) : row + (height // 2) + 1,\n",
    "        col - (width // 2) : col + (width // 2) + 1,\n",
    "    ]\n",
    "    return (receptive_field * kernel).sum()\n",
    "\n",
    "\n",
    "*_, w, h = xb.shape\n",
    "processed = [\n",
    "    [apply_kernel(i, j, xb.squeeze(), kernel) for j in range(1, w - 1)]\n",
    "    for i in range(1, h - 1)\n",
    "]\n",
    "processed = tensor(processed)\n",
    "\n",
    "show_images(\n",
    "    [kernel, xb.squeeze(), processed],\n",
    "    titles=[\"Kernel\", \"Original\", \"Original + Kernel\"],\n",
    "    figsize=(8, 8),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a1303-26bc-4812-a1af-1bd94b09f62e",
   "metadata": {},
   "source": [
    "You can see it is very easy to think of interesting features of images that are just convolutional filters.\n",
    "\n",
    "One clever way of implementing this is the `im2col` algorithm that represents a convolution as a matrix multiplication and take advantage of highly efficient algorithms for matrix multiplications. It does so by unrolling the input matrix matrix such that every receptive field is a contiguous block of values, with a correspondingly unrolled contiguous block of convolutional filter values. The values of the convolutional matrix can change, but they are shared.\n",
    "\n",
    "This has a corresponding function in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5994623e-a469-4062-bea7-5b1c78721d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdilation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Extracts sliding local blocks from a batched input tensor.\n",
       "\n",
       ".. warning::\n",
       "    Currently, only 4-D input tensors (batched image-like tensors) are\n",
       "    supported.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "    More than one element of the unfolded tensor may refer to a single\n",
       "    memory location. As a result, in-place operations (especially ones that\n",
       "    are vectorized) may result in incorrect behavior. If you need to write\n",
       "    to the tensor, please clone it first.\n",
       "\n",
       "\n",
       "See :class:`torch.nn.Unfold` for details\n",
       "\u001b[0;31mFile:\u001b[0m      ~/micromamba/envs/slowai/lib/python3.11/site-packages/torch/nn/functional.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.unfold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0afe2ba8-f3dd-4c0d-b6cc-b7f3035d2bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to add a channel dimension and a batch dimension\n",
    "xb_unfolded = F.unfold(xb, kernel_size=(3, 3)).float()\n",
    "kernel_unfolded = kernel.view(-1).float()\n",
    "\n",
    "# im2col\n",
    "xb_processed = kernel_unfolded @ xb_unfolded\n",
    "\n",
    "# Reshape\n",
    "xb_processed = xb_processed.view(26, 26)\n",
    "show_image(xb_processed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c05e5-3178-407b-9e92-89ba436d452d",
   "metadata": {},
   "source": [
    "This unfold trick is about the same efficiency as the built-in convolution layer. \n",
    "\n",
    "For better performance, we can apply a bunch of convolutions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494392f7-fc00-4828-9a0d-8f253eaca4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag1_edge = tensor([[0, -1, 1], [-1, 1, 0], [1, 0, 0]]).float()\n",
    "diag2_edge = tensor([[1, -1, 0], [0, 1, -1], [0, 0, 1]]).float()\n",
    "left_edge = tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]).float()\n",
    "\n",
    "edge_kernels = torch.stack([left_edge, diag1_edge, diag2_edge])\n",
    "edge_kernels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7fa38cc-7c47-4897-8d5d-ac86725d1741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_features = F.conv2d(xb, edge_kernels[:, None, ...])\n",
    "show_images([batch_features[0, i] for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbc594-6b7b-42d3-bbb3-c665493be34c",
   "metadata": {},
   "source": [
    "The parameters to a convolutional layer are the **stride** and **padding**, which allow us to manipulate the size of the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac4706-d548-4318-bf58-b19777455a3c",
   "metadata": {},
   "source": [
    "## Creating a CNN\n",
    "\n",
    "For classifying digits, we cannot solely use convolutional layers because it gives 10 outputs _per pixel_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398455ca-603d-484e-becc-96697f3228a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh, n_outputs = 30, 10\n",
    "\n",
    "partial_model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 30, kernel_size=3, padding=1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(30, 10, kernel_size=3, padding=1),\n",
    ")\n",
    "\n",
    "partial_model(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c49d49-8c9f-4ce6-9c9a-d59d79f687a9",
   "metadata": {},
   "source": [
    "Instead, we'll add a lot of layers and chew up the extra feature map dimensions until we have a 1x1x10 feature map.\n",
    "\n",
    "We'll add a convenience function to create conv layers with optional activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebca5e56-4d04-402d-8d40-c5a9373280f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def conv(ni, nf, ks=3, stride=2, act=True):\n",
    "    conv_ = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks // 2)\n",
    "    if act:\n",
    "        return nn.Sequential(conv_, nn.ReLU())\n",
    "    else:\n",
    "        return conv_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f33bb4-4973-4c6d-b8f4-460376c34d02",
   "metadata": {},
   "source": [
    "We also need some annoying `device` stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8d341b1-a0cd-43d6-bceb-06bf98959ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def_device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "    if isinstance(x, Mapping):\n",
    "        return {k: v.to(device) for k, v in x.items()}\n",
    "    return type(x)(to_device(o, device) for o in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4ff35-f921-432c-a3d4-54d9c00485f2",
   "metadata": {},
   "source": [
    "Now, define the model...\n",
    "\n",
    "Note here that the default stride is `2`, such that the feature map is downsampled by 2 each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71193195-3fde-40de-ad48-1a086f9c5cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): Conv2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (5): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        conv(1, 4),  # 14x14\n",
    "        conv(4, 8),  # 7x7\n",
    "        conv(8, 16),  # 4x4\n",
    "        conv(16, 16),  # 2x2\n",
    "        conv(16, 10, act=False),  # 1x1\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "    return model.to(def_device)\n",
    "\n",
    "\n",
    "get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfecc72-f4cd-4d76-bccd-5d92ec0356a8",
   "metadata": {},
   "source": [
    "And we can train! üèãÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562754a9-b577-41c1-a08c-c37779047887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def accuracy(y, y_pred):\n",
    "    n, _ = y.shape\n",
    "    return (y.argmax(axis=1) == y_pred).sum() / n\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False):\n",
    "    progress = tqdm if tqdm_ else lambda x: x\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in progress(train_dl):\n",
    "            xb, yb = map(to_device, batch)\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc, count = 0.0, 0.0, 0\n",
    "            for batch in progress(valid_dl):\n",
    "                xb, yb = map(to_device, batch)\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred, yb).item() * n\n",
    "                tot_acc += accuracy(pred, yb).item() * n\n",
    "\n",
    "        print(\n",
    "            f\"{epoch=}, validation loss={tot_loss / count:.3f}, validation accuracy={tot_acc / count:.2f}\"\n",
    "        )\n",
    "    return tot_loss / count, tot_acc / count\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def get_dls_from_dataset_dict(dsd, collate_fn=default_collate, bs=32):\n",
    "    datasets.logging.disable_progress_bar()\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        dls = []\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            dir_ = os.path.join(tdir, split)\n",
    "            dsd[split].save_to_disk(dir_)\n",
    "            ds = load_from_disk(dir_).with_format(\"torch\")\n",
    "            dl = DataLoader(ds, batch_size=bs, collate_fn=collate_fn, num_workers=8)\n",
    "            dls.append(dl)\n",
    "        yield dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e6225bd-02ac-4bf6-9efb-8405789ca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def fashion_collate(examples):\n",
    "    batch = default_collate(examples)\n",
    "    xb = batch[\"image\"][:, None, ...].float() / 255\n",
    "    yb = batch[\"label\"]\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def fashion_mnist(bs=256):\n",
    "    dsd = load_dataset(\"fashion_mnist\")\n",
    "    with get_dls_from_dataset_dict(dsd, collate_fn=fashion_collate, bs=bs) as dls:\n",
    "        yield dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b66fbeae-8346-4b25-8b03-ea373f31eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, validation loss=1.041, validation accuracy=0.62\n",
      "epoch=1, validation loss=0.767, validation accuracy=0.73\n",
      "epoch=2, validation loss=0.623, validation accuracy=0.78\n",
      "epoch=3, validation loss=0.561, validation accuracy=0.80\n",
      "epoch=4, validation loss=0.515, validation accuracy=0.81\n",
      "epoch=5, validation loss=0.483, validation accuracy=0.82\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "with fashion_mnist() as dls:\n",
    "    fit(6, model, F.cross_entropy, optim.SGD(model.parameters(), lr=0.1), *dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493863b-0364-40bd-8026-c6b72ff0bf63",
   "metadata": {},
   "source": [
    "This gives us comparable accuracy to the linear model, which had 39,760 parameters. In contrast..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b01e4ffb-a574-4e60-aaed-868f6b74add1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5274"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdf578-fc7e-40f5-8b11-206bb304a234",
   "metadata": {},
   "source": [
    "More tips:\n",
    "\n",
    "- Chewing up the feature map until we have one logit per dimension is not the only way to summarize the features before a classification/regression head. We can also use a dense layer or global average pooling\n",
    "- The receptive field of a convolution grows through the network, as the convolutions are functions of convolutions. Really nice illustration of that [here](https://www.youtube.com/watch?v=0Hi2r4CaHvk&t=2769s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c474e89c-3740-4d15-81cb-417f54316f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a3c99-8b08-43d7-8de5-dfa4af129ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
