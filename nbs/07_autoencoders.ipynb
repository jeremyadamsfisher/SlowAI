{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ce5939-77ef-4fc8-9dfd-6d68ca008190",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "> Autoencoders\n",
    "\n",
    "Adapted from:\n",
    "- https://youtu.be/0Hi2r4CaHvk?si=GA9KaGAnGOlS_NJO&t=3568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8511f0-53c5-402a-aa71-56fa2041955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b53d5d-1502-4b49-9c3a-c0248826c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from copy import copy\n",
    "from typing import Mapping\n",
    "\n",
    "import fastcore.all as fc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as T\n",
    "from datasets import load_dataset\n",
    "from torch import nn, optim, tensor\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torcheval.metrics import Mean\n",
    "\n",
    "from slowai.convs import conv, def_device, fashion_mnist, to_device\n",
    "from slowai.datasets import show_image, show_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137cd7ca-55e7-4022-87ad-36a39b37e2ef",
   "metadata": {},
   "source": [
    "Autoencoders learn a bottleneck representation that can be \"reversed\" to reconstruct the original image.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:600/1*nqzWupxC60iAH2dYrFT78Q.png)\n",
    "\n",
    "Typically, they are not used on their own but are used to produce compressed representations.\n",
    "\n",
    "We've seen how a convolutional neural network can produce a simple representation of an image: that is, the categorical probability distribution over all the fashion classes. How do reverse this process to reconstruct the original image.\n",
    "\n",
    "Transpose or \"Stride $\\frac{1}{2}$\"  convolutions work, but this notebook focuses on the [nearest neighbor upsampling](https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html). This upsamples the activations from the previous layer and applies a convolutional layer to restore detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0620667-61f1-46f1-9f51-e6054998d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def deconv(c_in, c_out, ks=3, act=True):\n",
    "    layers = [\n",
    "        nn.UpsamplingNearest2d(scale_factor=2),\n",
    "        nn.Conv2d(c_in, c_out, stride=1, kernel_size=ks, padding=ks // 2),\n",
    "    ]\n",
    "    if act:\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e78f0e-f2f5-4703-9e72-40b32dc9d107",
   "metadata": {},
   "source": [
    "We need to modify the `fit` function because the loss function is no longer of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d9445f-ca67-4821-8451-ab3c2dddb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False):\n",
    "    \"\"\"Modified fit function for reconstruction tasks\"\"\"\n",
    "    progress = tqdm if tqdm_ else lambda x: x\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        trn_loss, trn_count = 0.0, 0\n",
    "        for xb, _ in progress(train_dl):\n",
    "            xb = to_device(xb)\n",
    "            loss = loss_func(model(xb), xb)  # ðŸ‘ˆ\n",
    "            bs, *_ = xb.shape\n",
    "            trn_loss += loss.item() * bs\n",
    "            trn_count += bs\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tst_loss, tot_acc, tst_count = 0.0, 0.0, 0\n",
    "            for xb, _ in progress(valid_dl):\n",
    "                xb = to_device(xb)\n",
    "                pred = model(xb)\n",
    "                bs, *_ = xb.shape\n",
    "                tst_count += bs\n",
    "                tst_loss += loss_func(pred, xb).item() * bs\n",
    "\n",
    "        print(\n",
    "            f\"{epoch=}: trn_loss={trn_loss / trn_count:.3f}, tst_loss={tst_loss / tst_count:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8228e4-889a-4959-9c79-21ae465f7db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ZeroPad2d((2, 2, 2, 2))\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "    (1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "    (1): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "    (1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (7): ZeroPad2d((-2, -2, -2, -2))\n",
       "  (8): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model():\n",
    "    # input.shape[:2] == 28x28\n",
    "    return nn.Sequential(\n",
    "        nn.ZeroPad2d(padding=2),  # 32x32\n",
    "        conv(1, 2),  # 16x16\n",
    "        conv(2, 4),  # 8x8\n",
    "        conv(4, 8),  # 4x4\n",
    "        deconv(8, 4),  # 8x8\n",
    "        deconv(4, 2),  # 16x16\n",
    "        deconv(2, 1, act=False),  # 32x32\n",
    "        nn.ZeroPad2d(padding=-2),  # 28x28\n",
    "        nn.Sigmoid(),\n",
    "    ).to(def_device)\n",
    "\n",
    "\n",
    "autoencoder = get_model()\n",
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97a662f-18ca-4e37-afab-567d055b4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fashion_mnist() as (_, tst_dl):\n",
    "    xb, _ = next(iter(tst_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847f7d37-841b-40a7-92de-c2faff28d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xb.shape == autoencoder(xb.to(def_device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a67cf309-ea40-4a84-bcad-6a7e47f8d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: trn_loss=0.058, tst_loss=0.027\n",
      "epoch=1: trn_loss=0.026, tst_loss=0.025\n",
      "epoch=2: trn_loss=0.025, tst_loss=0.024\n",
      "epoch=3: trn_loss=0.024, tst_loss=0.023\n",
      "epoch=4: trn_loss=0.023, tst_loss=0.023\n",
      "epoch=5: trn_loss=0.022, tst_loss=0.023\n",
      "epoch=6: trn_loss=0.022, tst_loss=0.022\n",
      "epoch=7: trn_loss=0.022, tst_loss=0.021\n",
      "epoch=8: trn_loss=0.022, tst_loss=0.021\n",
      "epoch=9: trn_loss=0.021, tst_loss=0.021\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "with fashion_mnist() as dls:\n",
    "    opt = optim.AdamW(model.parameters(), lr=0.01)\n",
    "    fit(10, model, F.mse_loss, opt, *dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "452d1cf1-adae-4a8f-9daf-d5ed8f47edb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model(xb.to(def_device))\n",
    "show_images([xb[0, ...].squeeze(), pred[0, ...].squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260e77c-5450-45c8-b059-6defbac91924",
   "metadata": {},
   "source": [
    "That looks...not great.\n",
    "\n",
    "At this point, Jeremy points out that copying and pasting code leads to bottlenecks in modeling velocity. We need to start to build a framework to:\n",
    "\n",
    "- rapidly try things\n",
    "- figure out where things are broken\n",
    "- fast data loading\n",
    "- transparent CUDA device assignment\n",
    "\n",
    "The fundamental abstraction of such an abstraction is the `Learner`. Here's a simple example that takes the loop in the `convs` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206c90c7-8e20-4ff0-b3b1-7c921cc1d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLearner:\n",
    "    \"\"\"Adapted from: https://github.com/fastai/course22p2/blob/master/nbs/09_learner.ipynb\"\"\"\n",
    "\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.xb, self.yb = to_device(self.batch)\n",
    "        self.preds = self.model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            self.calc_stats()\n",
    "\n",
    "    def calc_stats(self):\n",
    "        acc = (self.preds.argmax(dim=1) == self.yb).float().sum()\n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss * n)\n",
    "        self.ns.append(n)\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for self.num, self.batch in enumerate(dl):\n",
    "            self.one_batch()\n",
    "        n = sum(self.ns)\n",
    "        print(\n",
    "            self.epoch,\n",
    "            self.model.training,\n",
    "            sum(self.losses).item() / n,\n",
    "            sum(self.accs).item() / n,\n",
    "        )\n",
    "\n",
    "    def fit(self, n_epochs):\n",
    "        self.accs, self.losses, self.ns = [], [], []\n",
    "        self.model.to(def_device)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(train=True)\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fe85c-1c4c-4ebc-b75e-06bdd15cf062",
   "metadata": {},
   "source": [
    "This needs to be more sophisticated to incorporate the various behaviors we'll need, starting with metrics. We can use a `Metric` class to encapsulate this logic. (This is nearly identical to `torchmetics`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd8c49c-41b8-4ef9-806c-96274a2529cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.vals, self.ns = [], []\n",
    "\n",
    "    def add(self, inp, targ=None, n=1):\n",
    "        self.last = self.calc(inp, targ)\n",
    "        self.vals.append(self.last)\n",
    "        self.ns.append(n)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        ns = tensor(self.ns)\n",
    "        return (tensor(self.vals) * ns).sum() / ns.sum()\n",
    "\n",
    "    def calc(self, inps, targs):\n",
    "        return inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "909c2d69-e7cd-4ddb-8257-f9cf169d6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "    def calc(self, inps, targs):\n",
    "        return (inps == targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7875ade0-8e8f-4d3b-bd38-a6a93190ebac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy()\n",
    "accuracy.add(torch.tensor([0, 1]), torch.tensor([0, 0]))\n",
    "accuracy.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12f8e140-21d4-4b42-9c0f-81bbe3e68b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "Loss = Metric  # Loss is just the average of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36217e34-197d-41b3-8827-24af7db17322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = Loss()\n",
    "loss.add(torch.tensor([0]))\n",
    "loss.add(torch.tensor([2]))\n",
    "loss.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941cb07-3b10-40c5-bd20-bd80159fca55",
   "metadata": {},
   "source": [
    "To incorporate metrics, we'll need a version of the learner with the same basic structure but with callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000b081f-a59b-46ff-94ae-5257d8010418",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CancelFitException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CancelBatchException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CancelEpochException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Callback:\n",
    "    order = 0\n",
    "\n",
    "\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=lambda x: getattr(x, \"order\")):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None:\n",
    "            method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0d74088-d230-4b82-948f-b02a4559bd27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, \"to\"):\n",
    "            learn.model.to(self.device)\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72093a6d-2fe9-41cd-8c16-6ce15e7c8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        xb, yb = self.batch\n",
    "        self.preds = self.model(xb)\n",
    "        self.loss = self.loss_func(self.preds, yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "\n",
    "        if isinstance(self.dls, (list, tuple)):\n",
    "            trn_dl, tst_dl = self.dls\n",
    "            self.dl = trn_dl if train else tst_dl\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        try:\n",
    "            self.callback(\"before_epoch\")\n",
    "            for self.iter, self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback(\"before_batch\")\n",
    "                    self.one_batch()\n",
    "                    self.callback(\"after_batch\")\n",
    "                except CancelBatchException:\n",
    "                    pass\n",
    "            self.callback(\"after_epoch\")\n",
    "        except CancelEpochException:\n",
    "            pass\n",
    "\n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback(\"before_fit\")\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback(\"after_fit\")\n",
    "        except CancelFitException:\n",
    "            pass\n",
    "\n",
    "    def callback(self, method_nm):\n",
    "        run_cbs(self.cbs, method_nm, self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c526c-35cc-4e08-9505-2b2856e9aa12",
   "metadata": {},
   "source": [
    "Now, we need a metrics callback to consume the `Metric`s objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2abbf243-dc89-436a-81e0-3c81eabf6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCB(Callback):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms:\n",
    "            metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics[\"loss\"] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d):\n",
    "        print(d)\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        learn.metrics = self\n",
    "\n",
    "    def before_epoch(self, learn):\n",
    "        [o.reset() for o in self.all_metrics.values()]\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k: f\"{v.compute():.3f}\" for k, v in self.all_metrics.items()}\n",
    "        log[\"epoch\"] = learn.epoch\n",
    "        log[\"train\"] = \"train\" if learn.model.training else \"eval\"\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        x, y, *_ = learn.batch\n",
    "        x = x.cpu()\n",
    "        y = y.cpu()\n",
    "        for m in self.metrics.values():\n",
    "            m.update(learn.preds.cpu(), y)\n",
    "        self.loss.update(learn.loss.cpu(), weight=len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e0401-430b-4470-87ca-758a798cfe4a",
   "metadata": {},
   "source": [
    "Finally, we subclass the learner for the reconstruction objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db5a5b3f-b71c-48d3-8d9a-09224098e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionLearner(Learner):\n",
    "    def one_batch(self):\n",
    "        xb, _ = self.batch\n",
    "        self.preds = self.model(xb)\n",
    "        self.loss = self.loss_func(self.preds, xb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73d30358-150d-488b-a134-8c7fec5deb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '0.039', 'epoch': 0, 'train': 'train'}\n",
      "{'loss': '0.022', 'epoch': 0, 'train': 'eval'}\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "metrics = MetricsCB()\n",
    "with fashion_mnist() as dls:\n",
    "    learn = ReconstructionLearner(\n",
    "        model,\n",
    "        dls,\n",
    "        F.mse_loss,\n",
    "        lr=0.01,\n",
    "        cbs=[metrics, DeviceCB()],\n",
    "        opt_func=torch.optim.AdamW,\n",
    "    )\n",
    "    learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c474e89c-3740-4d15-81cb-417f54316f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
