{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ce5939-77ef-4fc8-9dfd-6d68ca008190",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "> Autoencoders\n",
    "\n",
    "Adapted from:\n",
    "- https://youtu.be/0Hi2r4CaHvk?si=GA9KaGAnGOlS_NJO&t=3568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8511f0-53c5-402a-aa71-56fa2041955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b53d5d-1502-4b49-9c3a-c0248826c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from slowai.convs import conv, def_device, fashion_mnist, to_device\n",
    "from slowai.datasets import show_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137cd7ca-55e7-4022-87ad-36a39b37e2ef",
   "metadata": {},
   "source": [
    "Autoencoders learn a bottleneck representation that can be \"reversed\" to reconstruct the original image.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:600/1*nqzWupxC60iAH2dYrFT78Q.png)\n",
    "\n",
    "Typically, they are not used on their own but are used to produce compressed representations.\n",
    "\n",
    "We've seen how a convolutional neural network can produce a simple representation of an image: that is, the categorical probability distribution over all the fashion classes. How do reverse this process to reconstruct the original image.\n",
    "\n",
    "Transpose or \"Stride $\\frac{1}{2}$\"  convolutions work, but this notebook focuses on the [nearest neighbor upsampling](https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d.html). This upsamples the activations from the previous layer and applies a convolutional layer to restore detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0620667-61f1-46f1-9f51-e6054998d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def deconv(c_in, c_out, ks=3, act=True):\n",
    "    layers = [\n",
    "        nn.UpsamplingNearest2d(scale_factor=2),\n",
    "        nn.Conv2d(c_in, c_out, stride=1, kernel_size=ks, padding=ks // 2),\n",
    "    ]\n",
    "    if act:\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e78f0e-f2f5-4703-9e72-40b32dc9d107",
   "metadata": {},
   "source": [
    "We need to modify the `fit` function because the loss function is no longer of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d9445f-ca67-4821-8451-ab3c2dddb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False):\n",
    "    \"\"\"Modified fit function for reconstruction tasks\"\"\"\n",
    "    progress = tqdm if tqdm_ else lambda x: x\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        trn_loss, trn_count = 0.0, 0\n",
    "        for xb, _ in progress(train_dl):\n",
    "            xb = to_device(xb)\n",
    "            loss = loss_func(model(xb), xb)  # ðŸ‘ˆ\n",
    "            bs, *_ = xb.shape\n",
    "            trn_loss += loss.item() * bs\n",
    "            trn_count += bs\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tst_loss, tot_acc, tst_count = 0.0, 0.0, 0\n",
    "            for xb, _ in progress(valid_dl):\n",
    "                xb = to_device(xb)\n",
    "                pred = model(xb)\n",
    "                bs, *_ = xb.shape\n",
    "                tst_count += bs\n",
    "                tst_loss += loss_func(pred, xb).item() * bs\n",
    "\n",
    "        print(\n",
    "            f\"{epoch=}: trn_loss={trn_loss / trn_count:.3f}, tst_loss={tst_loss / tst_count:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6012ffec-70ad-40c4-9bcb-24f8b3d8aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_model():\n",
    "    # input.shape[:2] == 28x28\n",
    "    return nn.Sequential(\n",
    "        nn.ZeroPad2d(padding=2),  # 32x32\n",
    "        conv(1, 2),  # 16x16\n",
    "        conv(2, 4),  # 8x8\n",
    "        conv(4, 8),  # 4x4\n",
    "        deconv(8, 4),  # 8x8\n",
    "        deconv(4, 2),  # 16x16\n",
    "        deconv(2, 1, act=False),  # 32x32\n",
    "        nn.ZeroPad2d(padding=-2),  # 28x28\n",
    "        nn.Sigmoid(),\n",
    "    ).to(def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8228e4-889a-4959-9c79-21ae465f7db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ZeroPad2d((2, 2, 2, 2))\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "    (1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "    (1): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "    (1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (7): ZeroPad2d((-2, -2, -2, -2))\n",
       "  (8): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = get_model()\n",
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97a662f-18ca-4e37-afab-567d055b4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fashion_mnist() as (_, tst_dl):\n",
    "    xb, _ = next(iter(tst_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847f7d37-841b-40a7-92de-c2faff28d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xb.shape == autoencoder(xb.to(def_device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a67cf309-ea40-4a84-bcad-6a7e47f8d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: trn_loss=0.052, tst_loss=0.028\n",
      "epoch=1: trn_loss=0.024, tst_loss=0.021\n",
      "epoch=2: trn_loss=0.020, tst_loss=0.019\n",
      "epoch=3: trn_loss=0.019, tst_loss=0.018\n",
      "epoch=4: trn_loss=0.018, tst_loss=0.018\n",
      "epoch=5: trn_loss=0.018, tst_loss=0.018\n",
      "epoch=6: trn_loss=0.018, tst_loss=0.018\n",
      "epoch=7: trn_loss=0.017, tst_loss=0.017\n",
      "epoch=8: trn_loss=0.017, tst_loss=0.018\n",
      "epoch=9: trn_loss=0.017, tst_loss=0.017\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "with fashion_mnist() as dls:\n",
    "    opt = optim.AdamW(model.parameters(), lr=0.01)\n",
    "    fit(10, model, F.mse_loss, opt, *dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "452d1cf1-adae-4a8f-9daf-d5ed8f47edb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model(xb.to(def_device))\n",
    "show_images([xb[0, ...].squeeze(), pred[0, ...].squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260e77c-5450-45c8-b059-6defbac91924",
   "metadata": {},
   "source": [
    "That looks...not great.\n",
    "\n",
    "At this point, Jeremy pauses to go over building a framework to iterate on this problem more quickly. Continued in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c474e89c-3740-4d15-81cb-417f54316f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1598a-4e99-4093-a0cc-43fe3412ed44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
