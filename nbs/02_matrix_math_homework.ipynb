{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5567b491-b2f0-4e21-8d17-4fd377cc48a9",
   "metadata": {},
   "source": [
    "# Matrix multiplications, homework\n",
    "\n",
    "> Implementing negative prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73180f67-7ab1-45e7-964b-55341b381c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp matmul_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b19d4af-b182-4877-a1c6-9ca592922ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
    "from slowai.overview import TORCH_DEVICE, StableDiffusion\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cc938e1-4db5-4a51-8ee8-75bd870bf354",
   "metadata": {},
   "source": [
    "Negative prompts are an extension of the Classifier Free Guidance Module. Recall this is part of the `pred_noise` method of `StableDiffusion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5489d61d-222b-4191-9fbb-466f9968f086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mStableDiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguidance_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Code/SlowAI/slowai/overview.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "StableDiffusion.pred_noise?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1782022c-402d-4763-9170-fe7aea328c98",
   "metadata": {},
   "source": [
    "Let's define a helper method to load StableDiffusion, as in the \"Overview\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ef54a1-5b15-4aa2-b1c8-a5a1ea2d96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stable_diffusion(cls=StableDiffusion) -> StableDiffusion:\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "    # Use a simple noising scheduler for the initial draft\n",
    "    pipe.scheduler = LMSDiscreteScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000,\n",
    "    )\n",
    "    pipe = pipe.to(TORCH_DEVICE)\n",
    "    pipe.enable_attention_slicing()\n",
    "    return cls(\n",
    "        tokenizer=pipe.tokenizer,\n",
    "        text_encoder=pipe.text_encoder,\n",
    "        scheduler=pipe.scheduler,\n",
    "        unet=pipe.unet,\n",
    "        vae=pipe.vae,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4979962-6caa-4967-b723-6ee9450262a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "sd = get_stable_diffusion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "037f5ced-9c34-4e45-9143-5e98d4ed2189",
   "metadata": {},
   "source": [
    "`prompt_embedding` is a tensor four-rank tensor of `batch_size x seq_len x channels`, where the batch size is `2` because its the concatenated unconditional prompt and the conditional prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337bb9e7-af29-44ca-93eb-13191a96d169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.embed_prompt(\"a photo of a giraffe in paris\").shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fec2a403-d1e9-4ef4-b866-2f359421384e",
   "metadata": {},
   "source": [
    "We want to add the negative prompt and run this through the denoising unet at the same time. This should make the batch size into `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34753823-5f75-4ac7-80db-789f1c10aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def embed_prompt(sd, prompt, max_length):\n",
    "    prompt_tokens = sd.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        return sd.text_encoder(\n",
    "            prompt_tokens.input_ids.to(TORCH_DEVICE)\n",
    "        ).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d08ffc4-7063-4486-853e-bc09cfc110c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "class StableDiffusionWithNegativePrompt(StableDiffusion):\n",
    "    def embed_prompt(self, prompt, negative_prompt):\n",
    "        orig_embedding = super().embed_prompt(prompt)\n",
    "        _, max_length, _ = orig_embedding.shape\n",
    "        neg_text_embeddings = embed_prompt(self, negative_prompt, max_length)\n",
    "        return torch.cat([orig_embedding, neg_text_embeddings])\n",
    "\n",
    "\n",
    "sd = get_stable_diffusion(StableDiffusionWithNegativePrompt)\n",
    "embedding = sd.embed_prompt(\"a photo of a giraffe in paris\", \"blurry\")\n",
    "embedding.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "571e8ac8-5e92-4756-a53e-d51e36edf2bb",
   "metadata": {},
   "source": [
    "Now, we need to pretty much totally rewrite the denoising method to incorporate this negative guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a558ee-503a-406d-b6f7-7a024c1db7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pred_noise(sd, prompt_embedding, l, t, guidance_scale_pos, guidance_scale_neg):\n",
    "    latent_model_input = torch.cat([l] * 3)  # note all 3 latents injected with prompt\n",
    "    # Scale the initial noise by the variance required by the scheduler\n",
    "    latent_model_input = sd.scheduler.scale_model_input(latent_model_input, t)\n",
    "    with torch.no_grad():\n",
    "        noise_pred = sd.unet(\n",
    "            latent_model_input, t, encoder_hidden_states=prompt_embedding\n",
    "        ).sample\n",
    "    noise_pred_uncond, noise_pred_text_pos, noise_pred_text_neg = noise_pred.chunk(3)\n",
    "    noise_pred = noise_pred_uncond\n",
    "    noise_pred += guidance_scale_pos * (noise_pred_text_pos - noise_pred_uncond)\n",
    "    noise_pred -= guidance_scale_neg * (noise_pred_text_neg - noise_pred_uncond)\n",
    "    return noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a21db6-800d-48d9-bb2b-363b604b7b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_noise(sd, embedding, sd.init_latents(), 0, 7.5, 2).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3616909-71b6-4945-8775-d6af435664ab",
   "metadata": {},
   "source": [
    "Finally, we incorporate the negative prompt into the class API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40046982-2c9f-4252-aafe-9ff65eddf08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StableDiffusionWithNegativePrompts(StableDiffusion):\n",
    "    def embed_prompt(self, prompt, negative_prompt):\n",
    "        orig_embedding = super().embed_prompt(prompt)\n",
    "        _, max_length, _ = orig_embedding.shape\n",
    "        neg_text_embeddings = embed_prompt(self, negative_prompt, max_length)\n",
    "        return torch.cat([orig_embedding, neg_text_embeddings])\n",
    "\n",
    "        def denoise(\n",
    "            self,\n",
    "            prompt_embedding,\n",
    "            guidance_scale_pos,\n",
    "            guidance_scale_neg,\n",
    "            l,  # latents\n",
    "            t,  # timestep\n",
    "            i,  # global progress\n",
    "        ):\n",
    "            noise_pred = self.pred_noise(\n",
    "                self, prompt_embedding, l, t, guidance_scale_pos, guidance_scale_neg\n",
    "            )\n",
    "            return self.scheduler.step(noise_pred, t, l).prev_sample\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt,\n",
    "        negative_prompt,\n",
    "        guidance_scale=7.5,\n",
    "        neg_guidance_scale=2,\n",
    "        n_inference_steps=30,\n",
    "        as_pil=False,\n",
    "    ):\n",
    "        prompt_embedding = self.embed_prompt(prompt, negative_prompt)\n",
    "        l = self.init_latents()\n",
    "        self.init_schedule(n_inference_steps)\n",
    "        # Note that the time steps aren't neccesarily 1, 2, 3, etc\n",
    "        for i, t in tqdm(enumerate(self.scheduler.timesteps), total=n_inference_steps):\n",
    "            # workaround for ARM Macs where float64's are not supported\n",
    "            t = t.to(torch.float32).to(TORCH_DEVICE)\n",
    "            l = self.denoise(\n",
    "                prompt_embedding, guidance_scale, neg_guidance_scale, l, t, i\n",
    "            )\n",
    "        return decompress(l, vae, as_pil=as_pil)\n",
    "\n",
    "\n",
    "StableDiffusionWithNegativePrompts.pred_noise = pred_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "196986cb-7b4a-45c8-9344-55014abba095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "sd = get_stable_diffusion(StableDiffusionWithNegativePrompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d090dbe5-9857-4116-8b86-fddf95e5a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pred_noise() missing 1 required positional argument: 'guidance_scale_neg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleonardo da vinci painting of barack obama, renaissance masterpiece\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamateur, ugly, disfigured\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_pil\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36mStableDiffusionWithNegativePrompts.__call__\u001b[0;34m(self, prompt, negative_prompt, guidance_scale, neg_guidance_scale, n_inference_steps, as_pil)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mtimesteps), total\u001b[38;5;241m=\u001b[39mn_inference_steps):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# workaround for ARM Macs where float64's are not supported\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(TORCH_DEVICE)\n\u001b[0;32m---> 39\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decompress(l, vae, as_pil\u001b[38;5;241m=\u001b[39mas_pil)\n",
      "File \u001b[0;32m~/Code/SlowAI/slowai/overview.py:154\u001b[0m, in \u001b[0;36mStableDiffusion.denoise\u001b[0;34m(self, prompt_embedding, l, t, guidance_scale, i, return_noise_pred)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoise\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt_embedding, l, t, guidance_scale, i, return_noise_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    153\u001b[0m ):\n\u001b[0;32m--> 154\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep(noise_pred, t, l)\u001b[38;5;241m.\u001b[39mprev_sample\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_noise_pred:\n",
      "\u001b[0;31mTypeError\u001b[0m: pred_noise() missing 1 required positional argument: 'guidance_scale_neg'"
     ]
    }
   ],
   "source": [
    "sd(\n",
    "    \"leonardo da vinci painting of barack obama, renaissance masterpiece\",\n",
    "    \"amateur, ugly, disfigured\",\n",
    "    as_pil=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e2831-ac50-468e-9e63-737e0c6e9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b2fbf-1a15-4609-a6e8-2188829a0c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
