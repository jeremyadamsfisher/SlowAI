{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f01d2e2-889b-4358-8096-e59d85ddef0b",
   "metadata": {},
   "source": [
    "# Attention and Conditionality\n",
    "\n",
    "> In this module, we implement attention and conditionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd2a77",
   "metadata": {},
   "source": [
    "Adapted from\n",
    "\n",
    "- [https://youtu.be/DH5bp6zTPB4?si=ziAq_45vkifFx1R4&t=2942](https://youtu.be/DH5bp6zTPB4?si=ziAq_45vkifFx1R4&t=2942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc5bbe0-2c11-4627-aaf6-2e34de5ec18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfcce60-3377-4e37-85e2-a8e88da5a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import math\n",
    "from functools import lru_cache, partial\n",
    "from pathlib import Path\n",
    "from pdb import set_trace as st\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum, rearrange\n",
    "from torch import nn, tensor\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from slowai.activations import StoreModuleStatsCB\n",
    "from slowai.augmentation import GlobalAveragePooling, ResNetWithGlobalPoolingInitialConv\n",
    "from slowai.coco import get_coco_dataset_super_rez\n",
    "from slowai.cos_revisited import aesthetics, denoisify, noisify\n",
    "from slowai.ddim import fashion_unet\n",
    "from slowai.ddpm import DDPM\n",
    "from slowai.ddpm import get_dls\n",
    "from slowai.ddpm import get_dls as get_fashion_dls\n",
    "from slowai.ddpm import train\n",
    "from slowai.diffusion_unet import (\n",
    "    Conv,\n",
    "    SaveTimeActivationMixin,\n",
    "    TDownblock,\n",
    "    TimeEmbeddingMLP,\n",
    "    TUnet,\n",
    "    TUpblock,\n",
    "    ddpm,\n",
    "    train,\n",
    ")\n",
    "from slowai.fid import ImageEval\n",
    "from slowai.learner import (\n",
    "    Callback,\n",
    "    DeviceCB,\n",
    "    Learner,\n",
    "    MetricsCB,\n",
    "    ProgressCB,\n",
    "    TrainCB,\n",
    "    after,\n",
    "    def_device,\n",
    "    only,\n",
    ")\n",
    "from slowai.sgd import BatchSchedulerCB\n",
    "from slowai.super_rez import (\n",
    "    KaimingMixin,\n",
    "    ResidualConvBlock,\n",
    "    denorm,\n",
    "    get_imagenet_super_rez_dls,\n",
    ")\n",
    "from slowai.utils import show_image, show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4621a04f-ec53-4700-af1d-5465433f41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aesthetics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c26c24-2072-4645-8eb2-c381ee6a353b",
   "metadata": {},
   "source": [
    "Attention is a \"high-pass\" filter that can help with the limitations of a \"low-pass\" filter such as a convolution.\n",
    "\n",
    "Huggingface diffusers implements a 1D-attention by rasterizing the image, which is how we will implement this (although this is known to be \"suboptimal,\" accourding to Howard.) Then, each pixel has a $C$-dimensional embedding.\n",
    "\n",
    "Johno mentions that softmax tends to assign all weight to a single dimension, but this is often undesirable. Multi-headedness compensates for this by creating many orthoganol subspaces within which attention is assigned.\n",
    "\n",
    "See [jer.fish/posts/notes-on-self-attention](https://www.jer.fish/posts/notes-on-self-attention/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789fc105-289b-42de-b66a-698e2431ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class MultiheadSelfAttention1D(nn.Module):\n",
    "    def __init__(self, nc, nh):\n",
    "        super().__init__()\n",
    "        self.nc = nc\n",
    "        self.nh = nh\n",
    "        self.norm = nn.BatchNorm2d(nc)\n",
    "        self.kqv = nn.Linear(nc, nc * 3)\n",
    "        self.lin = nn.Linear(nc, nc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.shape\n",
    "        # Normalize\n",
    "        x = self.norm(x)\n",
    "        # Rasterize\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        # Project into a K, Q, V space\n",
    "        x = self.kqv(x)\n",
    "        # Divide into different heads of K, Q and V\n",
    "        k, q, v = rearrange(x, \"b t (nh hs k) -> k (b nh) t hs\", nh=self.nh, k=3)\n",
    "        # Compute affinity\n",
    "        affinity_scores = k @ q.transpose(1, 2)\n",
    "        affinity_scores /= math.sqrt(self.nc / self.nh)\n",
    "        affinity = affinity_scores.softmax(dim=2)\n",
    "        # Re-weight V by the affinity matrix\n",
    "        x = affinity @ v\n",
    "        # Concatenate the heads\n",
    "        x = rearrange(x, \"(b nh) t hs -> b t (nh hs)\", nh=self.nh)\n",
    "        # Linearly project (share information channel-wise instead of time-wise)\n",
    "        x = self.lin(x)\n",
    "        # Un-rasterize\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a845e2d6-a5a6-4212-99b1-fc62828c2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = MultiheadSelfAttention1D(nc=32, nh=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af26882-0465-47ad-a4b1-c9ef0c598990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(8, 32, 16, 16)\n",
    "attn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd53a25-11c9-43b2-a121-3c19e5b87bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "del attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100616af-38ff-404c-a776-d13c9e5449cd",
   "metadata": {},
   "source": [
    "Let's use this in a **Diffusion UNe**t.  Note that we cannot use attention near the beginning or head due to the quadratic time and space capacity (there are more time steps at these points). Typically, attention is only used when the feature map is 16x16 or 32x32 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130509ea-657a-47bf-8d22-0eb70a0a0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAResBlock(nn.Module):\n",
    "    def __init__(self, t_embed, c_in, c_out, ks=3, stride=2, nh=None):\n",
    "        super().__init__()\n",
    "        self.t_embed = t_embed\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "\n",
    "        self.t_emb_proj = nn.Linear(t_embed, c_out * 2)\n",
    "        self.conv_a = Conv(c_in, c_out)\n",
    "        self.conv_b = Conv(c_out, c_out)\n",
    "\n",
    "        if c_in != c_out:\n",
    "            self.id_conv = nn.Conv2d(c_in, c_out, kernel_size=1)\n",
    "        else:\n",
    "            self.id_conv = None\n",
    "\n",
    "        if nh:\n",
    "            self.attn = MultiheadSelfAttention1D(c_out, nh)\n",
    "        else:\n",
    "            self.attn = None\n",
    "\n",
    "    def forward(self, x_orig, t_emb):\n",
    "        # non-residual link\n",
    "        x = self.conv_a(x_orig)\n",
    "        t_emb = self.t_emb_proj(F.relu(t_emb))[:, :, None, None]\n",
    "        scale, shift = torch.chunk(t_emb, 2, dim=1)\n",
    "        x = x * (1 + scale) + shift\n",
    "        x = self.conv_b(x)\n",
    "\n",
    "        # residual link\n",
    "        xr = self.id_conv(x_orig) if self.id_conv else x_orig\n",
    "\n",
    "        x = x + xr\n",
    "\n",
    "        if self.attn:\n",
    "            x = x + self.attn(x)\n",
    "\n",
    "        self.output = x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7e10c9-58d4-4ac9-b231-388d66008c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TADownblock(TDownblock):\n",
    "    def __init__(self, t_embed, c_in, c_out, downsample=True, n_layers=1, nh=None):\n",
    "        super().__init__(t_embed, c_in, c_out, downsample, n_layers)\n",
    "\n",
    "        # Use the new Attention ResBlocks\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(TAResBlock(t_embed, c_in, c_out, stride=1, nh=nh))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.convs.append(TAResBlock(t_embed, c_out, c_out, stride=1, nh=nh))\n",
    "        self.downsampler = nn.Conv2d(c_out, c_out, kernel_size=3, stride=2, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8be57cd2-51f8-4f16-ae3b-c4f96ea3ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TAUpblock(TUpblock):\n",
    "    def __init__(self, t_embed, c_in, c_out, upsample=True, n_layers=1, nh=None):\n",
    "        super().__init__(t_embed, c_in, c_out, upsample, n_layers)\n",
    "\n",
    "        # Use the new Attention ResBlocks\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.convs.append(TAResBlock(t_embed, c_in * 2, c_in, stride=1))\n",
    "        self.convs.append(TAResBlock(t_embed, c_in * 2, c_out, stride=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3695dcfb-1467-4738-8167-62e4ddcefa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAUnet(TUnet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nfs=(224, 448, 672, 896),\n",
    "        attention_heads=(0, 8, 8, 8),\n",
    "        n_blocks=(3, 2, 2, 1, 1),\n",
    "        color_channels=3,\n",
    "    ):\n",
    "        assert len(n_blocks) == len(attention_heads) == len(nfs) + 1\n",
    "        super().__init__(nfs, n_blocks, color_channels)\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for c_in, c_out, n_layers, nh in zip(nfs, nfs[1:], n_blocks, attention_heads):\n",
    "            db = TADownblock(\n",
    "                self.time_embedding.c_out,\n",
    "                c_in,\n",
    "                c_out,\n",
    "                n_layers=n_layers,\n",
    "                nh=nh,\n",
    "            )\n",
    "            self.downblocks.append(db)\n",
    "            self.upblocks.insert(0, TAUpblock.from_downblock(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9211fb0b-f98d-4c46-bb30-efa154e91424",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_fashion_dls(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601f2bc3-084f-4304-9bf2-7dddaf99b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# un = train(\n",
    "#     TAUnet.kaiming(\n",
    "#         color_channels=1,\n",
    "#         nfs=(32, 64, 128, 256, 384),\n",
    "#         n_blocks=(3, 2, 1, 1, 1, 1),\n",
    "#         attention_heads=(0, 8, 8, 8, 8, 8),\n",
    "#     ),\n",
    "#     dls,\n",
    "#     lr=1e-3,\n",
    "#     n_epochs=25,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e96fff29-c521-45ca-8ece-beb4020ef90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_0, _ = ddpm(un, (8, 1, 32, 32), n_steps=250)\n",
    "# show_images(x_0, imsize=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f89bfe1-11e1-4029-86b2-632d97dcf0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del un"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a5e45-7c93-4628-be32-b9652855e4cf",
   "metadata": {},
   "source": [
    "# Adding conditionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "872e5521-12c2-4281-8d10-7a2a47e9e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalTAUnet(TAUnet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes,\n",
    "        nfs=(224, 448, 672, 896),\n",
    "        attention_heads=(0, 8, 8, 8),\n",
    "        n_blocks=(3, 2, 2, 1, 1),\n",
    "        color_channels=3,\n",
    "    ):\n",
    "        super().__init__(nfs, attention_heads, n_blocks, color_channels)\n",
    "        self.conditional_embedding = nn.Embedding(n_classes, self.time_embedding.c_out)\n",
    "\n",
    "    def forward(self, x_t, t, c):\n",
    "        t = self.time_embedding(t)\n",
    "        c = self.conditional_embedding(c)  # ðŸ‘ˆ\n",
    "        e = t + c\n",
    "        x = self.start(x_t)\n",
    "        for db in self.downblocks:\n",
    "            x = db(x, t)\n",
    "        x = self.middle(x, e)\n",
    "        for ub, db in zip(self.upblocks, reversed(self.downblocks)):\n",
    "            x = ub(x, db, e)\n",
    "        return self.end(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "590bf6c6-1002-423b-ad47-e8d13816bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalFashionDDPM(TrainCB):\n",
    "    def before_batch(self, learn):\n",
    "        x0, c = learn.batch\n",
    "        (x_t, t), epsilon = noisify(x0)\n",
    "        learn.batch = (x_t, t, c), epsilon\n",
    "\n",
    "    def predict(self, learn):\n",
    "        (x_t, t, c), _ = learn.batch\n",
    "        learn.preds = learn.model(x_t, t, c)\n",
    "\n",
    "    def get_loss(self, learn):\n",
    "        _, epsilon = learn.batch\n",
    "        learn.loss = learn.loss_func(learn.preds, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c11d6aa-aed2-405e-a08f-4fea80f006ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def conditional_train(\n",
    "    model, dls, lr=4e-3, n_epochs=25, extra_cbs=[], loss_fn=F.mse_loss\n",
    "):\n",
    "    T_max = len(dls[\"train\"]) * n_epochs\n",
    "    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)\n",
    "    cbs = [\n",
    "        DeviceCB(),\n",
    "        ProgressCB(plot=True),\n",
    "        scheduler,\n",
    "        MetricsCB(),\n",
    "        DeviceCB(),\n",
    "        ConditionalFashionDDPM(),  # ðŸ‘ˆ\n",
    "        *extra_cbs,\n",
    "    ]\n",
    "    learner = Learner(\n",
    "        model,\n",
    "        dls,\n",
    "        loss_fn,\n",
    "        lr=lr,\n",
    "        cbs=cbs,\n",
    "        opt_func=partial(torch.optim.AdamW, eps=1e-5),\n",
    "    )\n",
    "    learner.fit(n_epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9c77b-1c79-4d19-a8bd-fe4e4fbe92ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/25 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='69' class='' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      58.47% [69/118 00:24&lt;00:17 0.794]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "un = conditional_train(\n",
    "    ConditionalTAUnet.kaiming(\n",
    "        n_classes=10,\n",
    "        color_channels=1,\n",
    "        nfs=(32, 64, 128, 256, 384),\n",
    "        n_blocks=(3, 2, 1, 1, 1, 1),\n",
    "        attention_heads=(0, 8, 8, 8, 8, 8),\n",
    "    ),\n",
    "    dls,\n",
    "    lr=4e-3,\n",
    "    n_epochs=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf3983-2db2-4e67-801c-2aa4032a968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed507a9d-43fb-4df8-9c0f-7e4ed6f3bf21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
