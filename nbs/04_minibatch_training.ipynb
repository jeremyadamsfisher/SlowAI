{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ce5939-77ef-4fc8-9dfd-6d68ca008190",
   "metadata": {},
   "source": [
    "# Minibatch training\n",
    "\n",
    "> Training a model with apropriate loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186a601",
   "metadata": {},
   "source": [
    "Adapted from:\n",
    "\n",
    "- [https://youtu.be/vGdB4eI4KBs?si=q9fdqe6NPfJxf2NC&t=4736](https://youtu.be/vGdB4eI4KBs?si=q9fdqe6NPfJxf2NC&t=4736)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8511f0-53c5-402a-aa71-56fa2041955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp minibatch_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b53d5d-1502-4b49-9c3a-c0248826c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import fastcore.all as fc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "from slowai.calculus import MNISTDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd43c733-6eae-43bc-8845-7ad5993b3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "torch.set_printoptions(precision=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17baa36f-ca08-41e5-9a2b-c9845650cbb3",
   "metadata": {},
   "source": [
    "## Cross entropy loss\n",
    "Continuing the simple model from the previous notebook, we need to implement a formally apropriate loss function. Regression is inapropriate for categorical outputs because it implies that different categories are different \"distances\" from eachother depending on their ordinal.\n",
    "\n",
    "The proper output shall be a probability for each categories and the loss function is known as **Cross Entropy** loss.\n",
    "\n",
    "> In information theory, the cross-entropy between two probability distributions $p$ and $q$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution \n",
    "$q$, rather than the true distribution $p$.\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy)\n",
    "\n",
    "This works by:\n",
    "\n",
    "1. The model outputs an unnormalized logit for each category ($\\vec{z}$)\n",
    "2. The softmax of the output (i.e., expotentiating and dividing by the sum of the expotentiated values) is taken\n",
    "\n",
    "$$p_{y_i}=\\sigma_{\\vec{z}}(z_i)=\\frac{e^{z_i}}{\\Sigma e_{z_j}}$$\n",
    "\n",
    "3. The entropy is computed between each softmax and its corresponding label\n",
    "\n",
    "$$\n",
    "-\\Sigma_{i=1}^{N} \\left[ y_i log( p_{y_i} ) + ( 1 - y_i ) log( 1 - p_{y_i} ) ) \\right]\n",
    "$$\n",
    "\n",
    "Where $y_i \\in \\{0,1\\}$, such that for a single label output distribution, this simplifies to \n",
    "\n",
    "$$\n",
    "-log( p_{y_i} )\n",
    "$$\n",
    "\n",
    "More information [here.](https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d2bb6c-1b3d-4a33-baca-abe6a53e7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of predictions\n",
    "N = 5\n",
    "\n",
    "# Assign some random prediction logits to demonstrate the operation of log-softmax\n",
    "prd = torch.rand(N, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce88a734-6bed-405e-8416-e55a95ee0969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1, -2.1, -2.6, -2.0, -2.6, -2.4, -2.7, -2.2, -2.0, -2.8],\n",
       "        [-2.0, -2.4, -2.1, -2.4, -2.2, -2.5, -2.1, -2.4, -2.7, -2.3],\n",
       "        [-2.4, -2.2, -2.4, -1.8, -2.6, -2.4, -2.3, -2.5, -2.1, -2.7],\n",
       "        [-2.0, -2.9, -2.1, -2.4, -2.6, -2.1, -2.4, -2.0, -2.4, -2.6],\n",
       "        [-2.3, -2.6, -2.2, -2.0, -2.1, -2.7, -2.1, -2.3, -2.2, -2.7]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_softmax_naive(x):\n",
    "    softmax = x.exp() / x.exp().sum(axis=-1, keepdim=True)\n",
    "    return softmax.log()\n",
    "\n",
    "\n",
    "lsm_prd = log_softmax_naive(prd)\n",
    "lsm_prd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf5f19-320e-4a31-bc6e-817a61315510",
   "metadata": {},
   "source": [
    "In generally, $log$s are handy because these additions are more numerically stable than products. We can take advantage of this because we have a division within a log:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "log(p_{y_i}) &= log(\\frac{e^{z_i}}{\\Sigma e_{z_j}}) \\\\\n",
    "             &= log(e^{z_i}) - log({\\Sigma e_{z_j}}) \\\\\n",
    "             &= z_i - log({\\Sigma e_{z_j}})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45940a1d-7315-4595-8385-4da80fd371d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_less_naive(x):\n",
    "    return x - x.exp().sum(axis=-1, keepdim=True).log()\n",
    "\n",
    "\n",
    "assert torch.isclose(lsm_prd, log_softmax_less_naive(prd)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bace99-36bd-418b-b6a2-95ab2b4a0415",
   "metadata": {},
   "source": [
    "One more trick. These sums can get larger, and we can deal with smaller, more stable sums using the [LogSumExp trick](https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/). \n",
    "\n",
    "Let $a=max(\\vec{v})$. Then, $$\n",
    "\\begin{align*}\n",
    "\\sum e_{z_j-a} &= e^{z_i-a} + \\dots + e^{z_j-a} \\\\\n",
    "                 &= \\frac{e^{z_i} + \\dots + e^{z_j}}{e^{a\\vec{I}}} \\\\\n",
    "                 &= \\frac{ \\sum e^{z_j} }{e^{a}}\n",
    "\\end{align*}\n",
    "$$\n",
    "Therefore, $$\n",
    "\\begin{align*}\n",
    "\\sum e^{z_j} &= e^a  \\left( \\sum e_{z_j-a} \\right) \\\\\n",
    "log \\left( \\sum e_{z_j} \\right) &= log \\left( e^a \\cdot \\left( \\sum e_{z_j-a} \\right) \\right) \\\\\n",
    "                                &= log(e^a) + log \\left( \\sum e_{z_j-a} \\right) \\\\\n",
    "                                &= a + log \\left( \\sum e_{z_j-a} \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516d8be9-cc38-450e-ab34-1791ac29c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    # Since we're using a matrix instead of a vector, we take the row-wise max\n",
    "    # to vectorize across all rows\n",
    "    a = x.max(dim=-1).values\n",
    "    # We also covert `a` into a column vector to be broadcast the same value\n",
    "    # across each column\n",
    "    return a + (x - a[:, None]).exp().sum(-1).log()\n",
    "\n",
    "\n",
    "assert (logsumexp(prd) == prd.logsumexp(axis=1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72eb9ca-6a74-481e-8fb0-4491d8bb9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    a = x.max(dim=-1).values[:, None]\n",
    "    # This gives us the log-sum-exponent term, alternately (x-a).logsumexp(...)\n",
    "    lse = a + (x - a).exp().sum(axis=-1, keepdim=True).log()\n",
    "    # We subtract this from x to give the final log softmax\n",
    "    return x - lse\n",
    "\n",
    "\n",
    "assert torch.isclose(log_softmax(prd), lsm_prd).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbdcdc5-ae32-4466-9229-1afcf43a4d23",
   "metadata": {},
   "source": [
    "Now, for some target $x$, the prediction $p(x)$ is given by $$\n",
    "-\\Sigma_i^N x_i \\cdot log ( p(x_i ) )\n",
    "$$\n",
    "\n",
    "But since the $x$'s are one-hot encoded, this is simply $-log(p(x_{target}))$. We can index into this target by composing a slice of `(row_index, target_index)` pairs like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383cf51d-5a06-4c61-ad58-1a756b7e9e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 0, 1, 0, 7]),\n",
       " torch.Size([5, 10]),\n",
       " tensor([1.0, 0.9, 0.4, 1.0, 0.6]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt = torch.randint(0, 9, size=(N,))\n",
    "tgt, prd.shape, prd[range(N), tgt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08f089-4538-4d78-83cd-805d464f06a6",
   "metadata": {},
   "source": [
    "Alternately,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cb7769-c9d4-4f65-bd43-ce892e425ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(inp, tgt):\n",
    "    \"\"\"mean negative log likelihood loss\"\"\"\n",
    "    (n,) = tgt.shape\n",
    "    return -inp[range(n), tgt].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab78e3-fb91-4ce2-8820-5706f5c64111",
   "metadata": {},
   "source": [
    "This is equivalent to `F.nll_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88dd1b26-e8b7-425d-a177-80211f312eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1), tensor(2.1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(log_softmax(prd), tgt), F.nll_loss(F.log_softmax(prd, dim=-1), tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237dfe8-3206-4866-8579-b2fcb47bc1cb",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Here, we'll take what we have implemented by hand and substitute the PyTorch equivalents.\n",
    "\n",
    "### üíø Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58bc100d-9d4d-4a8c-935a-aef16b8c2725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784, 60000, 50, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = MNISTDataModule()\n",
    "dm.setup()\n",
    "X_trn, y_trn = dm.as_matrix(\"trn\")\n",
    "X_trn = rearrange(X_trn, \"n w h -> n (w h)\")\n",
    "bs = 128\n",
    "n, m = X_trn.shape\n",
    "nh = 50  # num. hidden dimensions\n",
    "n_output_categories = y_trn.max().item() + 1\n",
    "bs, m, n, nh, n_output_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c6102-63cf-4fa2-8f9a-3716e42d5120",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0191a6-09dd-4b2a-b6fc-cfb698ab2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            torch.nn.Linear(n_in, nh),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(nh, n_out),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model(m, nh, n_output_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d908c-68aa-433d-9952-6298b02812d3",
   "metadata": {},
   "source": [
    "### üßê Do a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d3f3d31-24cf-480b-9935-b0aca26d8e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "         1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "         9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
       "         1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
       "         7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
       "         2, 0, 2, 7, 1, 8, 6, 4]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = X_trn[:bs, :]\n",
    "yb = y_trn[:bs]\n",
    "preds = model(xb)\n",
    "preds, preds.shape\n",
    "preds.argmax(axis=1), yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcaaf5f0-d894-492b-b336-2e12eab11e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38adc0e4-79e7-4d81-9218-cc67e885920b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15.62%'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (preds.argmax(axis=1) == yb).sum() / bs\n",
    "f\"{accuracy:.2%}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b614009-ed2c-476a-b999-72a47ea9b943",
   "metadata": {},
   "source": [
    "### üèÉ Train in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87417a9-2cc4-4d8a-970a-7da5e3ca1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: loss=2.31, accuracy=15.62%\n",
      "epoch=1: loss=0.14, accuracy=96.09%\n",
      "epoch=2: loss=0.10, accuracy=96.88%\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "lr = 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        mask = slice(i, min(n, i + bs))\n",
    "        xb = X_trn[mask]\n",
    "        yb = y_trn[mask]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        if i == 0:\n",
    "            (bs,) = yb.shape\n",
    "            accuracy = (preds.argmax(axis=1) == yb).sum() / bs\n",
    "            print(f\"{epoch=}: loss={loss.item():.2f}, accuracy={accuracy.item():.2%}\")\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):  # i.e., trainable\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea90871-1b55-4d6e-8362-b9a11b72294f",
   "metadata": {},
   "source": [
    "At this point, Jeremy refactors the training loop to:\n",
    "\n",
    " - incorporate a module/parameter registry to make it cleaner to update the weights\n",
    " - reimplemented the models in the previous notebook as `torch.nn.Module`'s\n",
    " - implements an optimizer class that stores the parameters and updates them based on the gradient computed by torch itself\n",
    " - replaces the optimizer with the `torch.optim` equivalent\n",
    " - refactored the data loader with the apropriate torch primitives\n",
    "\n",
    "I'm skipping all this because I'm pretty solid with the PyTorch fundamentals already.\n",
    "\n",
    "**A few nice tips**\n",
    "\n",
    "1. In fastcore, How do I take the `*args, **kwargs` of a constructor and populate the object state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ee4588-8ef4-4cbe-bac0-5d0fdc2cba32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quz'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Foo:\n",
    "    def __init__(self, bar, baz=\"quz\"):\n",
    "        fc.store_attr()\n",
    "\n",
    "\n",
    "Foo(\"qux\").baz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0418c8b-2ee1-406f-b113-bef8e88a0f91",
   "metadata": {},
   "source": [
    "This is a bit like a `dataclass.__post_init__`.\n",
    "\n",
    "2. How do you control the generation of indecies to sample?\n",
    "\n",
    "> `torch.utils.data.Sampler` classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a Sampler could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c474e89c-3740-4d15-81cb-417f54316f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5a9da-4f96-4c8f-a590-64edb415256e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
