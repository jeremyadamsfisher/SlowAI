# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/22_tiny_imagenet_a.ipynb.

# %% auto 0
__all__ = ['Î¼', 'Î£', 'preprocess', 'fill', 'preprocess_and_augment', 'preprocess_and_trivial_augment', 'tiny_imagenet',
           'TinyImageNetDatasetGenerator', 'tiny_imagenet_dataset_dict', 'to_rgb', 'norm', 'denorm',
           'preprocess_factory', 'get_imagenet_dls', 'viz', 'lr_find', 'train', 'StackableResidualConvBlock']

# %% ../nbs/22_tiny_imagenet_a.ipynb 3
import subprocess
from functools import partial
from pathlib import Path

import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
import torchvision
import torchvision.transforms as T
from datasets import Dataset, DatasetDict
from datasets import Image as DatasetsImage
from datasets import load_dataset
from PIL import Image
from torch import nn
from torch.optim import AdamW, lr_scheduler
from torchmetrics.classification import MulticlassAccuracy

from .augmentation import ResidualConvBlock, ResNetWithGlobalPooling, summarize
from .initializations import init_leaky_weights
from slowai.learner import (
    Callback,
    DataLoaders,
    DeviceCB,
    MetricsCB,
    ProgressCB,
    TrainLearner,
)
from .resnets import Conv, GeneralReLU, ResidualConvBlock
from .sgd import BatchSchedulerCB
from .utils import show_images

# %% ../nbs/22_tiny_imagenet_a.ipynb 4
def tiny_imagenet(dir_=Path("../tiny-imagenet-200")):
    download_url = "http://cs231n.stanford.edu/tiny-imagenet-200.zip"
    if not dir_.exists():
        cmd = f"""
        cd .. && wget {download_url} && unzip tiny-imagenet-200.zip || rm "../tiny-imagenet-200.zip*"
        """
        subprocess.call(cmd, shell=True)
    return dir_

# %% ../nbs/22_tiny_imagenet_a.ipynb 6
class TinyImageNetDatasetGenerator:
    """Generate Tiny Imagenet rows lazily from zip file"""

    def __init__(self, base_dir=None):
        if base_dir is None:
            base_dir = Path(tiny_imagenet())
        self.base_dir = base_dir
        self.wn2word = {}
        with (base_dir / "words.txt").open() as f:
            for line in f:
                wn, word = line.strip().split("\t")
                self.wn2word[wn] = word
        self.wn2id = {}
        with (base_dir / "wnids.txt").open() as f:
            for idx, wn in enumerate(f):
                self.wn2id[wn.strip()] = idx

    def gen_train(self):
        for fp in (self.base_dir / "train").glob("*/images/*"):
            wn = fp.parent.parent.name.strip()
            word = self.wn2word[wn]
            idx = self.wn2id[wn]
            yield {"image": str(fp), "idx": idx, "word": word}

    def gen_test(self):
        annotations_fp = self.base_dir / "val" / "val_annotations.txt"
        with open(annotations_fp) as f:
            for line in f:
                fname, wn, *_ = line.split("\t")
                fp = self.base_dir / "val" / "images" / fname
                assert fp.exists()
                word = self.wn2word[wn]
                idx = self.wn2id[wn]
                yield {"image": str(fp), "idx": idx, "word": word}

# %% ../nbs/22_tiny_imagenet_a.ipynb 8
def tiny_imagenet_dataset_dict():
    """Huggingface dataset from the HF hub"""
    return load_dataset("jeremyf/tiny-imagent-200")

# %% ../nbs/22_tiny_imagenet_a.ipynb 9
def to_rgb(img):
    return img.convert("RGB")

# %% ../nbs/22_tiny_imagenet_a.ipynb 10
Î¼ = torch.tensor([0.48, 0.40, 0.31])
Î£ = torch.tensor([0.28, 0.24, 0.27])

# %% ../nbs/22_tiny_imagenet_a.ipynb 11
def norm(x):
    x = x - Î¼[:, None, None]
    x = x / Î£[:, None, None]
    return x

# %% ../nbs/22_tiny_imagenet_a.ipynb 12
def denorm(x):
    x = x * Î£[:, None, None]
    x = x + Î¼[:, None, None]
    return x

# %% ../nbs/22_tiny_imagenet_a.ipynb 13
preprocess = [
    to_rgb,
    T.PILToTensor(),
    T.ConvertImageDtype(torch.float),
    norm,
]

# %% ../nbs/22_tiny_imagenet_a.ipynb 14
def preprocess_factory(pipe_):
    pipe = torchvision.transforms.Compose(pipe_)

    def preprocess(examples):
        images = []
        for image in examples["image"]:
            image = pipe(image)
            images.append(image)
        x = torch.stack(images)
        examples["image"] = x
        return examples

    return preprocess

# %% ../nbs/22_tiny_imagenet_a.ipynb 15
def get_imagenet_dls(bs=512, with_word=False, training_preprocessor=preprocess):
    dsd = tiny_imagenet_dataset_dict()
    dsd["train"].set_transform(preprocess_factory(training_preprocessor))
    dsd["train"] = dsd["train"].shuffle()
    dsd["test"] = (
        dsd["test"]
        .map(preprocess_factory(preprocess), batched=True)
        .with_format("torch")
    )
    columns = ["image", "idx"]
    if with_word:
        columns.append("word")
    return DataLoaders.from_dsd(dsd, bs=bs).listify(columns=columns)

# %% ../nbs/22_tiny_imagenet_a.ipynb 17
def viz(dls):
    try:
        xb, _, words = dls.peek()
    except ValueError:
        xb, _ = dls.peek()
        show_images(denorm(xb)[:6].squeeze())
    else:
        titles = [w[:25].strip() for w in words]
        show_images(denorm(xb)[:6].squeeze(), titles=titles[:6])

# %% ../nbs/22_tiny_imagenet_a.ipynb 26
def lr_find(
    model,
    dls,
    start_lr,
    gamma=1.3,
    n_epochs=2,
    extra_cbs=(),
    loss_fn=F.cross_entropy,
):
    TrainLearner(
        model,
        dls,
        loss_fn,
        lr=start_lr,
        cbs=[DeviceCB(), ProgressCB(plot=True), *extra_cbs],
        opt_func=partial(torch.optim.AdamW, eps=1e-5),
    ).lr_find(
        start_lr=start_lr,
        gamma=gamma,
        max_epochs=n_epochs,
    )

# %% ../nbs/22_tiny_imagenet_a.ipynb 28
def train(
    model,
    dls,
    lr=1e-2,
    n_epochs=2,
    extra_cbs=tuple(),
):
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        MetricsCB(MulticlassAccuracy(num_classes=200)),
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        *extra_cbs,
    ]
    learner = TrainLearner(
        model,
        dls,
        F.cross_entropy,
        lr=lr,
        cbs=cbs,
        opt_func=partial(torch.optim.AdamW, eps=1e-5),
    )
    learner.fit(n_epochs)

# %% ../nbs/22_tiny_imagenet_a.ipynb 32
fill = list((Î¼ * 255).int())

preprocess_and_augment = [
    to_rgb,
    T.Pad(4, fill=fill),
    T.RandomCrop(64, fill=fill),
    T.RandomHorizontalFlip(),
    T.PILToTensor(),
    T.ConvertImageDtype(torch.float),
    T.RandomErasing(p=0.3, scale=(0.02, 0.2), value=list(Î¼)),
    norm,
]

# %% ../nbs/22_tiny_imagenet_a.ipynb 39
class StackableResidualConvBlock(nn.Sequential):
    def __init__(self, n_blocks, c_in, c_out, conv_cls=ResidualConvBlock):
        layers = []
        if n_blocks < 1:
            raise ValueError
        if n_blocks == 1:
            # Only one block, make sure to downsample
            downsample_height_width = conv_cls(c_in, c_out, stride=2)
            layers.append(downsample_height_width)
        else:
            for i in range(n_blocks - 1):
                # This differs slightly from Howard's implementation
                # because he disables the post-activation BatchNorm2d
                if i == 0:
                    layer = conv_cls(c_in, c_out, stride=1)
                else:
                    layer = conv_cls(c_out, c_out, stride=1)
                layers.append(layer)
            # In the last ResConvBlock, downsample the feature map
            # width and height by 2x, but do NOT change the number
            # of channels
            downsample_height_width = conv_cls(c_out, c_out, stride=2)
            layers.append(downsample_height_width)
        super().__init__(*layers)

# %% ../nbs/22_tiny_imagenet_a.ipynb 46
preprocess_and_trivial_augment = [
    to_rgb,
    T.Pad(4, fill=fill),
    T.RandomCrop(64, fill=fill),
    T.RandomHorizontalFlip(),
    T.TrivialAugmentWide(fill=fill),  # ðŸ‘ˆ
    T.PILToTensor(),
    T.ConvertImageDtype(torch.float),
    norm,
]
