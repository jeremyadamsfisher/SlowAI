# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/13_ddpm.ipynb.

# %% auto 0
__all__ = ['pipe', 'get_dls', 'DDPM', 'sample', 'train', 'fashion_unet', 'init_', 'MixedPrecision', 'AccelerateCB']

# %% ../nbs/13_ddpm.ipynb 3
from functools import partial

import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
import torchvision.transforms as T
from accelerate import Accelerator
from diffusers import LMSDiscreteScheduler, UNet2DModel
from torch.nn import init
from torch.optim import lr_scheduler
from tqdm import tqdm

from slowai.learner import (
    Callback,
    DataLoaders,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    after,
    def_device,
    only,
    tensorize_images,
)
from .sgd import BatchSchedulerCB
from .utils import show_image, show_images

# %% ../nbs/13_ddpm.ipynb 6
pipe = [
    T.Resize((32, 32)),  # ðŸ‘ˆ here...
    T.PILToTensor(),
    T.ConvertImageDtype(torch.float),
    T.Lambda(lambda x: x - 0.5),
]

# %% ../nbs/13_ddpm.ipynb 7
def get_dls(bs=128):
    return tensorize_images(
        DataLoaders.from_hf("fashion_mnist", bs=bs),
        pipe=pipe,
        normalize=False,  # ðŸ‘ˆ ...and here
    ).listify()

# %% ../nbs/13_ddpm.ipynb 15
class DDPM(Callback, order=after(DeviceCB)):
    def __init__(self, n_steps=1000, Î²min=0.0001, Î²max=0.02):
        self.n_steps = n_steps
        self.Î²min = Î²min
        self.Î²max = Î²max
        self.Î² = torch.linspace(Î²min, Î²max, n_steps)
        self.É‘ = 1 - self.Î²
        self.á¾± = torch.cumprod(self.É‘, dim=0)
        self.Ïƒ = self.Î².sqrt()

    def before_batch(self, learn):
        # Starting with the original image
        x_0, _ = learn.batch
        device = x_0.device
        self.á¾± = self.á¾±.to(device)

        # Sample 2D noise for each example in the batch
        Îµ = torch.randn(x_0.shape, device=device)
        n, *_ = x_0.shape

        # Add noise according to the equation in Algorithm 1, such
        # that the variance of the distribution does not change
        t = torch.randint(0, self.n_steps, (n,), device=device, dtype=torch.long)
        á¾±_t = self.á¾±[t].reshape(-1, 1, 1, 1).to(device)
        x_t = á¾±_t.sqrt() * x_0 + (1 - á¾±_t).sqrt() * Îµ

        learn.batch = ((x_t, t), Îµ)

    @only
    def predict(self, learn):
        (x_t, t), _ = learn.batch
        learn.preds = learn.model(x_t, t).sample

# %% ../nbs/13_ddpm.ipynb 17
@torch.no_grad()
def sample(self, model, sz=(16, 1, 32, 32), device=def_device, return_all=False):
    á¾±, É‘, Ïƒ = self.á¾±.to(device), self.É‘.to(device), self.Ïƒ.to(device)
    x_t = torch.randn(sz, device=device)
    bs, *_ = sz
    preds = []
    iter_ = list(reversed(range(1, self.n_steps)))
    for t in tqdm(iter_, unit="time step"):
        # Predict the noise for each example in the image
        t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)
        noise_pred = model(x_t, t_batch).sample

        # Predict the image without noise
        x_0_pred = x_t - (1 - É‘[t]) / torch.sqrt(1 - á¾±[t]) * noise_pred

        # Add noise to the predicted noiseless image such that it ulimately
        # has slightly less noise than before
        x_t_minus_1 = x_0_pred / É‘[t].sqrt() + (Ïƒ[t] * torch.randn(sz, device=device))

        # Repeat
        x_t = x_t_minus_1
        preds.append(x_t)

    # At the last step, simply rescale and do not add noise
    x_0 = x_0_pred / É‘[0].sqrt()
    preds.append(x_t_minus_1)

    if return_all:
        return preds
    return x_0


DDPM.sample = sample

# %% ../nbs/13_ddpm.ipynb 20
def train(
    model,
    lr=4e-3,
    n_epochs=2,
    bs=128,
    opt_func=torch.optim.Adam,
    extra_cbs=[],
    ddpm=DDPM(),
):
    dls = get_dls(bs)
    T_max = len(dls["train"]) * n_epochs
    cbs = [
        ddpm,
        TrainCB(),
        MetricsCB(),
        DeviceCB(),
        ProgressCB(plot=True),
        BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max),
        *extra_cbs,
    ]
    Learner(
        model,
        dls,
        F.mse_loss,
        lr=lr,
        cbs=cbs,
        opt_func=opt_func,
    ).fit(n_epochs)
    return ddpm

# %% ../nbs/13_ddpm.ipynb 21
def fashion_unet():
    return UNet2DModel(
        sample_size=(32, 32),
        in_channels=1,
        out_channels=1,
        block_out_channels=(16, 32, 64, 128),
        # Group norm splits channels into groups, such that the number of
        # convolutional channels should be divisable by `norm_num_groups`
        norm_num_groups=8,
    )

# %% ../nbs/13_ddpm.ipynb 26
@torch.no_grad()
def init_(self):
    for down_block in self.down_blocks:
        for resnet in down_block.resnets:
            resnet.conv2.weight.data.zero_()

            # Not sure why this is indented
            if down_block.downsamplers is not None:
                for downsampler in down_block.downsamplers:
                    init.orthogonal_(downsampler.conv.weight)

    for up_block in self.up_blocks:
        for resnet in up_block.resnets:
            resnet.conv2.weight.zero_()

    self.conv_out.weight.data.zero_()


UNet2DModel.init_ = init_

# %% ../nbs/13_ddpm.ipynb 32
class MixedPrecision(TrainCB, order=after(DeviceCB)):
    def before_fit(self, learn):
        self.scaler = torch.cuda.amp.GradScaler()

    def before_batch(self, learn):
        self.autocast = torch.autocast("cuda", dtype=torch.float16)
        self.autocast.__enter__()

    def after_loss(self, learn):
        self.autocast.__exit__(None, None, None)

    @only
    def backward(self, learn):
        self.scaler.scale(learn.loss).backward()

    @only
    def step(self, learn):
        self.scaler.step(learn.opt)
        self.scaler.update()

# %% ../nbs/13_ddpm.ipynb 40
class AccelerateCB(TrainCB, order=after(DeviceCB)):
    def __init__(self, mixed_precision="fp16"):
        self.acc = Accelerator(mixed_precision=mixed_precision)

    def before_fit(self, learn):
        learn.model, learn.opt, train, test = self.acc.prepare(
            learn.model,
            learn.opt,
            learn.dls["train"],
            learn.dls["test"],
        )
        learn.dls = {"train": train, "test": test}

    @only
    def backward(self, learn):
        self.acc.backward(learn.loss)
