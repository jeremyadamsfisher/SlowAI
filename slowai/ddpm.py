# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/13_ddpm.ipynb.

# %% auto 0
__all__ = ['get_dls', 'DDPM', 'train', 'fashion_unet', 'MixedPrecision', 'AccelerateCB']

# %% ../nbs/13_ddpm.ipynb 3
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
import torchvision.transforms as T
from accelerate import Accelerator
from diffusers import LMSDiscreteScheduler, UNet2DModel
from torch.nn import init
from torch.optim import lr_scheduler
from tqdm import tqdm
from slowai.learner import (
    Callback,
    DataLoaders,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    after,
    def_device,
    only,
    tensorize_images,
)
from .sgd import BatchSchedulerCB
from .utils import show_image, show_images

# %% ../nbs/13_ddpm.ipynb 6
def get_dls(bs=128):
    pipe = [
        T.Resize((32, 32)),  # ðŸ‘ˆ here...
        T.PILToTensor(),
        T.ConvertImageDtype(torch.float),
    ]
    return tensorize_images(
        DataLoaders.from_hf("fashion_mnist", bs=bs),
        pipe=pipe,
        normalize=False,  # ðŸ‘ˆ ...and here
    ).listify()

# %% ../nbs/13_ddpm.ipynb 13
class DDPM(Callback, order=after(DeviceCB)):
    def __init__(self, n_steps=1000, Î²min=0.0001, Î²max=0.02):
        self.n_steps = n_steps
        self.Î²min = Î²min
        self.Î²max = Î²max
        self.Î² = torch.linspace(Î²min, Î²max, n_steps)
        self.É‘ = 1 - self.Î²
        self.á¾± = torch.cumprod(self.É‘, dim=0)
        self.Ïƒ = self.Î².sqrt()

    def before_batch(self, learn):
        # Starting with the original image
        x_0, _ = learn.batch
        device = x_0.device
        self.á¾± = self.á¾±.to(device)

        # Sample 2D noise for each example in the batch
        Îµ = torch.randn(x_0.shape, device=device)
        n, *_ = x_0.shape

        # Add noise according to the equation in Algorithm 1, such
        # that the variance of the distribution does not change
        t = torch.randint(0, self.n_steps, (n,), device=device, dtype=torch.long)
        á¾±_t = self.á¾±[t].reshape(-1, 1, 1, 1).to(device)
        x_t = á¾±_t.sqrt() * x_0 + (1 - á¾±_t).sqrt() * Îµ

        learn.batch = ((x_t, t), Îµ)

    @only
    def predict(self, learn):
        (x_t, t), _ = learn.batch
        learn.preds = learn.model(x_t, t).sample

# %% ../nbs/13_ddpm.ipynb 19
def train(
    model,
    lr=4e-3,
    n_epochs=2,
    bs=128,
    opt_func=torch.optim.Adam,
    extra_cbs=[],
    ddpm=DDPM(),
):
    dls = get_dls(bs)
    T_max = len(dls["train"]) * n_epochs
    cbs = [
        ddpm,
        TrainCB(),
        MetricsCB(),
        DeviceCB(),
        ProgressCB(plot=True),
        BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max),
        *extra_cbs,
    ]
    Learner(
        model,
        dls,
        F.mse_loss,
        lr=lr,
        cbs=cbs,
        opt_func=opt_func,
    ).fit(n_epochs)
    return ddpm

# %% ../nbs/13_ddpm.ipynb 20
def fashion_unet():
    return UNet2DModel(
        sample_size=(32, 32),
        in_channels=1,
        out_channels=1,
        block_out_channels=(16, 32, 64, 128),
        # Group norm splits channels into groups, such that the number of
        # convolutional channels should be divisable by `norm_num_groups`
        norm_num_groups=8,
    )

# %% ../nbs/13_ddpm.ipynb 31
class MixedPrecision(TrainCB, order=after(DeviceCB)):
    def before_fit(self, learn):
        self.scaler = torch.cuda.amp.GradScaler()

    def before_batch(self, learn):
        self.autocast = torch.autocast("cuda", dtype=torch.float16)
        self.autocast.__enter__()

    def after_loss(self, learn):
        self.autocast.__exit__(None, None, None)

    @only
    def backward(self, learn):
        self.scaler.scale(learn.loss).backward()

    @only
    def step(self, learn):
        self.scaler.step(learn.opt)
        self.scaler.update()

# %% ../nbs/13_ddpm.ipynb 38
class AccelerateCB(TrainCB, order=after(DeviceCB)):
    def __init__(self, mixed_precision="fp16"):
        self.acc = Accelerator(mixed_precision=mixed_precision)

    def before_fit(self, learn):
        learn.model, learn.opt, train, test = self.acc.prepare(
            learn.model,
            learn.opt,
            learn.dls["train"],
            learn.dls["test"],
        )
        learn.dls = {"train": train, "test": test}

    @only
    def backward(self, learn):
        self.acc.backward(learn.loss)
