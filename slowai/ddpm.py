# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/13_ddpm.ipynb.

# %% auto 0
__all__ = ['MixedPrecision', 'AccelerateCB']

# %% ../nbs/13_ddpm.ipynb 3
import random
import re
from contextlib import contextmanager
from dataclasses import dataclass
from functools import partial
from math import sqrt
from typing import Sequence

import matplotlib.pyplot as plt
import numpy as np
import timm
import torch
import torch.nn.functional as F
import torchvision.transforms as T
from accelerate import Accelerator
from diffusers import LMSDiscreteScheduler
from einops import rearrange
from IPython.display import Markdown, display
from torch import distributions, nn
from torch.nn import init
from torch.optim import lr_scheduler
from torchmetrics.classification import MulticlassAccuracy
from torchvision import transforms
from tqdm import tqdm

from .activations import StoreModuleStatsCB, set_seed
from .initializations import init_leaky_weights, set_seed
from slowai.learner import (
    Callback,
    DataLoaders,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    TrainLearner,
    after,
    batchify,
    def_device,
    fashion_mnist,
    only,
    tensorize_images,
)
from .resnets import Conv, ResidualConvBlock
from .sgd import BatchSchedulerCB
from .utils import show_image, show_images

# %% ../nbs/13_ddpm.ipynb 31
class MixedPrecision(TrainCB, order=after(DeviceCB)):
    def before_fit(self, learn):
        self.scaler = torch.cuda.amp.GradScaler()

    def before_batch(self, learn):
        self.autocast = torch.autocast("cuda", dtype=torch.float16)
        self.autocast.__enter__()

    def after_loss(self, learn):
        self.autocast.__exit__(None, None, None)

    @only
    def backward(self, learn):
        self.scaler.scale(learn.loss).backward()

    @only
    def step(self, learn):
        self.scaler.step(learn.opt)
        self.scaler.update()

# %% ../nbs/13_ddpm.ipynb 38
class AccelerateCB(TrainCB, order=after(DeviceCB)):
    def __init__(self, mixed_precision="fp16"):
        self.acc = Accelerator(mixed_precision=mixed_precision)

    def before_fit(self, learn):
        learn.model, learn.opt, train, test = self.acc.prepare(
            learn.model,
            learn.opt,
            learn.dls["train"],
            learn.dls["test"],
        )
        learn.dls = {"train": train, "test": test}

    @only
    def backward(self, learn):
        self.acc.backward(learn.loss)
