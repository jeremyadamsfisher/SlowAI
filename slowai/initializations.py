# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_initializations.ipynb.

# %% auto 0
__all__ = ['C', 'BatchTransformCB', 'NormalizeBatchCB', 'GeneralReLU', 'init_leaky_weights', 'LSUVHook', 'LSUVInitialization',
           'Conv2dGeneral', 'BatchNorm', 'CNNWithGeneralReLUAndBatchNorm']

# %% ../nbs/10_initializations.ipynb 3
import math
import random
from contextlib import contextmanager
from functools import partial
from itertools import cycle

import fastcore.all as fc
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from einops import rearrange
from torch import Tensor, nn, tensor
from torch.nn import init
from torchmetrics.classification import MulticlassAccuracy

from slowai.activations import (
    Conv2dWithReLU,
    Hook,
    HooksCallback,
    StoreModuleStats,
    StoreModuleStatsCB,
    set_seed,
)
from .convs import to_device
from .datasets import get_grid, show_image
from slowai.learner import (
    Callback,
    DeviceCB,
    LRFinderCB,
    MetricsCB,
    MomentumCB,
    ProgressCB,
    TrainLearner,
    fashion_mnist,
    to_cpu,
)
from .utils import clean_mem

# %% ../nbs/10_initializations.ipynb 39
class BatchTransformCB(Callback):
    """Arbitrarily transform a batch"""

    def __init__(self, tfm, on_train=True, on_val=True):
        fc.store_attr()

    def before_batch(self, learn):
        if (self.on_train and learn.training) or (self.on_val and not learn.training):
            learn.batch = self.tfm(learn.batch)

# %% ../nbs/10_initializations.ipynb 40
class NormalizeBatchCB(BatchTransformCB):
    """Unit normalize a batch"""

    def __init__(self, on_train=True, on_val=True):
        fc.store_attr()

    def tfm(self, batch):
        xb, *other = batch
        mu = xb.mean()
        sigma = xb.std()
        return (xb - mu) / sigma, *other

# %% ../nbs/10_initializations.ipynb 44
class GeneralReLU(nn.Module):
    def __init__(self, leak=None, sub=None, max_=None):
        super().__init__()
        self.leak = leak
        self.sub = sub
        self.max_ = max_

    def forward(self, x):
        if self.leak:
            x = F.leaky_relu(x, self.leak)
        else:
            x = F.relu(x)
        if self.sub:
            x -= self.sub
        if self.max_:
            x.clamp_max_(self.max_)
        return x

# %% ../nbs/10_initializations.ipynb 49
def init_leaky_weights(module, leak=0.0):
    if isinstance(module, (nn.Conv2d,)):
        init.kaiming_normal_(module.weight, a=leak)  # ðŸ‘ˆ weirdly, called `a` here

# %% ../nbs/10_initializations.ipynb 55
class LSUVHook(Hook):
    """An abstract hook for performing LSUV for a single layer. `.normalize()`
    needs to be implemented for the specific layers"""

    def __init__(self, m, tol=1e-3):
        self.mean = None
        self.std = None
        self.tol = tol

        def append_moments(module, _, activations):
            activations = to_cpu(activations)
            self.mean = activations.mean()
            self.std = activations.std()

        self.hook = m.register_forward_hook(append_moments)
        self.m = m

    def normalized(self):
        assert self.mean and self.std, "Attempted normalization before processing data"
        return abs(self.mean) <= self.tol and abs(self.std - 1) <= self.tol

    def normalize(self):
        raise NotImplementedError

# %% ../nbs/10_initializations.ipynb 57
class LSUVInitialization(HooksCallback):
    """Layer wise sequential unit variance initialization"""

    def __init__(
        self,
        mods=None,
        mod_filter=fc.noop,
        on_train=True,
        on_valid=False,
        hook_cls=LSUVHook,
    ):
        fc.store_attr()

    def before_fit(self, learn):
        try:
            trn = learn.dls["train"]
            xb, *_ = next(iter(trn))
            xb = to_device(xb)
            super().before_fit(learn)
            with torch.no_grad():
                for i, h in enumerate(self.hooks):
                    for n_batches in range(1_000):
                        model(xb)
                        if not h.normalized():
                            h.normalize()
                        else:
                            print(
                                f"Layer {i} normalized after {n_batches} batches ({h.mean:.2f}, {h.std:.2f})"
                            )
                            break
                    else:
                        raise ValueError("Initialization failed!")
        finally:
            # We don't need these hooks anymore, so we can get rid of them
            # before beginning training
            self.cleanup_fit(learn)
            self.hooks = []

# %% ../nbs/10_initializations.ipynb 63
class Conv2dGeneral(nn.Module):
    """Convolutional neural network with a built in activation"""

    @fc.delegates(nn.Conv2d)
    def __init__(
        self,
        *args,
        nonlinearity=F.relu,
        norm=None,
        **kwargs,
    ):
        super().__init__()
        self.conv = nn.Conv2d(*args, **kwargs)
        self.nonlinearity = nonlinearity
        self.norm = norm

        if not isinstance(self.norm, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            bias = getattr(self.conv, "bias", None)
            assert (
                bias is not None
            ), "Convolutional layer should have a bias if it does not use batchnorm"

    def forward(self, x):
        x = self.conv(x)
        if self.nonlinearity:
            x = self.nonlinearity(x)
        if self.norm:
            x = self.norm(x)
        return x

# %% ../nbs/10_initializations.ipynb 64
C = Conv2dGeneral

# %% ../nbs/10_initializations.ipynb 68
class BatchNorm(nn.Module):
    def __init__(self, num_filters, mom=0.1, eps=1e-5):
        super().__init__()
        self.mom = mom
        self.eps = eps
        self.mults = nn.Parameter(torch.ones(num_filters, 1, 1))
        self.adds = nn.Parameter(torch.zeros(num_filters, 1, 1))

        # These need to be saved in the model
        self.register_buffer("vars", torch.ones(1, num_filters, 1, 1))
        self.register_buffer("means", torch.zeros(1, num_filters, 1, 1))

    def update_stats(self, x):
        bs, c, h, w = x.shape
        m = x.mean((0, 2, 3), keepdim=True)
        v = x.var((0, 2, 3), keepdim=True)

        # Update with an exponentially moving average
        self.means.lerp_(m, self.mom)
        self.vars.lerp_(v, self.mom)
        return m, v

    def forward(self, x):
        if self.training:
            with torch.no_grad():
                m, v = self.update_stats(x)
        else:
            m, v = self.means, self.vars
        x = (x - m) / (v + self.eps).sqrt()
        return x * self.mults + self.adds

# %% ../nbs/10_initializations.ipynb 73
class CNNWithGeneralReLUAndBatchNorm(nn.Module):
    """Six layer convolutional neural network with GeneralRelU"""

    def __init__(self, gr=GeneralReLU):
        super().__init__()
        layers = [
            C(1, 8, 5, 2, 2, nonlinearity=gr(), norm=BatchNorm(8)),
            C(8, 16, 3, 2, 1, nonlinearity=gr(), norm=BatchNorm(16)),
            C(16, 32, 3, 2, 1, nonlinearity=gr(), norm=BatchNorm(32)),
            C(32, 64, 3, 2, 1, nonlinearity=gr(), norm=BatchNorm(64)),
            nn.Conv2d(64, 10, 3, 2, 1),  # 1x1
        ]
        self.layers = nn.ModuleList(layers)

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        *_, w, h = x.shape
        assert w == h == 1
        return rearrange(x, "bs c w h -> bs (c w h)")
