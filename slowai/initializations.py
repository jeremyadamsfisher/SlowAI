# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_initializations.ipynb.

# %% auto 0
__all__ = ['BatchTransformCB', 'NormalizeBatchCB', 'LSUVHook', 'LSUVInitialization']

# %% ../nbs/10_initializations.ipynb 3
import math
import random
from contextlib import contextmanager
from functools import partial

import fastcore.all as fc
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from einops import rearrange
from torch import Tensor, nn
from torch.nn import init
from torchmetrics.classification import MulticlassAccuracy

from slowai.activations import (
    Conv2dWithReLU,
    Hook,
    HooksCallback,
    StoreModuleStats,
    StoreModuleStatsCB,
    set_seed,
)
from .convs import to_device
from .datasets import get_grid, show_image
from slowai.learner import (
    Callback,
    DeviceCB,
    LRFinderCB,
    MetricsCB,
    MomentumCB,
    ProgressCB,
    TrainLearner,
    fashion_mnist,
    to_cpu,
)
from .utils import clean_mem

# %% ../nbs/10_initializations.ipynb 40
class BatchTransformCB(Callback):
    """Arbitrarily transform a batch"""

    def __init__(self, tfm, on_train=True, on_val=True):
        fc.store_attr()

    def before_batch(self, learn):
        if (self.on_train and learn.training) or (self.on_val and not learn.training):
            learn.batch = self.tfm(learn.batch)

# %% ../nbs/10_initializations.ipynb 41
class NormalizeBatchCB(BatchTransformCB):
    """Unit normalize a batch"""

    def __init__(self, on_train=True, on_val=True):
        fc.store_attr()

    def tfm(self, batch):
        xb, *other = batch
        mu = xb.mean()
        sigma = xb.std()
        return (xb - mu) / sigma, *other

# %% ../nbs/10_initializations.ipynb 56
class LSUVHook(Hook):
    """A hook for performing LSUV for a single layer"""

    def __init__(self, m, tol=1e-3):
        self.mean = None
        self.std = None
        self.tol = tol

        def append_moments(module, _, activations):
            activations = to_cpu(activations)
            self.mean = activations.mean()
            self.std = activations.std()

        self.hook = m.register_forward_hook(append_moments)
        self.m = m

    def normalized(self):
        assert self.mean and self.std, "Attempted normalization before processing data"
        return abs(self.mean) > self.tol or abs(self.std - 1) > self.tol

    def normalize(self):
        assert self.mean and self.std, "Attempted normalization before processing data"
        self.m.bias -= self.mean
        self.m.weight.data /= self.std

# %% ../nbs/10_initializations.ipynb 57
class LSUVInitialization(HooksCallback):
    """Layer wise sequential unit variance initialization"""

    def __init__(
        self,
        seed_hook_f,
        mods=None,
        mod_filter=fc.noop,
        on_train=True,
        on_valid=False,
    ):
        fc.store_attr()
        self.hook_cls = LSUVHook

    def before_fit(self, learn):
        super().before_fit(learn)
        with torch.no_grad():
            for h in self.hooks:
                while True:
                    self.seed_hook_f()
                    if not h.normalized():
                        h.normalize()
                    else:
                        break
        # We don't need these hooks anymore, so we can get rid of them
        # before beginning training
        self.cleanup_fit(learn)
        self.hooks = []
