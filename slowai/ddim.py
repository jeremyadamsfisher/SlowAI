# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/19_ddim.ipynb.

# %% auto 0
__all__ = ['fashion_unet', 'sample', 'animate', 'diffusers_sample', 'DDIMOutput', 'DiffusersStyleDDPM', 'DiffusersStyleDDIM']

# %% ../nbs/19_ddim.ipynb 3
from dataclasses import dataclass
from functools import partial
from pathlib import Path

import matplotlib.animation
import matplotlib.pyplot as plt
import torch
from diffusers import (
    DDIMPipeline,
    DDIMScheduler,
    DDPMPipeline,
    DDPMScheduler,
    UNet2DModel,
)
from IPython.display import HTML
from IPython.utils import io
from tqdm import tqdm

from .augmentation import GlobalAveragePooling, ResNetWithGlobalPoolingInitialConv
from .ddpm import DDPM, get_dls, train
from .fid import ImageEval
from .learner import def_device
from .utils import show_images

# %% ../nbs/19_ddim.ipynb 6
def fashion_unet():
    return UNet2DModel(
        sample_size=(32, 32),
        in_channels=1,
        out_channels=1,
        block_out_channels=(32, 64, 128, 256),  # ðŸ‘ˆ double the size
        norm_num_groups=8,
    )

# %% ../nbs/19_ddim.ipynb 9
@torch.no_grad()
def sample(ddpm, model, n=16, device=def_device, return_all=False):
    sz = (n, 1, 32, 32)
    á¾±, É‘, Ïƒ = ddpm.á¾±.to(device), ddpm.É‘.to(device), ddpm.Ïƒ.to(device)
    x_t = torch.randn(sz, device=device)
    bs, *_ = sz
    preds = []
    iter_ = list(reversed(range(1, ddpm.n_steps)))
    for t in tqdm(iter_, unit="time step"):
        # Predict the noise for each example in the image
        if t % 3 == 0 or t < 50:
            t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)
            noise_pred = model(x_t, t_batch).sample

        # Predict the image without noise
        x_0_pred = x_t - (1 - É‘[t]) / torch.sqrt(1 - á¾±[t]) * noise_pred

        # Add noise to the predicted noiseless image such that it ulimately
        # has slightly less noise than before
        x_t_minus_1 = x_0_pred / É‘[t].sqrt() + (Ïƒ[t] * torch.randn(sz, device=device))

        # Repeat
        x_t = x_t_minus_1
        preds.append((x_t, x_0_pred))

    # At the last step, simply rescale and do not add noise
    x_0 = x_0_pred / É‘[0].sqrt()
    preds.append((x_0, x_0))

    if return_all:
        x_ts, x_0s = zip(*preds)
        return x_ts, x_0s
    return x_0

# %% ../nbs/19_ddim.ipynb 11
def animate(imgs):
    fig, axes = plt.subplots(2, 4, figsize=(4, 2))

    def animate(i):
        for ax, im in zip(axes.flatten(), imgs[i]):
            ax.clear()
            ax.imshow(im.cpu().squeeze())
            ax.set_axis_off()
        fig.tight_layout()

    ani = matplotlib.animation.FuncAnimation(
        fig, animate, frames=len(imgs), interval=10.0
    )

    return HTML(ani.to_jshtml())

# %% ../nbs/19_ddim.ipynb 16
@torch.no_grad()
def diffusers_sample(sched, sz=(512, 1, 32, 32), skip_steps=None, **kwargs):
    if skip_steps is None:
        skip_steps = []
    x_t = torch.randn(sz).to(def_device)
    for t in tqdm(sched.timesteps):
        if t not in skip_steps:
            noise = unet(x_t, t).sample
        x_t = sched.step(noise, t, x_t).prev_sample
    return x_t

# %% ../nbs/19_ddim.ipynb 26
@dataclass
class DDIMOutput:
    prev_sample: torch.Tensor


class DiffusersStyleDDPM(DDPM):
    @property
    def timesteps(self):
        return range(self.n_steps - 1, 0, -1)

    def step(self, noise, t, x_t):
        raise NotImplementedError


class DiffusersStyleDDIM(DiffusersStyleDDPM):
    def __init__(self, n_steps=1000, Î²min=0.0001, Î²max=0.02, Î·=1.0):  # Î· is eta
        super().__init__(n_steps, Î²min, Î²max)
        self.Î· = Î·

    def step(self, noise, t, x_t):
        á¾±, Î· = self.á¾±, self.Î·

        # Determine \hat{x}_0
        x_0_pred = (x_t - (1 - á¾±[t]).sqrt() * noise) / á¾±[t].sqrt()

        # Determine the direction towards x_t
        Ïƒ_t = Î· * ((1 - á¾±[t - 1]) / (1 - á¾±[t])).sqrt() * (1 - á¾±[t] / á¾±[t - 1]).sqrt()
        x_t_direction = (1 - á¾±[t - 1] - Ïƒ_t**2).sqrt() * noise

        # Determine the x_{t-1} without nosie
        prev_sample = á¾±[t - 1].sqrt() * x_0_pred + x_t_direction

        # Add random noise, if needed
        if t > 0:
            prev_sample += Ïƒ_t * torch.randn(x_t.shape).to(x_t.device)

        return DDIMOutput(prev_sample=prev_sample)
