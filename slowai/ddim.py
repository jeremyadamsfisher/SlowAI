# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/20_ddim.ipynb.

# %% auto 0
__all__ = ['sample', 'animate', 'diffusers_sample', 'DDIMOutput', 'DiffusersStyleDDPM', 'DiffusersStyleDDIM']

# %% ../nbs/20_ddim.ipynb 3
from dataclasses import dataclass
from functools import partial
from pathlib import Path

import matplotlib.animation
import matplotlib.pyplot as plt
import torch
from diffusers import (
    DDIMPipeline,
    DDIMScheduler,
    DDPMPipeline,
    DDPMScheduler,
    UNet2DModel,
)
from IPython.display import HTML
from IPython.utils import io
from tqdm import tqdm

from .augmentation import GlobalAveragePooling, ResNetWithGlobalPoolingInitialConv
from .ddpm import DDPM, get_dls, train
from .fid import ImageEval
from .learner import def_device
from .utils import show_images

# %% ../nbs/20_ddim.ipynb 8
@torch.no_grad()
def sample(ddpm, model, n=16, device=def_device, return_all=False):
    sz = (n, 1, 32, 32)
    ᾱ, ɑ, σ = ddpm.ᾱ.to(device), ddpm.ɑ.to(device), ddpm.σ.to(device)
    x_t = torch.randn(sz, device=device)
    bs, *_ = sz
    preds = []
    iter_ = list(reversed(range(1, ddpm.n_steps)))
    for t in tqdm(iter_, unit="time step"):
        # Predict the noise for each example in the image
        if t % 3 == 0 or t < 50:
            t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)
            noise_pred = model(x_t, t_batch).sample

        # Predict the image without noise
        x_0_pred = x_t - (1 - ɑ[t]) / torch.sqrt(1 - ᾱ[t]) * noise_pred

        # Add noise to the predicted noiseless image such that it ulimately
        # has slightly less noise than before
        x_t_minus_1 = x_0_pred / ɑ[t].sqrt() + (σ[t] * torch.randn(sz, device=device))

        # Repeat
        x_t = x_t_minus_1
        preds.append((x_t, x_0_pred))

    # At the last step, simply rescale and do not add noise
    x_0 = x_0_pred / ɑ[0].sqrt()
    preds.append((x_0, x_0))

    if return_all:
        x_ts, x_0s = zip(*preds)
        return x_ts, x_0s
    return x_0

# %% ../nbs/20_ddim.ipynb 10
def animate(imgs):
    fig, axes = plt.subplots(2, 4, figsize=(4, 2))

    def animate(i):
        for ax, im in zip(axes.flatten(), imgs[i]):
            ax.clear()
            ax.imshow(im.cpu().squeeze())
            ax.set_axis_off()
        fig.tight_layout()

    ani = matplotlib.animation.FuncAnimation(
        fig, animate, frames=len(imgs), interval=10.0
    )

    return HTML(ani.to_jshtml())

# %% ../nbs/20_ddim.ipynb 15
@torch.no_grad()
def diffusers_sample(sched, sz=(512, 1, 32, 32), skip_steps=None, **kwargs):
    if skip_steps is None:
        skip_steps = []
    x_t = torch.randn(sz).to(def_device)
    for t in tqdm(sched.timesteps):
        if t not in skip_steps:
            noise = unet(x_t, t).sample
        x_t = sched.step(noise, t, x_t).prev_sample
    return x_t

# %% ../nbs/20_ddim.ipynb 25
@dataclass
class DDIMOutput:
    prev_sample: torch.Tensor


class DiffusersStyleDDPM(DDPM):
    @property
    def timesteps(self):
        return range(self.n_steps - 1, 0, -1)

    def step(self, noise, t, x_t):
        raise NotImplementedError


class DiffusersStyleDDIM(DiffusersStyleDDPM):
    def __init__(self, n_steps=1000, βmin=0.0001, βmax=0.02, η=1.0):  # η is eta
        super().__init__(n_steps, βmin, βmax)
        self.η = η

    def step(self, noise, t, x_t):
        ᾱ, η = self.ᾱ, self.η

        # Determine \hat{x}_0
        x_0_pred = (x_t - (1 - ᾱ[t]).sqrt() * noise) / ᾱ[t].sqrt()

        # Determine the direction towards x_t
        σ_t = η * ((1 - ᾱ[t - 1]) / (1 - ᾱ[t])).sqrt() * (1 - ᾱ[t] / ᾱ[t - 1]).sqrt()
        x_t_direction = (1 - ᾱ[t - 1] - σ_t**2).sqrt() * noise

        # Determine the x_{t-1} without nosie
        prev_sample = ᾱ[t - 1].sqrt() * x_0_pred + x_t_direction

        # Add random noise, if needed
        if t > 0:
            prev_sample += σ_t * torch.randn(x_t.shape).to(x_t.device)

        return DDIMOutput(prev_sample=prev_sample)
