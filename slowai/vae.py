# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/30_vae.ipynb.

# %% auto 0
__all__ = ['Perceptron', 'KaimingMixin', 'VAE', 'kld_loss', 'vae_loss', 'MeanlikeMetric', 'KLDMetric', 'BCEMetric',
           'MetricsCBWithKLDAndBCE', 'VAETrainCB', 'train', 'FashionMNISTForReconstruction']

# %% ../nbs/30_vae.ipynb 3
from functools import partial

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from einops import rearrange
from torch import nn
from torch.nn import init
from torch.optim import lr_scheduler
from torchmetrics import Metric

from .attention import ConditionalTAUnet, conditional_train
from .cos_revisited import aesthetics
from .ddpm import get_dls as get_fashion_dls
from slowai.learner import (
    Callback,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    def_device,
    fashion_mnist,
    only,
    to_cpu,
)
from .sgd import BatchSchedulerCB
from .super_rez import KaimingMixin
from .tinyimagenet_a import denorm, get_imagenet_dls
from .utils import show_images

# %% ../nbs/30_vae.ipynb 5
class Perceptron(nn.Sequential):
    def __init__(self, c_in, c_out, bias=True, act=nn.SiLU):
        layers = [nn.Linear(c_in, c_out, bias=bias)]
        if act:
            layers.append(act())
        layers.append(nn.BatchNorm1d(c_out))
        super().__init__(*layers)

# %% ../nbs/30_vae.ipynb 6
class KaimingMixin:
    """Helper to initialize the network using Kaiming"""

    @staticmethod
    def init_kaiming(m):
        if isinstance(m, torch.nn.Linear):
            torch.nn.init.kaiming_normal_(m.weight, a=0.2)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)

    @classmethod
    def kaiming(cls, *args, **kwargs):
        model = cls(*args, **kwargs)
        model.apply(cls.init_kaiming)
        return model

# %% ../nbs/30_vae.ipynb 7
class VAE(nn.Module, KaimingMixin):
    """Variational autoencoder"""

    def __init__(self, c_in, c_hidden, c_bottleneck, layers=1):
        super().__init__()
        self.c_in = c_in
        self.c_hidden = c_hidden
        self.c_bottleneck = c_bottleneck

        middle_encoder = [Perceptron(c_hidden, c_hidden) for _ in range(layers)]
        self.encoder = nn.Sequential(Perceptron(c_in, c_hidden), *middle_encoder)
        self.mu = Perceptron(c_hidden, c_bottleneck, act=None)
        self.log_sigma = Perceptron(c_hidden, c_bottleneck, act=None)

        middle_decoder = [Perceptron(c_hidden, c_hidden) for _ in range(layers)]
        self.decoder = nn.Sequential(
            Perceptron(c_bottleneck, c_hidden),
            *middle_decoder,
            Perceptron(c_hidden, c_in, act=None)
        )

    def forward(self, x, _y=None):
        bs, *_ = x.shape
        x = self.encoder(x)
        μ = self.mu(x)
        log_σ = self.log_sigma(x)

        ε = torch.randn(bs, self.c_bottleneck, device=x.device)
        # Reconstruct the image by taking some noise, modulated by a learned variance,
        # and adding it to a learned mean
        z = μ + (0.5 * log_σ).exp() * ε
        # We return the reconstructed result, but except for training we are only
        # interested in the hidden activations
        return self.decoder(z), μ, log_σ

# %% ../nbs/30_vae.ipynb 9
def kld_loss(μ, log_σ, eps=0, dim=None):
    return -0.5 * (1 + log_σ - μ.pow(2) - (log_σ + eps).exp()).mean(dim)

# %% ../nbs/30_vae.ipynb 12
def vae_loss(inputs, x_pred, μ, log_σ):
    (x, _) = inputs
    kld_loss_ = kld_loss(μ, log_σ)
    bce_loss_ = F.binary_cross_entropy_with_logits(x_pred, x)
    loss = kld_loss_ + bce_loss_
    return loss

# %% ../nbs/30_vae.ipynb 14
class MeanlikeMetric(Metric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.add_state("total", default=torch.tensor(0.0), dist_reduce_fx="sum")
        self.add_state("n", default=torch.tensor(0), dist_reduce_fx="sum")

    def update(self, *args, **kwargs):
        self.total += self.delta(*args, **kwargs)
        self.n += 1

    def delta(self):
        raise NotImplementedError

    def compute(self):
        return self.total / self.n

# %% ../nbs/30_vae.ipynb 15
class KLDMetric(MeanlikeMetric):
    def delta(self, μ, log_σ):
        return kld_loss(μ, log_σ).to(self.device)

# %% ../nbs/30_vae.ipynb 16
class BCEMetric(MeanlikeMetric):
    def delta(self, x, x_pred):
        return F.binary_cross_entropy_with_logits(x_pred, x).to(self.device)

# %% ../nbs/30_vae.ipynb 17
class MetricsCBWithKLDAndBCE(MetricsCB):
    def __init__(self, *ms, **metrics):
        super().__init__(*ms, **metrics)
        self.all_metrics["kld"] = self.kld = KLDMetric()
        self.all_metrics["bce"] = self.bce = BCEMetric()

    def after_batch(self, learn):
        super().after_batch(learn)
        x, _ = learn.batch
        x_pred, μ, log_σ = learn.preds
        self.kld.update(μ, log_σ)
        self.bce.update(x, x_pred)

# %% ../nbs/30_vae.ipynb 18
class VAETrainCB(TrainCB):
    def get_loss(self, learn):
        x_pred, μ, log_σ = learn.preds
        learn.loss = learn.loss_func(learn.batch, x_pred, μ, log_σ)

    def backward(self, learn):
        learn.loss.backward()
        torch.nn.utils.clip_grad_norm_(learn.model.parameters(), max_norm=1.0)

# %% ../nbs/30_vae.ipynb 19
def train(model, dls, lr=4e-3, n_epochs=4, extra_cbs=[], loss_fn=vae_loss):
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        MetricsCBWithKLDAndBCE(),
        DeviceCB(),
        VAETrainCB(),
        *extra_cbs,
    ]
    learner = Learner(
        model,
        dls,
        loss_fn,
        lr=lr,
        cbs=cbs,
        opt_func=partial(torch.optim.AdamW, eps=1e-5),
    )
    learner.fit(n_epochs)
    return model

# %% ../nbs/30_vae.ipynb 24
class FashionMNISTForReconstruction(Callback):
    def before_batch(self, learn):
        x, y = learn.batch
        x = rearrange(x, "b c h w -> b (c h w)")
        learn.batch = (x, y)
