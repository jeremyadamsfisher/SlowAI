# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/27_diffusion_unet.ipynb.

# %% auto 0
__all__ = ['timestep_embedding', 'Conv', 'EmbeddingPreactResBlock', 'SaveTimeActivationMixin', 'TResBlock', 'TDownblock',
           'TUpblock', 'TimeEmbeddingMLP', 'TUnet', 'FashionDDPM', 'train', 'ddpm']

# %% ../nbs/27_diffusion_unet.ipynb 2
import math
from functools import partial
from pathlib import Path
from pdb import set_trace as st

import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from torch import nn, tensor
from torch.optim import AdamW, lr_scheduler
from tqdm import tqdm

from .activations import StoreModuleStatsCB
from .coco import get_coco_dataset_super_rez
from .cos_revisited import aesthetics, denoisify, noisify
from .ddpm import get_dls as get_fashion_dls
from slowai.learner import (
    Callback,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    def_device,
)
from .sgd import BatchSchedulerCB
from slowai.super_rez import (
    KaimingMixin,
    ResidualConvBlock,
    denorm,
    get_imagenet_super_rez_dls,
)
from .utils import show_image, show_images

# %% ../nbs/27_diffusion_unet.ipynb 17
def timestep_embedding(ts, emb_dim, max_period=10_000):
    exponent = -math.log(max_period) * torch.linspace(
        0, 1, emb_dim // 2, device=ts.device
    )
    embedding = ts[:, None].float() * exponent.exp()[None, :]
    embedding = torch.cat([embedding.sin(), embedding.cos()], dim=-1)
    return embedding

# %% ../nbs/27_diffusion_unet.ipynb 23
class Conv(nn.Module):
    def __init__(self, c_in, c_out, ks=3, stride=1):
        super().__init__()
        self.norm = nn.BatchNorm2d(c_in)
        self.act = nn.ReLU()
        self.conv = nn.Conv2d(
            c_in,
            c_out,
            stride=stride,
            kernel_size=ks,
            padding=ks // 2,
            bias=False,
        )

    def forward(self, x):
        x = self.norm(x)
        x = self.act(x)
        x = self.conv(x)
        return x

# %% ../nbs/27_diffusion_unet.ipynb 24
class EmbeddingPreactResBlock(nn.Module):
    def __init__(self, t_embed, c_in, c_out, ks=3, stride=2):
        super().__init__()
        self.t_embed = t_embed
        self.c_in = c_in
        self.c_out = c_out

        self.t_emb_proj = nn.Linear(t_embed, c_out * 2)
        self.conv_a = Conv(c_in, c_out)
        self.conv_b = Conv(c_out, c_out)
        if c_in != c_out:
            self.id_conv = nn.Conv2d(c_in, c_out, kernel_size=1)
        else:
            self.id_conv = None

    def forward(self, x_orig, t_emb):
        # non-residual link
        x = self.conv_a(x_orig)
        t_emb = self.t_emb_proj(F.relu(t_emb))[:, :, None, None]
        scale, shift = torch.chunk(t_emb, 2, dim=1)
        x = x * (1 + scale) + shift
        x = self.conv_b(x)

        # residual link
        xr = self.id_conv(x_orig) if self.id_conv else x_orig

        return x + xr

# %% ../nbs/27_diffusion_unet.ipynb 25
class SaveTimeActivationMixin:
    def forward(self, x, t):
        self.output = super().forward(x, t)
        return self.output

# %% ../nbs/27_diffusion_unet.ipynb 26
class TResBlock(
    SaveTimeActivationMixin,
    EmbeddingPreactResBlock,
):
    ...

# %% ../nbs/27_diffusion_unet.ipynb 27
class TDownblock(nn.Module):
    def __init__(self, t_embed, c_in, c_out, downsample=True, n_layers=1):
        super().__init__()
        self.t_embed = t_embed
        self.c_in = c_in
        self.c_out = c_out
        self.downsample = downsample
        self.n_layers = n_layers

        self.convs = nn.ModuleList()
        self.convs.append(TResBlock(t_embed, c_in, c_out, stride=1))
        for _ in range(n_layers - 1):
            self.convs.append(TResBlock(t_embed, c_out, c_out, stride=1))
        self.downsampler = nn.Conv2d(c_out, c_out, kernel_size=3, stride=2, padding=1)

    def forward(self, x, t):
        for conv in self.convs:
            x = conv(x, t)
        if self.downsample:
            x = self.downsampler(x)
        return x

# %% ../nbs/27_diffusion_unet.ipynb 28
class TUpblock(nn.Module):
    def __init__(self, t_embed, c_in, c_out, upsample=True, n_layers=1):
        super().__init__()
        self.t_embed = t_embed
        self.c_in = c_in
        self.c_out = c_out
        self.upsample = upsample
        self.n_layers = n_layers

        self.upsampler = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2d(c_in, c_in, kernel_size=3, padding=1),
        )
        self.convs = nn.ModuleList()
        for _ in range(n_layers - 1):
            self.convs.append(TResBlock(t_embed, c_in * 2, c_in, stride=1))
        self.convs.append(TResBlock(t_embed, c_in * 2, c_out, stride=1))

    @classmethod
    def from_downblock(cls, downblock):
        return cls(
            t_embed=downblock.t_embed,
            c_in=downblock.c_out,
            c_out=downblock.c_in,
            upsample=downblock.downsample,
            n_layers=downblock.n_layers,
        )

    def forward(self, x, downblock, t):
        if self.upsample:
            x = self.upsampler(x)
        for up, down in zip(self.convs, reversed(downblock.convs)):
            x = up(torch.cat((x, down.output), dim=1), t)
        return x

# %% ../nbs/27_diffusion_unet.ipynb 29
class TimeEmbeddingMLP(nn.Module):
    def __init__(self, c_in, c_out):
        super().__init__()
        self.c_in = c_in
        self.c_out = c_out
        self.time_emb_mlp = nn.Sequential(
            nn.BatchNorm1d(c_in),
            nn.Linear(c_in, c_out),
            nn.ReLU(),
            nn.Linear(c_out, c_out),
        )

    def forward(self, t):
        # Look up the sin/cos embedding  of the time step
        x = timestep_embedding(t, self.c_in).to(t.device)
        # Allow the model to slightly modify the embeddings
        x = self.time_emb_mlp(x)
        return x


class TUnet(nn.Module, KaimingMixin):
    """Diffusion U-net with a diffusion time dimension"""

    def __init__(
        self,
        nfs=(224, 448, 672, 896),
        n_blocks=(3, 2, 2, 1, 1),
        color_channels=3,
    ):
        assert len(n_blocks) - 1 == len(nfs)
        super().__init__()

        self.time_embedding = TimeEmbeddingMLP(nfs[0], 4 * nfs[0])
        t_embed = self.time_embedding.c_out

        self.start = nn.Conv2d(
            color_channels, nfs[0], kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        )
        self.downblocks = nn.ModuleList()
        self.upblocks = nn.ModuleList()
        for c_in, c_out, n_layers in zip(nfs, nfs[1:], n_blocks):
            db = TDownblock(
                t_embed,
                c_in,
                c_out,
                n_layers=n_layers,
            )
            self.downblocks.append(db)
            self.upblocks.insert(0, TUpblock.from_downblock(db))
        self.middle = TResBlock(t_embed, nfs[-1], nfs[-1], stride=1)
        self.end = Conv(nfs[0], color_channels)

    def forward(self, x_t, t):
        t = self.time_embedding(t)
        x = self.start(x_t)
        for db in self.downblocks:
            x = db(x, t)
        x = self.middle(x, t)
        for ub, db in zip(self.upblocks, reversed(self.downblocks)):
            x = ub(x, db, t)
        return self.end(x)

# %% ../nbs/27_diffusion_unet.ipynb 30
class FashionDDPM(TrainCB):
    def before_batch(self, learn):
        x0, _ = learn.batch
        learn.batch = noisify(x0)

    def predict(self, learn):
        (x_t, t), _ = learn.batch
        learn.preds = learn.model(x_t, t)

    def get_loss(self, learn):
        _, epsilon = learn.batch
        learn.loss = learn.loss_func(learn.preds, epsilon)

# %% ../nbs/27_diffusion_unet.ipynb 31
def train(model, dls, lr=4e-3, n_epochs=25, extra_cbs=[], loss_fn=F.mse_loss):
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        MetricsCB(),
        DeviceCB(),
        FashionDDPM(),  # ðŸ‘ˆ
        *extra_cbs,
    ]
    learner = Learner(
        model,
        dls,
        loss_fn,
        lr=lr,
        cbs=cbs,
        opt_func=partial(torch.optim.AdamW, eps=1e-5),
    )
    learner.fit(n_epochs)
    return model

# %% ../nbs/27_diffusion_unet.ipynb 36
@torch.no_grad()
def ddpm(model, sz=(16, 1, 32, 32), device=def_device, n_steps=100):
    x_0s = []
    x_t = torch.randn(sz, device=device)
    ts = torch.linspace(1 - (1 / n_steps), 0, n_steps).to(device)
    for t, t_next in tqdm(zip(ts, ts[1:]), unit="time step", total=n_steps - 1):
        # Predict the noise for each example in the image

        bs, *_ = x_t.shape
        noise_pred = model(x_t, t.repeat(bs))

        # Predict the image without noise
        x_0_pred = denoisify(x_t, noise_pred, t)
        x_0s.append(x_0_pred)

        # Renoise
        (prev_sample, _), _ = noisify(x_0_pred, t_next)

        # Repeat
        x_t = prev_sample

    # At the last step, simply rescale and do not add noise
    t = tensor(0.0, device=device).repeat(bs)
    x_0 = denoisify(x_t, model(x_t, t), t)
    x_0s.append(x_0_pred)

    return x_0, x_0s
