# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_matrix_math_homework.ipynb.

# %% auto 0
__all__ = ['get_stable_diffusion', 'embed_prompt', 'pred_noise', 'StableDiffusionWithNegativePrompts']

# %% ../nbs/02_matrix_math_homework.ipynb 2
from typing import List

import torch
from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline
from .overview import TORCH_DEVICE, StableDiffusion

# %% ../nbs/02_matrix_math_homework.ipynb 6
def get_stable_diffusion(cls=StableDiffusion) -> StableDiffusion:
    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
    # Use a simple noising scheduler for the initial draft
    pipe.scheduler = LMSDiscreteScheduler(
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        num_train_timesteps=1000,
    )
    pipe = pipe.to(TORCH_DEVICE)
    pipe.enable_attention_slicing()
    return cls(
        tokenizer=pipe.tokenizer,
        text_encoder=pipe.text_encoder,
        scheduler=pipe.scheduler,
        unet=pipe.unet,
        vae=pipe.vae,
    )

# %% ../nbs/02_matrix_math_homework.ipynb 11
def embed_prompt(sd, prompt, max_length):
    prompt_tokens = sd.tokenizer(
        prompt,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        return_tensors="pt",
    )
    with torch.no_grad():
        return sd.text_encoder(
            prompt_tokens.input_ids.to(TORCH_DEVICE)
        ).last_hidden_state

# %% ../nbs/02_matrix_math_homework.ipynb 15
def pred_noise(sd, prompt_embedding, l, t, guidance_scale_pos, guidance_scale_neg):
    latent_model_input = torch.cat([l] * 3)  # note all 3 latents injected with prompt
    # Scale the initial noise by the variance required by the scheduler
    latent_model_input = sd.scheduler.scale_model_input(latent_model_input, t)
    with torch.no_grad():
        noise_pred = sd.unet(
            latent_model_input, t, encoder_hidden_states=prompt_embedding
        ).sample
    noise_pred_uncond, noise_pred_text_pos, noise_pred_text_neg = noise_pred.chunk(3)
    noise_pred = noise_pred_uncond
    noise_pred += guidance_scale_pos * (noise_pred_text_pos - noise_pred_uncond)
    noise_pred -= guidance_scale_neg * (noise_pred_text_neg - noise_pred_uncond)
    return noise_pred

# %% ../nbs/02_matrix_math_homework.ipynb 18
class StableDiffusionWithNegativePrompts(StableDiffusion):
    def embed_prompt(self, prompt, negative_prompt):
        orig_embedding = super().embed_prompt(prompt)
        _, max_length, _ = orig_embedding.shape
        neg_text_embeddings = embed_prompt(self, negative_prompt, max_length)
        return torch.cat([orig_embedding, neg_text_embeddings])

        def denoise(
            self,
            prompt_embedding,
            guidance_scale_pos,
            guidance_scale_neg,
            l,  # latents
            t,  # timestep
            i,  # global progress
        ):
            noise_pred = self.pred_noise(
                sd, prompt_embedding, l, t, guidance_scale_pos, guidance_scale_neg
            )
            return self.scheduler.step(noise_pred, t, l).prev_sample

    def __call__(
        self,
        prompt,
        negative_prompt,
        guidance_scale=7.5,
        neg_guidance_scale=2,
        n_inference_steps=30,
        as_pil=False,
    ):
        prompt_embedding = self.embed_prompt(prompt, negative_prompt)
        l = self.init_latents()
        self.init_schedule(n_inference_steps)
        # Note that the time steps aren't neccesarily 1, 2, 3, etc
        for i, t in tqdm(enumerate(self.scheduler.timesteps), total=n_inference_steps):
            # workaround for ARM Macs where float64's are not supported
            t = t.to(torch.float32).to(TORCH_DEVICE)
            l = self.denoise(
                prompt_embedding, guidance_scale, neg_guidance_scale, l, t, i
            )
        return decompress(l, vae, as_pil=as_pil)


StableDiffusionWithNegativePrompts.pred_noise = pred_noise
