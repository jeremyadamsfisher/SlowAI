# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/11_resnets.ipynb.

# %% auto 0
__all__ = ['GeneralReLU', 'Conv', 'init_leaky_weights', 'CNN', 'ResidualConvBlock', 'SummaryCB', 'summarize']

# %% ../nbs/11_resnets.ipynb 3
from functools import partial

import fastcore.all as fc
import matplotlib.pyplot as plt
import numpy as np
import timm
import torch
import torch.nn.functional as F
from einops import rearrange
from IPython.display import Latex, Markdown, display
from torch import nn
from torch.nn import init
from torch.optim import lr_scheduler
from torch.optim.lr_scheduler import OneCycleLR
from torchmetrics.classification import MulticlassAccuracy

from .activations import Hook, HooksCallback, set_seed
from .learner import CancelFitException, DeviceCB, TrainLearner, fashion_mnist
from .sgd import train_1cycle

# %% ../nbs/11_resnets.ipynb 6
class GeneralReLU(nn.Module):
    def __init__(self, a=0.1, b=0.4):
        """Generalized ReLU activation function with bias and leaky slope."""
        super().__init__()
        self.a = a
        self.b = b

    def forward(self, x):
        if self.a:
            x = F.leaky_relu(x, self.a)
        else:
            x = F.relu(x)
        if self.b:
            x -= self.b
        return x


class Conv(nn.Conv2d):
    """Convolutional block with norms and activations"""

    def __init__(self, c_in, c_out, stride=2, ks=3, act=True, norm=True):
        super().__init__(
            c_in,
            c_out,
            stride=stride,
            kernel_size=ks,
            padding=ks // 2,
            bias=not norm,
        )
        self.norm = nn.BatchNorm2d(c_out) if norm else None
        self.act = GeneralReLU() if act else None

    def forward(self, x):
        x = super(Conv, self).forward(x)
        if self.act:
            x = self.act(x)
        if self.norm:
            x = self.norm(x)
        return x


def init_leaky_weights(module):
    if isinstance(module, Conv):
        if module.act is not None:
            init.kaiming_normal_(module.weight, a=module.act.a)
        else:
            init.kaiming_normal_(module.weight)


class CNN(nn.Module):
    """6 layer convolutional neural network with GeneralReLU"""

    def __init__(self, nfs=(8, 16, 32, 64), n_outputs=10, block=None):
        super().__init__()
        self.nfs = nfs
        self.n_outputs = n_outputs
        self.layers = nn.Sequential(*self.get_layers(nfs, n_outputs, block))

    def get_layers(self, nfs, n_outputs=10, C=None):
        if C is None:
            C = Conv
        assert len(nfs) == 4
        layers = [C(1, 8, ks=5)]
        for c_in, c_out in zip(nfs, nfs[1:]):
            layers.append(C(c_in, c_out))
        layers.append(C(nfs[-1], n_outputs, act=False))
        return layers

    def forward(self, x):
        x = self.layers(x)
        bs, c, w, h = x.shape
        assert c == self.n_outputs
        assert w == h == 1
        return rearrange(x, "bs c w h -> bs (c w h)")

    @classmethod
    def kaiming(cls, *args, **kwargs):
        model = cls(*args, **kwargs)
        model.apply(init_leaky_weights)
        return model

# %% ../nbs/11_resnets.ipynb 16
class ResidualConvBlock(nn.Module):
    """Convolutional block with residual links"""

    def __init__(self, c_in, c_out, stride=2, ks=3, act=True, norm=True):
        super().__init__()
        self.conv_a = Conv(c_in, c_out, stride=1, ks=ks, act=act, norm=norm)
        self.conv_b = Conv(c_out, c_out, stride=stride, ks=ks, act=False, norm=norm)
        self.id_conv = nn.Conv2d(c_in, c_out, stride=1, kernel_size=1)
        self.act = GeneralReLU() if act else None

    def forward(self, x):
        x_orig = x.clone()
        # Downsample the original, if neccesary
        if self.conv_b.stride == (2, 2):
            x_orig = F.avg_pool2d(x_orig, kernel_size=2, ceil_mode=True)
        elif self.conv_b.stride[0] > 2 or self.conv_b.stride[1] > 2:
            raise ValueError
        else:
            assert self.conv_b.stride == (1, 1)
        # Add extra channels, if neccesary
        if self.conv_a.in_channels != self.conv_b.out_channels:
            x_orig = self.id_conv(x_orig)
        x = self.conv_a(x)
        x = self.conv_b(x)
        x += x_orig
        if self.act:
            x = self.act(x)
        return x

# %% ../nbs/11_resnets.ipynb 19
class SummaryCB(HooksCallback):
    """Summarize the model"""

    def __init__(self, mods=None, mod_filter=fc.noop):
        super().__init__(hook_cls=partial(Hook, f=self.shape))
        self.mods = mods
        self.mod_filter = fc.noop
        self.total = 0
        self.lines = [
            "| Type | Input | Output | N. params | MFlops |",
            "| -- | -- | -- | -- | -- |",
        ]

    def flops(self, x, w, h):
        """Estimate flops"""
        if x.dim() < 3:
            return x.numel()
        else:
            return x.numel() * w * h

    def shape(self, hook, module, input_, output_):
        i, o = input_[0], output_[0]
        *_, h, w = o.shape
        np = sum(p.numel() for p in module.parameters())
        self.total += np
        flops = sum(self.flops(p, h, w) for p in module.parameters()) / 1e6
        l = f"| {type(module).__name__} | {tuple(i.shape)} | {tuple(o.shape)} | {np:,} | {flops:.1f}"
        self.lines.append(l)

    def after_batch(self, learn):
        self.lines.append(f"| Total | | | {self.total:,} | |")
        display(Markdown("\n".join(self.lines)))
        self.cleanup_fit(learn)
        raise CancelFitException

# %% ../nbs/11_resnets.ipynb 20
def summarize(m, mods, dls=fashion_mnist(512)):
    TrainLearner(
        m,
        dls,
        F.cross_entropy,
        lr=0.0,
        cbs=[SummaryCB(mods=mods), DeviceCB()],
        opt_func=torch.optim.AdamW,
    ).fit()
