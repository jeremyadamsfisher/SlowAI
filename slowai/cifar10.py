# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/16_cifar10.ipynb.

# %% auto 0
__all__ = ['WandBDDPM', 'cifar10_unet', 'train']

# %% ../nbs/16_cifar10.ipynb 10
class WandBDDPM(MetricsCB, order=1000):
    def __init__(self, config, *ms, project=None, sample_f, **metrics):
        if project is None:
            raise ValueError("Please specify a wandb project")
        fc.store_attr()
        super().__init__(*ms, **metrics)

    def before_fit(self, learn):
        wandb.init(project=self.project, config=self.config)

    def after_fit(self, learn):
        wandb.finish()

    def _log(self, d, learn):
        if learn.model.training:
            wandb.log({"train_" + m: float(d[m]) for m in self.all_metrics})
        else:
            wandb.log({"val_" + m: float(d[m]) for m in self.all_metrics})
            wandb.log({"samples": self.sample_figure(learn)})
        print(d)

    @torch.no_grad()
    def sample_figure(self, learn):
        samples = self.sample_f()
        bs, *chw = samples.shape
        K = math.ceil(math.sqrt(bs))
        fig, axes = plt.subplots(K, K, figsize=(2 * K, 2 * K))
        for im, ax in zip(samples, axes.flat):
            show_image(im.clamp(0.0, 1.0), ax=ax)
        return fig

    def after_batch(self, learn):
        super().after_batch(learn)
        wandb.log({"loss": learn.loss})

# %% ../nbs/16_cifar10.ipynb 12
def cifar10_unet(small: bool):
    if small:
        return UNet2DModel(
            in_channels=3,
            out_channels=3,
            block_out_channels=(32, 64, 128, 256),
            norm_num_groups=8,
        )
    else:
        return UNet2DModel(in_channels=3, out_channels=3)

# %% ../nbs/16_cifar10.ipynb 14
def train(
    model,
    lr=1e-3,
    n_epochs=2,
    bs=256,
    opt_func=partial(torch.optim.Adam, eps=1e-5),
    extra_cbs=[],
    ddpm=DDPM(),
):
    dls = get_dls(bs)

    def sample():
        return ddpm.sample(model, sz=(9, 3, 32, 32), n_steps=100)

    wb = WandBDDPM(
        config={"lr": lr, "epochs": n_epochs, "bs": bs},
        project="cifar10-ddpm",
        sample_f=sample,
    )
    T_max = len(dls["train"]) * n_epochs

    def get_lr(recorder):
        return recorder.learn.opt.param_groups[0]["lr"]

    recorder = RecorderCB(lr=get_lr)
    recorder.order = 999
    cbs = [
        ddpm,
        wb,
        TrainCB(),
        MetricsCB(),
        DeviceCB(),
        ProgressCB(plot=True),
        BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max),
        recorder,
        *extra_cbs,
    ]
    Learner(
        model,
        dls,
        F.mse_loss,
        lr=lr,
        cbs=cbs,
        opt_func=opt_func,
    ).fit(n_epochs)
    recorder.plot()
    return ddpm
