# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12_augmentation.ipynb.

# %% auto 0
__all__ = ['train', 'ResNet', 'flops', 'hooks', 'summarize', 'GlobalAveragePooling', 'ResNetWithGlobalPooling',
           'ResNetWithGlobalPoolingInitialConv', 'RandCopy']

# %% ../nbs/12_augmentation.ipynb 3
import random
import re
from contextlib import contextmanager
from math import sqrt
from typing import Sequence

import matplotlib.pyplot as plt
import numpy as np
import timm
import torch
import torch.nn.functional as F
from einops import rearrange
from IPython.display import Markdown, display
from torch import distributions, nn
from torch.nn import init
from torch.optim import lr_scheduler
from torchmetrics.classification import MulticlassAccuracy
from torchvision import transforms

from .activations import StoreModuleStatsCB, set_seed
from .initializations import init_leaky_weights, set_seed
from slowai.learner import (
    DeviceCB,
    MetricsCB,
    ProgressCB,
    TrainLearner,
    batchify,
    def_device,
    fashion_mnist,
)
from .resnets import Conv, ResidualConvBlock
from .sgd import BatchSchedulerCB
from .utils import show_images

# %% ../nbs/12_augmentation.ipynb 6
def train(model, lr=1e-2, n_epochs=2, dls=fashion_mnist(512), extra_cbs=tuple()):
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        MetricsCB(MulticlassAccuracy(num_classes=10)),
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        *extra_cbs,
    ]
    learner = TrainLearner(
        model,
        dls,
        F.cross_entropy,
        lr=lr,
        cbs=cbs,
        opt_func=torch.optim.AdamW,
    )
    learner.fit(n_epochs)

# %% ../nbs/12_augmentation.ipynb 8
class ResNet(nn.Module):
    """Arbitrarily wide and deep residual neural network"""

    def __init__(self, nfs: Sequence[int] = [16, 32, 64, 128, 256, 512], n_outputs=10):
        super().__init__()
        self.nfs = nfs
        self.n_outputs = n_outputs
        self.layers = nn.Sequential(*self.get_layers(nfs, n_outputs))

    def get_layers(self, nfs, n_outputs=10):
        layers = [ResidualConvBlock(1, nfs[0], stride=1, ks=5)]
        for c_in, c_out in zip(nfs, nfs[1:]):
            layers.append(ResidualConvBlock(c_in, c_out))
        layers.append(ResidualConvBlock(nfs[-1], n_outputs, act=False))
        return layers

    def forward(self, x):
        x = self.layers(x)
        bs, c, w, h = x.shape
        assert c == self.n_outputs
        assert w == h == 1
        return rearrange(x, "bs c w h -> bs (c w h)")

    @classmethod
    def kaiming(cls, *args, **kwargs):
        model = cls(*args, **kwargs)
        model.apply(init_leaky_weights)
        return model

# %% ../nbs/12_augmentation.ipynb 11
def flops(x, w, h):
    """Estimate flops"""
    if x.dim() < 3:
        return x.numel()
    else:
        return x.numel() * w * h


@contextmanager
def hooks(mods, f):
    hooks_ = []
    for m in mods:
        h = m.register_forward_hook(f)
        hooks_.append(h)
    yield
    for h in hooks_:
        h.remove()


def summarize(m, mods, dls=fashion_mnist(8)):
    xb, _ = dls.peek()
    tp = 0
    tf = 0
    rows = ["|Type|Input|Output|N. params|MFlops|", "|--|--|--|--|--|"]

    if isinstance(mods, str):
        mods_ = []
        for name, mod in m.named_modules():
            if re.match(mods, type(mod).__name__):
                mods_.append(mod)
        mods = mods_

    def summarize_module(m, i, o):
        nonlocal tp, tf, rows
        (i,) = i
        *_, h, w = o.shape
        np = sum(p.numel() for p in m.parameters())
        tp += np
        nflops = sum(flops(o, h, w) for o in m.parameters()) / 1e6
        tf += nflops
        l = f"| {type(m).__name__} | {tuple(i.shape)} | {tuple(o.shape)} | {np:,} | {nflops:.1f}"
        rows.append(l)

    with hooks(mods, summarize_module):
        m(xb)

    rows.append(f"| Total | | | {tp:,} | |")
    display(Markdown("\n".join(rows)))

# %% ../nbs/12_augmentation.ipynb 14
class GlobalAveragePooling(nn.Module):
    def forward(self, x):
        bs, c, w, h = x.shape
        return x.mean((2, 3))

# %% ../nbs/12_augmentation.ipynb 15
class ResNetWithGlobalPooling(nn.Module):
    """Arbitrarily wide and deep residual neural network"""

    def __init__(self, nfs: Sequence[int] = [16, 32, 64, 128, 256, 512], n_outputs=10):
        super().__init__()
        self.nfs = nfs
        self.n_outputs = n_outputs
        self.layers = nn.Sequential(*self.get_layers(nfs, n_outputs))
        self.pool = GlobalAveragePooling()
        self.lin = nn.Linear(nfs[-1], n_outputs)
        self.norm = nn.BatchNorm1d(n_outputs)

    def get_layers(self, nfs, n_outputs=10):
        layers = [ResidualConvBlock(1, nfs[0], ks=5, stride=1)]
        for c_in, c_out in zip(nfs, nfs[1:]):
            block = ResidualConvBlock(c_in, c_out)
            layers.append(block)
        return layers

    def forward(self, x):
        x = self.layers(x)
        x = self.pool(x)
        x = self.lin(x)
        x = self.norm(x)
        return x

    @classmethod
    def kaiming(cls, *args, **kwargs):
        model = cls(*args, **kwargs)
        model.apply(init_leaky_weights)
        return model

# %% ../nbs/12_augmentation.ipynb 19
class ResNetWithGlobalPoolingInitialConv(ResNetWithGlobalPooling):
    """Arbitrarily wide and deep residual neural network"""

    def get_layers(self, nfs, n_outputs=10):
        layers = [Conv(1, nfs[0], ks=5, stride=1)]
        for c_in, c_out in zip(nfs, nfs[1:]):
            block = ResidualConvBlock(c_in, c_out)
            layers.append(block)
        return layers

# %% ../nbs/12_augmentation.ipynb 36
class RandCopy(nn.Module):
    def __init__(self, pct=0.2, max_num=4):
        super().__init__()
        self.pct = pct
        self.max_num = max_num

    def forward(self, x):
        x = x.clone()
        for i in range(random.randint(0, self.max_num)):
            *_, w, h = x.shape
            szx = int(self.pct * w)
            szy = int(self.pct * h)
            stx1 = int(random.random() * (1 - self.pct) * w)
            sty1 = int(random.random() * (1 - self.pct) * h)
            stx2 = int(random.random() * (1 - self.pct) * w)
            sty2 = int(random.random() * (1 - self.pct) * h)
            segment = x[..., stx2 : stx2 + szx, sty2 : sty2 + szy]
            x[..., stx1 : stx1 + szx, sty1 : sty1 + szy] = segment
        return x
