# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12_augmentation.ipynb.

# %% auto 0
__all__ = ['train', 'flops', 'hooks', 'summarize']

# %% ../nbs/12_augmentation.ipynb 3
import re
from contextlib import contextmanager
from functools import partial
from typing import Sequence

import fastcore.all as fc
import matplotlib.pyplot as plt
import numpy as np
import timm
import torch
import torch.nn.functional as F
from einops import rearrange
from IPython.display import Latex, Markdown, display
from torch import nn
from torch.nn import init
from torch.optim import lr_scheduler
from torch.optim.lr_scheduler import OneCycleLR
from torchmetrics.classification import MulticlassAccuracy
from torchvision import transforms

from slowai.activations import (
    Conv2dWithReLU,
    Hook,
    HooksCallback,
    StoreModuleStatsCB,
    set_seed,
)
from slowai.initializations import (
    BatchTransformCB,
    CNNWithGeneralReLUAndBatchNorm,
    GeneralReLU,
    LSUVHook,
    LSUVInitialization,
    init_leaky_weights,
    set_seed,
)
from slowai.learner import (
    Callback,
    CancelFitException,
    DataLoaders,
    DeviceCB,
    MetricsCB,
    MomentumCB,
    ProgressCB,
    TrainLearner,
    batchify,
    before,
    fashion_mnist,
)
from .resnets import CNN, Conv, ResidualConvBlock
from .sgd import BatchSchedulerCB, RecorderCB
from .utils import show_images

# %% ../nbs/12_augmentation.ipynb 6
def train(model, lr=1e-2, n_epochs=2, dls=fashion_mnist(512), extra_cbs=tuple()):
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        MetricsCB(MulticlassAccuracy(num_classes=10)),
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        *extra_cbs,
    ]
    learner = TrainLearner(
        model,
        dls,
        F.cross_entropy,
        lr=lr,
        cbs=cbs,
        opt_func=torch.optim.AdamW,
    )
    learner.fit(n_epochs)

# %% ../nbs/12_augmentation.ipynb 11
def flops(x, w, h):
    """Estimate flops"""
    if x.dim() < 3:
        return x.numel()
    else:
        return x.numel() * w * h


@contextmanager
def hooks(mods, f):
    hooks_ = []
    for m in mods:
        h = m.register_forward_hook(f)
        hooks_.append(h)
    yield
    for h in hooks_:
        h.remove()


def summarize(m, mods, dls=fashion_mnist(8)):
    xb, _ = dls.peek()
    tp = 0
    tf = 0
    rows = ["|Type|Input|Output|N. params|MFlops|", "|--|--|--|--|--|"]

    if isinstance(mods, str):
        mods_ = []
        for name, mod in m.named_modules():
            if re.match(mods, type(mod).__name__):
                mods_.append(mod)
        mods = mods_

    def summarize_module(m, i, o):
        nonlocal tp, tf, rows
        (i,) = i
        *_, h, w = o.shape
        np = sum(p.numel() for p in m.parameters())
        tp += np
        nflops = sum(flops(o, h, w) for o in m.parameters()) / 1e6
        tf += nflops
        l = f"| {type(m).__name__} | {tuple(i.shape)} | {tuple(o.shape)} | {np:,} | {nflops:.1f}"
        rows.append(l)

    with hooks(mods, summarize_module):
        m(xb)

    rows.append(f"| Total | | | {tp:,} | |")
    display(Markdown("\n".join(rows)))
