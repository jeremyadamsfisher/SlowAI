# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/16_cosine.ipynb.

# %% auto 0
__all__ = ['ᾱ', 'noisify', 'DDPM', 'train', 'denoisify', 'sample']

# %% ../nbs/16_cosine.ipynb 3
import math

import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from torch import tensor
from torch.optim import lr_scheduler
from tqdm import tqdm

from .ddpm import fashion_unet, get_dls
from slowai.learner import (
    Callback,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    after,
    def_device,
    only,
)
from .sgd import BatchSchedulerCB
from .utils import show_images

# %% ../nbs/16_cosine.ipynb 6
def ᾱ(t):
    assert (0 <= t).all() and (t <= 1).all()
    return ((t * math.pi / 2).cos() ** 2).clamp(0.0, 0.999)

# %% ../nbs/16_cosine.ipynb 9
def noisify(x_0, t=None):
    n, *_ = x_0.shape
    device = x_0.device

    if t is None:
        t = torch.rand((n,), device=device)

    # Sample 2D noise for each example in the batch
    ε = torch.randn(x_0.shape, device=device)

    # Add noise according to the equation in Algorithm 1, such
    # that the variance of the distribution does not change
    ᾱ_t = ᾱ(t).reshape(-1, 1, 1, 1).to(device)
    x_t = ᾱ_t.sqrt() * x_0 + (1 - ᾱ_t).sqrt() * ε

    return ((x_t, t), ε)

# %% ../nbs/16_cosine.ipynb 11
class DDPM(Callback, order=after(DeviceCB)):
    def before_batch(self, learn):
        x_0, _ = learn.batch
        learn.batch = noisify(x_0)

    @only
    def predict(self, learn):
        (x_t, t), _ = learn.batch
        learn.preds = learn.model(x_t, t).sample

# %% ../nbs/16_cosine.ipynb 12
def train(
    model,
    lr=4e-3,
    n_epochs=2,
    bs=128,
    opt_func=torch.optim.Adam,
    extra_cbs=[],
    ddpm=DDPM(),
):
    dls = get_dls(bs)
    T_max = len(dls["train"]) * n_epochs
    cbs = [
        ddpm,
        TrainCB(),
        MetricsCB(),
        DeviceCB(),
        ProgressCB(plot=True),
        BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max),
        *extra_cbs,
    ]
    Learner(
        model,
        dls,
        F.mse_loss,
        lr=lr,
        cbs=cbs,
        opt_func=opt_func,
    ).fit(n_epochs)
    return ddpm

# %% ../nbs/16_cosine.ipynb 16
def denoisify(x_t, noise, t):
    ᾱ_t = ᾱ(t).reshape(-1, 1, 1, 1)
    return (x_t - (1 - ᾱ_t).sqrt() * noise) / ᾱ_t.sqrt()

# %% ../nbs/16_cosine.ipynb 21
@torch.no_grad()
def sample(self, model, sz=(16, 1, 32, 32), device=def_device, n_steps=100):
    x_t = torch.randn(sz, device=device)
    ts = torch.linspace(1 - (1 / n_steps), 0, n_steps).to(device)
    for t, t_next in tqdm(zip(ts, ts[1:]), unit="time step", total=n_steps - 1):
        # Predict the noise for each example in the image
        noise_pred = model(x_t, t).sample

        # Predict the image without noise
        x_0_pred = denoisify(x_t, noise_pred, t)

        # Renoise
        (x_t_minus_1, _), _ = noisify(x_0_pred, t_next)

        # Repeat
        x_t = x_t_minus_1

    # At the last step, simply rescale and do not add noise
    t = tensor(0.0, device=device)
    x_0 = denoisify(x_t, model(x_t, t).sample, t)

    return x_t


DDPM.sample = sample
