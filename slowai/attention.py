# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/28_attention.ipynb.

# %% auto 0
__all__ = ['MultiheadSelfAttention1D', 'TADownblock', 'TAUpblock', 'conditional_train']

# %% ../nbs/28_attention.ipynb 3
import math
from functools import lru_cache, partial
from pathlib import Path
from pdb import set_trace as st

import matplotlib
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from einops import einsum, rearrange
from torch import nn, tensor
from torch.optim import AdamW, lr_scheduler
from tqdm import tqdm

from .activations import StoreModuleStatsCB
from .augmentation import GlobalAveragePooling, ResNetWithGlobalPoolingInitialConv
from .coco import get_coco_dataset_super_rez
from .cos_revisited import aesthetics, denoisify, noisify
from .ddim import fashion_unet
from .ddpm import DDPM
from .ddpm import get_dls
from .ddpm import get_dls as get_fashion_dls
from .ddpm import train
from slowai.diffusion_unet import (
    Conv,
    SaveTimeActivationMixin,
    TDownblock,
    TimeEmbeddingMLP,
    TUnet,
    TUpblock,
    ddpm,
    train,
)
from .fid import ImageEval
from slowai.learner import (
    Callback,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    after,
    def_device,
    only,
)
from .sgd import BatchSchedulerCB
from slowai.super_rez import (
    KaimingMixin,
    ResidualConvBlock,
    denorm,
    get_imagenet_super_rez_dls,
)
from .utils import show_image, show_images

# %% ../nbs/28_attention.ipynb 6
class MultiheadSelfAttention1D(nn.Module):
    def __init__(self, nc, nh):
        super().__init__()
        self.nc = nc
        self.nh = nh
        self.norm = nn.BatchNorm2d(nc)
        self.kqv = nn.Linear(nc, nc * 3)
        self.lin = nn.Linear(nc, nc)

    def forward(self, x):
        _, _, h, w = x.shape
        # Normalize
        x = self.norm(x)
        # Rasterize
        x = rearrange(x, "b c h w -> b (h w) c")
        # Project into a K, Q, V space
        x = self.kqv(x)
        # Divide into different heads of K, Q and V
        k, q, v = rearrange(x, "b t (nh hs k) -> k (b nh) t hs", nh=self.nh, k=3)
        # Compute affinity
        affinity_scores = k @ q.transpose(1, 2)
        affinity_scores /= math.sqrt(self.nc / self.nh)
        affinity = affinity_scores.softmax(dim=2)
        # Re-weight V by the affinity matrix
        x = affinity @ v
        # Concatenate the heads
        x = rearrange(x, "(b nh) t hs -> b t (nh hs)", nh=self.nh)
        # Linearly project (share information channel-wise instead of time-wise)
        x = self.lin(x)
        # Un-rasterize
        x = rearrange(x, "b (h w) c -> b c h w", h=h, w=w)
        return x

# %% ../nbs/28_attention.ipynb 12
class TADownblock(TDownblock):
    def __init__(self, t_embed, c_in, c_out, downsample=True, n_layers=1, nh=None):
        super().__init__(t_embed, c_in, c_out, downsample, n_layers)

        # Use the new Attention ResBlocks
        self.convs = nn.ModuleList()
        self.convs.append(TAResBlock(t_embed, c_in, c_out, stride=1, nh=nh))
        for _ in range(n_layers - 1):
            self.convs.append(TAResBlock(t_embed, c_out, c_out, stride=1, nh=nh))
        self.downsampler = nn.Conv2d(c_out, c_out, kernel_size=3, stride=2, padding=1)

# %% ../nbs/28_attention.ipynb 13
class TAUpblock(TUpblock):
    def __init__(self, t_embed, c_in, c_out, upsample=True, n_layers=1, nh=None):
        super().__init__(t_embed, c_in, c_out, upsample, n_layers)

        # Use the new Attention ResBlocks
        self.convs = nn.ModuleList()
        for _ in range(n_layers - 1):
            self.convs.append(TAResBlock(t_embed, c_in * 2, c_in, stride=1))
        self.convs.append(TAResBlock(t_embed, c_in * 2, c_out, stride=1))

# %% ../nbs/28_attention.ipynb 22
def conditional_train(
    model, dls, lr=4e-3, n_epochs=25, extra_cbs=[], loss_fn=F.mse_loss
):
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        MetricsCB(),
        DeviceCB(),
        ConditionalFashionDDPM(),  # ðŸ‘ˆ
        *extra_cbs,
    ]
    learner = Learner(
        model,
        dls,
        loss_fn,
        lr=lr,
        cbs=cbs,
        opt_func=partial(torch.optim.AdamW, eps=1e-5),
    )
    learner.fit(n_epochs)
    return model
