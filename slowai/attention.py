# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/28_attention.ipynb.

# %% auto 0
__all__ = ['MultiheadSelfAttention1D', 'TAResBlock', 'TADownblock', 'TAUpblock', 'TAUnet', 'ConditionalTAUnet',
           'ConditionalFashionDDPM', 'conditional_train', 'conditional_ddpm']

# %% ../nbs/28_attention.ipynb 3
import math
from functools import partial

import torch
import torch.nn.functional as F
from einops import rearrange
from torch import nn, tensor
from torch.optim import lr_scheduler
from tqdm import tqdm

from .cos_revisited import aesthetics, denoisify, noisify
from .ddpm import get_dls as get_fashion_dls
from .diffusion_unet import Conv, TDownblock, TUnet, TUpblock, ddpm, train
from .learner import DeviceCB, Learner, MetricsCB, ProgressCB, TrainCB, def_device
from .sgd import BatchSchedulerCB
from .utils import show_images

# %% ../nbs/28_attention.ipynb 6
class MultiheadSelfAttention1D(nn.Module):
    """Multi-head self-attention"""

    def __init__(self, nc, nh):
        super().__init__()
        self.nc = nc
        self.nh = nh
        self.norm = nn.BatchNorm2d(nc)
        self.kqv = nn.Linear(nc, nc * 3)
        self.lin = nn.Linear(nc, nc)

    def forward(self, x):
        _, _, h, w = x.shape
        # Normalize
        x = self.norm(x)
        # Rasterize
        x = rearrange(x, "b c h w -> b (h w) c")
        # Project into a K, Q, V space
        x = self.kqv(x)
        # Divide into different heads of K, Q and V
        k, q, v = rearrange(x, "b t (nh hs kqv) -> kqv (b nh) t hs", nh=self.nh, kqv=3)
        # Compute affinity
        affinity_scores = k @ q.transpose(1, 2)
        affinity_scores /= math.sqrt(self.nc / self.nh)
        affinity = affinity_scores.softmax(dim=2)
        # Re-weight V by the affinity matrix
        x = affinity @ v
        # Concatenate the heads
        x = rearrange(x, "(b nh) t hs -> b t (nh hs)", nh=self.nh)
        # Share information channel-wise (instead of time-wise)
        x = self.lin(x)
        # Un-rasterize
        x = rearrange(x, "b (h w) c -> b c h w", h=h, w=w)
        return x

# %% ../nbs/28_attention.ipynb 11
class TAResBlock(nn.Module):
    """Res-block with attention"""

    def __init__(self, t_embed, c_in, c_out, ks=3, stride=2, nh=None):
        super().__init__()
        self.t_embed = t_embed
        self.c_in = c_in
        self.c_out = c_out

        self.t_emb_proj = nn.Linear(t_embed, c_out * 2)
        self.conv_a = Conv(c_in, c_out)
        self.conv_b = Conv(c_out, c_out)

        if c_in != c_out:
            self.id_conv = nn.Conv2d(c_in, c_out, kernel_size=1)
        else:
            self.id_conv = None

        if nh:
            self.attn = MultiheadSelfAttention1D(c_out, nh)
        else:
            self.attn = None

    def forward(self, x_orig, t_emb):
        # non-residual link
        x = self.conv_a(x_orig)
        t_emb = self.t_emb_proj(F.relu(t_emb))[:, :, None, None]
        scale, shift = torch.chunk(t_emb, 2, dim=1)
        x = x * (1 + scale) + shift
        x = self.conv_b(x)

        # residual link
        xr = self.id_conv(x_orig) if self.id_conv else x_orig

        x = x + xr

        if self.attn:
            x = x + self.attn(x)

        self.output = x

        return x

# %% ../nbs/28_attention.ipynb 12
class TADownblock(TDownblock):
    """Resdownblock with attention"""

    def __init__(self, t_embed, c_in, c_out, downsample=True, n_layers=1, nh=None):
        super().__init__(t_embed, c_in, c_out, downsample, n_layers)

        # Use the new Attention ResBlocks
        self.convs = nn.ModuleList()
        self.convs.append(TAResBlock(t_embed, c_in, c_out, stride=1, nh=nh))
        for _ in range(n_layers - 1):
            self.convs.append(TAResBlock(t_embed, c_out, c_out, stride=1, nh=nh))
        self.downsampler = nn.Conv2d(c_out, c_out, kernel_size=3, stride=2, padding=1)

# %% ../nbs/28_attention.ipynb 13
class TAUpblock(TUpblock):
    """Resupblock with attention"""

    def __init__(self, t_embed, c_in, c_out, upsample=True, n_layers=1, nh=None):
        super().__init__(t_embed, c_in, c_out, upsample, n_layers)

        # Use the new Attention ResBlocks
        self.convs = nn.ModuleList()
        for _ in range(n_layers - 1):
            self.convs.append(TAResBlock(t_embed, c_in * 2, c_in, stride=1))
        self.convs.append(TAResBlock(t_embed, c_in * 2, c_out, stride=1))

# %% ../nbs/28_attention.ipynb 14
class TAUnet(TUnet):
    """U-net with attention up/down-blocks"""

    def __init__(
        self,
        nfs=(224, 448, 672, 896),
        attention_heads=(0, 8, 8, 8),
        n_blocks=(3, 2, 2, 1, 1),
        color_channels=3,
    ):
        assert len(n_blocks) == len(attention_heads) == len(nfs) + 1
        super().__init__(nfs, n_blocks, color_channels)
        self.downblocks = nn.ModuleList()
        self.upblocks = nn.ModuleList()
        for c_in, c_out, n_layers, nh in zip(nfs, nfs[1:], n_blocks, attention_heads):
            db = TADownblock(
                self.time_embedding.c_out,
                c_in,
                c_out,
                n_layers=n_layers,
                nh=nh,
            )
            self.downblocks.append(db)
            self.upblocks.insert(0, TAUpblock.from_downblock(db))

# %% ../nbs/28_attention.ipynb 21
class ConditionalTAUnet(TAUnet):
    def __init__(
        self,
        n_classes,
        nfs=(224, 448, 672, 896),
        attention_heads=(0, 8, 8, 8),
        n_blocks=(3, 2, 2, 1, 1),
        color_channels=3,
    ):
        super().__init__(nfs, attention_heads, n_blocks, color_channels)
        self.conditional_embedding = nn.Embedding(n_classes, self.time_embedding.c_out)

    def forward(self, x_t, t, c):
        t = self.time_embedding(t)
        c = self.conditional_embedding(c)  # ðŸ‘ˆ
        e = t + c
        x = self.start(x_t)
        for db in self.downblocks:
            x = db(x, t)
        x = self.middle(x, e)
        for ub, db in zip(self.upblocks, reversed(self.downblocks)):
            x = ub(x, db, e)
        return self.end(x)

# %% ../nbs/28_attention.ipynb 22
class ConditionalFashionDDPM(TrainCB):
    def before_batch(self, learn):
        x0, c = learn.batch
        (x_t, t), epsilon = noisify(x0)
        learn.batch = (x_t, t, c), epsilon

    def predict(self, learn):
        (x_t, t, c), _ = learn.batch
        learn.preds = learn.model(x_t, t, c)

    def get_loss(self, learn):
        _, epsilon = learn.batch
        learn.loss = learn.loss_func(learn.preds, epsilon)

# %% ../nbs/28_attention.ipynb 23
def conditional_train(
    model, dls, lr=4e-3, n_epochs=25, extra_cbs=[], loss_fn=F.mse_loss
):
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        MetricsCB(),
        DeviceCB(),
        ConditionalFashionDDPM(),  # ðŸ‘ˆ
        *extra_cbs,
    ]
    learner = Learner(
        model,
        dls,
        loss_fn,
        lr=lr,
        cbs=cbs,
        opt_func=partial(torch.optim.AdamW, eps=1e-5),
    )
    learner.fit(n_epochs)
    return model

# %% ../nbs/28_attention.ipynb 25
@torch.no_grad()
def conditional_ddpm(model, c, sz=(16, 1, 32, 32), device=def_device, n_steps=100):
    x_0s = []
    x_t = torch.randn(sz, device=device)
    c = c.to(device)
    ts = torch.linspace(1 - (1 / n_steps), 0, n_steps).to(device)
    for t, t_next in tqdm(zip(ts, ts[1:]), unit="time step", total=n_steps - 1):
        # Predict the noise for each example in the image
        bs, *_ = x_t.shape
        noise_pred = model(x_t, t.repeat(bs), c)

        # Predict the image without noise
        x_0_pred = denoisify(x_t, noise_pred, t)
        x_0s.append(x_0_pred)

        # Renoise
        (prev_sample, _), _ = noisify(x_0_pred, t_next)

        # Repeat
        x_t = prev_sample

    # At the last step, simply rescale and do not add noise
    t = tensor(0.0, device=device).repeat(bs)
    x_0 = denoisify(x_t, model(x_t, t, c), t)
    x_0s.append(x_0_pred)

    return x_0, x_0s
