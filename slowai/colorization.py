# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/26_colorization.ipynb.

# %% auto 0
__all__ = ['TinyUnetWithMultiresolutionOutputs', 'ColorizationLearner', 'train2']

# %% ../nbs/26_colorization.ipynb 2
from functools import partial

import torch
import torch.nn.functional as F
from torch import nn
from torch.optim import AdamW, lr_scheduler
from torchmetrics.classification import MulticlassAccuracy

from .coco import get_coco_dataset_colorization, get_coco_dataset_super_rez
from .convs import def_device
from .ddpm import AccelerateCB
from slowai.learner import (
    Callback,
    DataLoaders,
    DeviceCB,
    Learner,
    MetricsCB,
    ProgressCB,
    TrainCB,
    TrainLearner,
    only,
)
from .sgd import BatchSchedulerCB
from .super_rez import KaimingMixin, ResidualConvBlock
from .super_rez import TinyUnetWithCrossConvolutions as Unet
from .super_rez import UpsamplingResidualConvBlock, train, viz
from .tinyimagenet_a import denorm, lr_find
from .utils import show_image, show_images

# %% ../nbs/26_colorization.ipynb 24
class TinyUnetWithMultiresolutionOutputs(nn.Module, KaimingMixin):
    """U-net with outputs at multiple resolutions, enabling training
    that target these lower resolutions"""

    def __init__(
        self,
        nfs: list[int] = (32, 64, 128, 256, 512, 1024),
        n_blocks=(3, 2, 2, 1, 1),
    ):
        super().__init__()
        self.start = ResidualConvBlock(3, nfs[0], ks=5, stride=1)
        cs = list(zip(nfs, nfs[1:]))
        self.downsamplers = nn.ModuleList()
        for c_in, c_out in cs:
            ld = ResidualConvBlock(c_in, c_out, stride=2)
            self.downsamplers.append(ld)
        self.upsamplers = nn.ModuleList()
        self.adapters = nn.ModuleList()
        for c_out, c_in in reversed(cs):
            lu = UpsamplingResidualConvBlock(c_in, c_out, stride=1)
            self.upsamplers.append(lu)
            a = nn.Conv2d(c_out, 3, kernel_size=3, padding=1)
            self.adapters.append(a)
        self.final = ResidualConvBlock(nfs[0], 3, stride=1, with_final_activation=False)

    def forward(self, x):
        x = self.start(x)
        x_orig = x.clone()
        xis, xos = [], []
        for l in self.downsamplers:
            x = l(x)
            xis.append(x.clone())
        for xu, l, a in zip(reversed(xis), self.upsamplers, self.adapters):
            x = l(x + xu)
            if self.training:
                # project into color space
                xc = a(x)
                xos.append(xc)
        x = self.final(x + x_orig)
        return x, xos

# %% ../nbs/26_colorization.ipynb 25
class ColorizationLearner(TrainCB):
    def predict(self, learn):
        xb, _ = learn.batch
        learn.preds = learn.model(xb)

    def get_loss(self, learn):
        _, yb = learn.batch
        xp_final, xp_intermediaries = learn.preds
        loss = learn.loss_func(xp_final, yb)
        for xp_intermediary in xp_intermediaries:
            _, _, *size = xp_intermediary.shape
            yb_resized = F.interpolate(yb, size=size)
            loss += learn.loss_func(xp_final, yb)
        learn.loss = loss

# %% ../nbs/26_colorization.ipynb 26
def train2(
    model, dls, lr=4e-3, n_epochs=25, extra_cbs=[MetricsCB()], loss_fn=F.mse_loss
):
    """Train the Unet with multiple resolution outputs"""
    T_max = len(dls["train"]) * n_epochs
    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)
    cbs = [
        DeviceCB(),
        ProgressCB(plot=True),
        scheduler,
        ColorizationLearner(),
        *extra_cbs,
    ]
    learner = Learner(
        model,
        dls,
        loss_fn,
        lr=lr,
        cbs=cbs,
        opt_func=partial(torch.optim.AdamW, eps=1e-5),
    )
    learner.fit(n_epochs)
    return model
