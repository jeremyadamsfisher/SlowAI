[
  {
    "objectID": "tiny_imagenet_a.html",
    "href": "tiny_imagenet_a.html",
    "title": "Tiny Imagenet (Part I)",
    "section": "",
    "text": "Adapted from\nsource",
    "crumbs": [
      "Tiny Imagenet (Part I)"
    ]
  },
  {
    "objectID": "tiny_imagenet_a.html#how-much-better-can-we-do",
    "href": "tiny_imagenet_a.html#how-much-better-can-we-do",
    "title": "Tiny Imagenet (Part I)",
    "section": "How much better can we do?",
    "text": "How much better can we do?\n\nGoing deeper\nTo answer this, we can check Papers With Code. The best approach documented there is the ResNext paper at 72% accurate. This benchmark has shifted since the original course publication date, but here is the paper Jeremy refers to.\nSo, how much work is involved in going from 60% accurate to 70% accurate? ‚ÄúReal resnets‚Äù have multiple ResBlocks at the same feature map resolution.\n\nn_blocks = (3, 2, 2, 1, 1)\n\nThis specification has five downsampling layers but nine ResBlocks\n\nsource\n\n\nStackableResidualConvBlock\n\n StackableResidualConvBlock (n_blocks, c_in, c_out, conv_cls=&lt;class\n                             'slowai.resnets.ResidualConvBlock'&gt;)\n\n*A sequential container.\nModules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module it contains. It then ‚Äúchains‚Äù outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are each a registered submodule of the Sequential).\nWhat‚Äôs the difference between a Sequential and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like‚Äìa list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))*\nThis was a tricky module to implement. Make sure that the channel, height and width dimensions are apropriate for all values of n_blocks.\n\nclass TinyImageResNet2(TinyImageResNet1):\n    def __init__(\n        self,\n        nfs,\n        n_blocks,\n        n_outputs=10,\n        p_drop=0.1,\n    ):\n        self.n_blocks = n_blocks\n        super().__init__(nfs, n_outputs, p_drop)\n\n    def get_layers(self, nfs, n_outputs=10):\n        layers = [Conv(3, nfs[0], stride=1, ks=5)]\n        for c_in, c_out, n_blocks in zip(nfs, nfs[1:], self.n_blocks):\n            layers.append(StackableResidualConvBlock(n_blocks, c_in, c_out))\n        return layers\n\n\nmodel = TinyImageResNet2.kaiming(\n    n_outputs=200,\n    nfs=[32, 64, 128, 256, 512, 1024],\n    n_blocks=n_blocks,\n)\nsummarize(model, [*model.layers, model.lin, model.norm], dls)\n\n\n\n\n\n\n\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nConv\n(512, 3, 64, 64)\n(512, 32, 64, 64)\n2,464\n9.8\n\n\nStackableResidualConvBlock\n(512, 32, 64, 64)\n(512, 64, 32, 32)\n213,952\n218.1\n\n\nStackableResidualConvBlock\n(512, 64, 32, 32)\n(512, 128, 16, 16)\n541,952\n138.4\n\n\nStackableResidualConvBlock\n(512, 128, 16, 16)\n(512, 256, 8, 8)\n2,165,248\n138.4\n\n\nStackableResidualConvBlock\n(512, 256, 8, 8)\n(512, 512, 4, 4)\n3,672,576\n58.7\n\n\nStackableResidualConvBlock\n(512, 512, 4, 4)\n(512, 1024, 2, 2)\n14,685,184\n58.7\n\n\nLinear\n(512, 1024)\n(512, 200)\n204,800\n0.2\n\n\nBatchNorm1d\n(512, 200)\n(512, 200)\n400\n0.0\n\n\nTotal\n\n\n21,486,576\n622.416528\n\n\n\n\n\n\ntrain(model, dls, lr=0.1, n_epochs=25)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.068\n4.621\n0\ntrain\n\n\n0.101\n4.285\n0\neval\n\n\n0.160\n3.876\n1\ntrain\n\n\n0.187\n3.676\n1\neval\n\n\n0.226\n3.429\n2\ntrain\n\n\n0.221\n3.561\n2\neval\n\n\n0.277\n3.138\n3\ntrain\n\n\n0.222\n3.567\n3\neval\n\n\n0.312\n2.952\n4\ntrain\n\n\n0.283\n3.124\n4\neval\n\n\n0.339\n2.806\n5\ntrain\n\n\n0.269\n3.274\n5\neval\n\n\n0.360\n2.696\n6\ntrain\n\n\n0.225\n3.778\n6\neval\n\n\n0.383\n2.586\n7\ntrain\n\n\n0.290\n3.208\n7\neval\n\n\n0.400\n2.494\n8\ntrain\n\n\n0.293\n3.169\n8\neval\n\n\n0.419\n2.405\n9\ntrain\n\n\n0.324\n3.169\n9\neval\n\n\n0.437\n2.322\n10\ntrain\n\n\n0.356\n2.849\n10\neval\n\n\n0.454\n2.236\n11\ntrain\n\n\n0.291\n3.322\n11\neval\n\n\n0.471\n2.162\n12\ntrain\n\n\n0.352\n2.957\n12\neval\n\n\n0.490\n2.068\n13\ntrain\n\n\n0.326\n3.154\n13\neval\n\n\n0.510\n1.976\n14\ntrain\n\n\n0.352\n2.911\n14\neval\n\n\n0.534\n1.864\n15\ntrain\n\n\n0.390\n2.655\n15\neval\n\n\n0.560\n1.743\n16\ntrain\n\n\n0.442\n2.527\n16\neval\n\n\n0.589\n1.611\n17\ntrain\n\n\n0.455\n2.454\n17\neval\n\n\n0.620\n1.470\n18\ntrain\n\n\n0.498\n2.181\n18\neval\n\n\n0.658\n1.306\n19\ntrain\n\n\n0.514\n2.130\n19\neval\n\n\n0.704\n1.113\n20\ntrain\n\n\n0.529\n2.054\n20\neval\n\n\n0.751\n0.924\n21\ntrain\n\n\n0.583\n1.807\n21\neval\n\n\n0.803\n0.734\n22\ntrain\n\n\n0.599\n1.746\n22\neval\n\n\n0.838\n0.607\n23\ntrain\n\n\n0.613\n1.712\n23\neval\n\n\n0.860\n0.533\n24\ntrain\n\n\n0.614\n1.706\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYet more augmentation\n\nT.TrivialAugmentWide?\n\n\nInit signature:\nT.TrivialAugmentWide(\n    num_magnitude_bins: int = 31,\n    interpolation: torchvision.transforms.functional.InterpolationMode = &lt;InterpolationMode.NEAREST: 'nearest'&gt;,\n    fill: Optional[List[float]] = None,\n) -&gt; None\nDocstring:     \nDataset-independent data-augmentation with TrivialAugment Wide, as described in\n`\"TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation\" &lt;https://arxiv.org/abs/2103.10158&gt;`_.\nIf the image is torch Tensor, it should be of type torch.uint8, and it is expected\nto have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\nIf img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\nArgs:\n    num_magnitude_bins (int): The number of different magnitude values.\n    interpolation (InterpolationMode): Desired interpolation enum defined by\n        :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n        If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n    fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n        image. If given a number, the value is used for all bands respectively.\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/micromamba/envs/slowai/lib/python3.11/site-packages/torchvision/transforms/autoaugment.py\nType:           type\nSubclasses:     \n\n\n\n\ndls = get_imagenet_dls(bs=512, training_preprocessor=preprocess_and_trivial_augment)\nviz(dls)\n\n\n\n\n\n\n\n\n\nmodel = TinyImageResNet2.kaiming(\n    n_outputs=200,\n    nfs=[32, 64, 128, 256, 512, 1024],\n    n_blocks=n_blocks,\n)\nsummarize(model, [*model.layers, model.lin, model.norm], dls)\n\n\n\n\n\n\n\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nConv\n(512, 3, 64, 64)\n(512, 32, 64, 64)\n2,464\n9.8\n\n\nStackableResidualConvBlock\n(512, 32, 64, 64)\n(512, 64, 32, 32)\n213,952\n218.1\n\n\nStackableResidualConvBlock\n(512, 64, 32, 32)\n(512, 128, 16, 16)\n541,952\n138.4\n\n\nStackableResidualConvBlock\n(512, 128, 16, 16)\n(512, 256, 8, 8)\n2,165,248\n138.4\n\n\nStackableResidualConvBlock\n(512, 256, 8, 8)\n(512, 512, 4, 4)\n3,672,576\n58.7\n\n\nStackableResidualConvBlock\n(512, 512, 4, 4)\n(512, 1024, 2, 2)\n14,685,184\n58.7\n\n\nLinear\n(512, 1024)\n(512, 200)\n204,800\n0.2\n\n\nBatchNorm1d\n(512, 200)\n(512, 200)\n400\n0.0\n\n\nTotal\n\n\n21,486,576\n622.416528\n\n\n\n\n\n\ntrain(model, dls, lr=0.1, n_epochs=25)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.040\n4.902\n0\ntrain\n\n\n0.075\n4.431\n0\neval\n\n\n0.097\n4.338\n1\ntrain\n\n\n0.136\n4.074\n1\neval\n\n\n0.147\n3.954\n2\ntrain\n\n\n0.180\n3.691\n2\neval\n\n\n0.189\n3.684\n3\ntrain\n\n\n0.161\n4.008\n3\neval\n\n\n0.223\n3.488\n4\ntrain\n\n\n0.193\n3.875\n4\neval\n\n\n0.244\n3.360\n5\ntrain\n\n\n0.270\n3.188\n5\neval\n\n\n0.266\n3.237\n6\ntrain\n\n\n0.256\n3.541\n6\neval\n\n\n0.285\n3.128\n7\ntrain\n\n\n0.242\n3.555\n7\neval\n\n\n0.305\n3.023\n8\ntrain\n\n\n0.302\n3.071\n8\neval\n\n\n0.323\n2.937\n9\ntrain\n\n\n0.322\n2.903\n9\neval\n\n\n0.338\n2.860\n10\ntrain\n\n\n0.300\n3.106\n10\neval\n\n\n0.355\n2.770\n11\ntrain\n\n\n0.340\n2.840\n11\neval\n\n\n0.369\n2.690\n12\ntrain\n\n\n0.320\n3.068\n12\neval\n\n\n0.388\n2.598\n13\ntrain\n\n\n0.358\n2.851\n13\neval\n\n\n0.404\n2.512\n14\ntrain\n\n\n0.377\n2.662\n14\neval\n\n\n0.428\n2.402\n15\ntrain\n\n\n0.442\n2.354\n15\neval\n\n\n0.450\n2.293\n16\ntrain\n\n\n0.451\n2.319\n16\neval\n\n\n0.473\n2.178\n17\ntrain\n\n\n0.433\n2.439\n17\neval\n\n\n0.501\n2.047\n18\ntrain\n\n\n0.521\n1.995\n18\neval\n\n\n0.532\n1.894\n19\ntrain\n\n\n0.555\n1.840\n19\neval\n\n\n0.566\n1.747\n20\ntrain\n\n\n0.570\n1.761\n20\neval\n\n\n0.600\n1.602\n21\ntrain\n\n\n0.598\n1.657\n21\neval\n\n\n0.630\n1.464\n22\ntrain\n\n\n0.617\n1.567\n22\neval\n\n\n0.656\n1.358\n23\ntrain\n\n\n0.627\n1.526\n23\neval\n\n\n0.669\n1.307\n24\ntrain\n\n\n0.629\n1.521\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(model, \"../models/tiny_imagenet_200_classifier.pt\")\n\nContinued in part 2‚Ä¶",
    "crumbs": [
      "Tiny Imagenet (Part I)"
    ]
  },
  {
    "objectID": "ddim.html",
    "href": "ddim.html",
    "title": "Denoising Diffusion Implicit Modeling",
    "section": "",
    "text": "Adapted from\nimport matplotlib\n\nmatplotlib.rcParams[\"image.cmap\"] = \"gray_r\"",
    "crumbs": [
      "Denoising Diffusion Implicit Modeling"
    ]
  },
  {
    "objectID": "ddim.html#training-a-model",
    "href": "ddim.html#training-a-model",
    "title": "Denoising Diffusion Implicit Modeling",
    "section": "Training a model",
    "text": "Training a model\nTo start with, let‚Äôs train a model like the DDPM V3 notebook and try to achieve our best FID yet.\n\nsource\n\nfashion_unet\n\n fashion_unet ()\n\n\nfp = Path(\"../models/fashion_unet_2x.pt\")\nddpm = DDPM(Œ≤max=0.01)  # üëà reduce maximum beta\nif fp.exists():\n    unet = torch.load(fp)\nelse:\n    unet = fashion_unet()\n    train(\n        unet,\n        lr=1e-2,  # üëà increase the maximum learning rate\n        n_epochs=25,  # üëà dramatically increase the number of epochs\n        bs=128,\n        opt_func=partial(torch.optim.Adam, eps=1e-5),  # üëà increase Adam epsilon\n        ddpm=ddpm,\n    )\n    torch.save(unet, fp)\n\nWe also want a sampler that‚Äôs quite fast, so we‚Äôll re-use the predicted noise\n\nsource\n\n\nsample\n\n sample (ddpm, model, n=16, device='cpu', return_all=False)\n\n\nx_ts, x_0s = sample(ddpm, unet, return_all=True, n=256)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:29&lt;00:00, 34.38time step/s]\n\n\nCPU times: user 11.6 s, sys: 13 s, total: 24.6 s\nWall time: 29.2 s\n\n\n\nsource\n\n\nanimate\n\n animate (imgs)\n\n\nanimate([*x_0s[::25], x_0s[-1]])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImageEval.fashion_mnist?\n\n\nSignature: ImageEval.fashion_mnist(fp='../models/fashion_mnist_classifier.pt', bs=512)\nDocstring: &lt;no docstring&gt;\nFile:      ~/Desktop/SlowAI/nbs/slowai/fid.py\nType:      method\n\n\n\n\nimg_eval = ImageEval.fashion_mnist(bs=256)\nimg_eval.fid(x_0s[-1])\n\n936.2686767578125",
    "crumbs": [
      "Denoising Diffusion Implicit Modeling"
    ]
  },
  {
    "objectID": "ddim.html#diffusers-api",
    "href": "ddim.html#diffusers-api",
    "title": "Denoising Diffusion Implicit Modeling",
    "section": "Diffusers API",
    "text": "Diffusers API\nNow, for comparison, we‚Äôll use the diffusers API.\n\nsource\n\ndiffusers_sample\n\n diffusers_sample (sched, sz=(256, 1, 32, 32), skip_steps=None, **kwargs)\n\n\nsched = DDPMScheduler(beta_end=0.01)\nsched.set_timesteps((1000 - 50) // 3 + 50)\nx_t = diffusers_sample(sched)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:21&lt;00:00, 17.04it/s]\n\n\nCPU times: user 22.4 s, sys: 60.8 ms, total: 22.5 s\nWall time: 21.5 s\n\n\n\n\n\n\nshow_images(x_t[:8, ...], imsize=0.8)\n\n\n\n\n\n\n\n\n\nimg_eval.fid(x_t)\n\n1133.3358154296875\n\n\nFor DDIM:\n\nsched = DDIMScheduler(beta_end=0.01)\nsched.set_timesteps((1000 - 50) // 3 + 50)\nx_t = diffusers_sample(sched)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:21&lt;00:00, 16.98it/s]\n\n\nCPU times: user 22.5 s, sys: 69.2 ms, total: 22.6 s\nWall time: 21.6 s\n\n\n\n\n\n\nshow_images(x_t[:8, ...], imsize=0.8)\n\n\n\n\n\n\n\n\n\nimg_eval.fid(x_t)\n\n1313.16455078125\n\n\nIt turns out, these are similar quality.",
    "crumbs": [
      "Denoising Diffusion Implicit Modeling"
    ]
  },
  {
    "objectID": "ddim.html#ddim-algorithm",
    "href": "ddim.html#ddim-algorithm",
    "title": "Denoising Diffusion Implicit Modeling",
    "section": "DDIM, algorithm",
    "text": "DDIM, algorithm\nThe basic idea is that different time steps may benefit from having different amounts of noise.\nThis article does a good job of explaining the motivation.\nIn either algorithm, we determine the predicted noise, scale the predict apropriately for the time step, remove it from the latent representation and scale the sum.\n\\[\n\\hat{x}_0 = \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_{\\theta}(x_t)}{ \\sqrt{ \\bar{\\alpha}_t } }\n\\]\nThen, for DDPM, we re-add a fixed amount of noise to predict \\(x_{t-1}\\). For DDIM, we add noise as a function of \\(\\sigma\\). (Because this can be made stochastic or deterministic, but the training objective is compatible, the name was changed to Denoising Diffusion Implicit Model.)\n\\[\nq_\\sigma ( x_{t-1} | x_t, x_0 ) = \\mathcal{N} \\left(\n    \\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{ 1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2 } \\cdot \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{\\sqrt{1-\\bar{\\alpha_t}}},\n    \\sigma_t^2 I\n\\right)\n\\]\nWe can rewrite this in terms of \\(x_{t-1}\\), which is what we need to calculate for each step. This is composed of:\n\nPredicted \\(x_0 = \\left( \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_{\\theta}^{(t)} }{\\sqrt{ \\bar{\\alpha}_t }} \\right)\\) (this is the same as DDPM)\nDirection towards \\(x_t = \\sqrt{ 1 - \\bar{\\alpha}_{t-1} - \\sigma^{2}_{t} } \\cdot \\epsilon_{\\theta}^{(t)} (x_t)\\)\nRandom noise \\(= \\sigma_t \\epsilon_t\\)\n\n\\[\nx_{t-1} = \\sqrt{ \\bar{\\alpha}_{t-1} } \\left( \\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_{\\theta}^{(t)} }{\\sqrt{ \\bar{\\alpha}_t }} \\right) + \\sqrt{ 1 - \\bar{\\alpha}_{t-1} - \\sigma^{2}_{t} } \\cdot \\epsilon_{\\theta}^{(t)} (x_t) + \\sigma_t \\epsilon_t\n\\]\nand\n\\[\n\\begin{align*}\n\\sigma_t &= \\eta \\sqrt{(1-\\bar{\\alpha}_{t-1}) / (1-\\bar{\\alpha}_t)} \\sqrt{1-\\bar{\\alpha_t} / \\bar{\\alpha}_{t-1}} \\\\\n\\eta &\\in [0,1]\n\\end{align*}\n\\]\nTypically, we use an \\(\\eta\\) parameter to interpolate between DDPM and DDPM, where \\(\\eta=1\\) corresponds to DDIM; if \\(\\sigma_t=0\\), this corresponds to DDPM.\n\nsource\n\nDiffusersStyleDDIM\n\n DiffusersStyleDDIM (n_steps=1000, Œ≤min=0.0001, Œ≤max=0.02, Œ∑=1.0)\n\nModify the training behavior\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_steps\nint\n1000\n\n\n\nŒ≤min\nfloat\n0.0001\n\n\n\nŒ≤max\nfloat\n0.02\n\n\n\nŒ∑\nfloat\n1.0\nŒ∑ is eta\n\n\n\n\nsource\n\n\nDiffusersStyleDDPM\n\n DiffusersStyleDDPM (n_steps=1000, Œ≤min=0.0001, Œ≤max=0.02)\n\nModify the training behavior\n\nsource\n\n\nDDIMOutput\n\n DDIMOutput (prev_sample:torch.Tensor)\n\nThis is nice because the only parameters are \\(\\bar{\\alpha}\\) and \\(\\eta\\).\n\nsched = DiffusersStyleDDIM(Œ≤max=0.01)\nskip_steps = list(sched.timesteps)[:-50]\nskip_steps = skip_steps[1::3] + skip_steps[2::3]\nlen(skip_steps)\n\n632\n\n\n\nx_t = diffusers_sample(sched, skip_steps=skip_steps)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:22&lt;00:00, 44.14it/s]\n\n\nCPU times: user 23.5 s, sys: 123 ms, total: 23.6 s\nWall time: 22.6 s\n\n\n\n\n\n\nshow_images(x_t[:8, ...], imsize=0.8)\n\n\n\n\n\n\n\n\n\nimg_eval.fid(x_t)\n\n913.7852783203125\n\n\n\nsched = DiffusersStyleDDIM(Œ≤max=0.01)\nsteps = list(sched.timesteps)\nskip_steps = {step for step in steps if step not in steps[::10] and step &gt; 50}\nx_t = diffusers_sample(sched, skip_steps=skip_steps)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:09&lt;00:00, 102.68it/s]\n\n\nCPU times: user 10.7 s, sys: 72.3 ms, total: 10.7 s\nWall time: 9.73 s\n\n\n\n\n\n\nshow_images(x_t[:8, ...], imsize=0.8)\n\n\n\n\n\n\n\n\n\nimg_eval.fid(x_t)\n\n900.450927734375\n\n\nThis gives us a slight improvement in FID and a 2x increase in speed.",
    "crumbs": [
      "Denoising Diffusion Implicit Modeling"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Adapted from:\nimport math\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\nfrom torch import tensor\nfrom torch.distributions.multivariate_normal import MultivariateNormal\ntorch.manual_seed(42);",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "clustering.html#homework",
    "href": "clustering.html#homework",
    "title": "Clustering",
    "section": "Homework",
    "text": "Homework\nImplement DBSCAN\n\nX = data.clone()\n\n\nn, _ = X.shape\n\n\neps = 2.5\nmin_points = 25\n\n\nnorms = torch.pow(X[None, :] - X[:, None], 2).sum(axis=2).sqrt()\nnorms\n\ntensor([[ 0.0000,  4.6538,  4.8366,  ..., 48.0356, 51.1620, 45.6434],\n        [ 4.6538,  0.0000,  0.6129,  ..., 43.3835, 46.5297, 40.9945],\n        [ 4.8366,  0.6129,  0.0000,  ..., 43.2585, 46.4482, 40.8819],\n        ...,\n        [48.0356, 43.3835, 43.2585,  ...,  0.0000,  4.5282,  2.5374],\n        [51.1620, 46.5297, 46.4482,  ...,  4.5282,  0.0000,  5.9866],\n        [45.6434, 40.9945, 40.8819,  ...,  2.5374,  5.9866,  0.0000]])\n\n\n\ncluster_assignment = torch.zeros(n)\ncluster_assignment\n\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n\n\n\nfrom functools import lru_cache\nfrom typing import Optional\n\n\nclass DontReassignCluster(Exception):\n    ...\n\n\n@lru_cache\ndef core_point_neighbords(point_idx) -&gt; Optional:\n    dists = norms[point_idx, :]\n    neighboring_point_idxs = dists &lt; eps\n    if neighboring_point_idxs.sum() &gt; min_points:\n        return neighboring_point_idxs\n\n\ndef assign_cluster(cluster_id, point_idx):\n    if cluster_assignment[point_idx]:\n        raise DontReassignCluster\n\n    # First, assign point to cluster\n    cluster_assignment[point_idx] = cluster_id\n\n    # Then, if it is a core point, recursively assign the neighbors\n    dists = norms[point_idx, :]\n    neighboring_point_idxs = core_point_neighbords(point_idx)\n    if neighboring_point_idxs is not None:\n        for i, is_neighbor in enumerate(neighboring_point_idxs):\n            if is_neighbor:\n                try:\n                    assign_cluster(cluster_id, i)\n                except DontReassignCluster:\n                    continue\n\n\ncurrent_cluster_id = 1\n\nfor point_idx in range(n):\n    if core_point_neighbords(point_idx) is not None:\n        try:\n            assign_cluster(current_cluster_id, point_idx)\n        except DontReassignCluster:\n            continue\n        else:\n            current_cluster_id += 1\n\n\nfig, ax = plt.subplots(figsize=(4, 4))\nfor cluster in cluster_assignment.unique():\n    ax.scatter(\n        *X[cluster_assignment == cluster, :].T,\n        label=f\"Cluster {int(cluster)}\" if cluster else \"Noise\",\n    )\nax.legend();",
    "crumbs": [
      "Clustering"
    ]
  },
  {
    "objectID": "cosine.html",
    "href": "cosine.html",
    "title": "Cosine scheduler, revisited",
    "section": "",
    "text": "Adapted from\n\nhttps://www.youtube.com/watch?v=PXiD7ZjOKhA&t=2s\n\nThe idea we‚Äôll be exploring is the removing the concept of having \\(\\frac{t}{T}\\)\n\nsource\n\naesthetics\n\n aesthetics ()\n\nImprove the look and feel of our visualizations\n\naesthetics()\n\nOften in diffusion, we refer to time steps like so\nT = 1000\nfor t in range(T):\n    print(f\"Progress: {t/T}\")\nJeremy notes that we can simply use a progress variable \\(\\in [0, 1]\\). This allows us to simplify the \\(\\bar{\\alpha}\\) expression like so:\n\nsource\n\n\n·æ±\n\n ·æ± (t, reshape=True, device='cpu')\n\n\nx = torch.linspace(0, 1, 100)\nplt.plot(x, ·æ±(x, reshape=False));\n\n\n\n\n\n\n\n\nAnd, furthermore, the noisify function can be simplified like so:\n\nsource\n\n\nnoisify\n\n noisify (x_0, t=None)\n\nNow, (a) we don‚Äôt have to deal with knowing the time steps, computing \\(\\alpha\\) and \\(\\beta\\) and (b) the process is continuous.\nLet‚Äôs add this to the DDPM training callback. Notice that the constructor has been deleted.\n\nsource\n\n\nContinuousDDPM\n\n ContinuousDDPM (n_steps=1000, Œ≤min=0.0001, Œ≤max=0.02)\n\nModify the training behavior\n\ndls = get_dls()\nxb, _ = dls.peek()\nxb.shape  # Note: 32x32\n\ntorch.Size([128, 1, 32, 32])\n\n\nWe can use the parameters from our training run in the previous notebook.\n\nfp = Path(\"../models/fashion_unet_2x_continuous.pt\")\nddpm = ContinuousDDPM(Œ≤max=0.01)\nif fp.exists():\n    unet = torch.load(fp)\nelse:\n    unet = fashion_unet()\n    train(\n        unet,\n        lr=1e-2,\n        n_epochs=25,\n        bs=128,\n        opt_func=partial(torch.optim.Adam, eps=1e-5),\n        ddpm=ddpm,\n    )\n    torch.save(unet, fp)\n\n\nf\"{sum(p.numel() for p in unet.parameters() if p.requires_grad):,}\"\n\n'15,890,753'\n\n\nTo denoise, we need to reverse the noisification. Recall, for a given sample \\(x_0\\), the noised sample is defined as: x_t = ·æ±_t.sqrt() * x_0 + (1 - ·æ±_t).sqrt() * Œµ \\[\nx_t = \\sqrt{ \\bar{\\alpha}_t } x_0 + \\left( \\sqrt{ 1 - \\bar{\\alpha}_t } \\right) \\epsilon\n\\]\nThus,\n\\[\nx_0 = \\frac{ x_t - \\left( \\sqrt{ 1 - \\bar{\\alpha}_t } \\right) \\epsilon }{ \\sqrt{ \\bar{\\alpha}_t } }\n\\]\n\nsource\n\n\ndenoisify\n\n denoisify (x_t, noise, t)\n\n\n(x_t, ts), eps = noisify(xb)\nshow_images(x_t[:8], titles=[f\"{t.item():.2f}\" for t in ts[:8]])\n\n\n\n\n\n\n\n\nThis looks impressive for one step, but recall xb is part of the training data.\n\neps_pred = unet(x_t.to(def_device), ts.to(def_device)).sample\nx_0 = denoisify(x_t, eps_pred.cpu(), ts)\nshow_images(x_0[:8])\n\n\n\n\n\n\n\n\nFinally, we can rewrite the sampling algorithm without any \\(\\alpha\\)s or \\(\\beta\\)s. The only function we need is \\(\\bar{\\alpha}_t\\), which are part of the noisify and denoisify functions. This also means we can make the sampler a single, elegant function.\n\nsource\n\n\nddpm\n\n ddpm (model, sz=(16, 1, 32, 32), device='cpu', n_steps=100)\n\n\nx_0 = ddpm(unet, sz=(8, 1, 32, 32), n_steps=100)\nshow_images(x_0, imsize=0.8)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:01&lt;00:00, 67.69time step/s]\n\n\n\n\n\n\n\n\n\nThis code had a few bugs in it initially that led to deep-fried results.\n\ndenoisify was given torch.randn instead of noise_pred\nThe last denoising iteration was given t=1 instead of t=0\n\nLet‚Äôs try this with DDIM\n\nsource\n\n\nddim_noisify\n\n ddim_noisify (Œ∑, x_0_pred, noise_pred, t, t_next)\n\n\nsource\n\n\nddim\n\n ddim (model, sz=(16, 1, 32, 32), device='cpu', n_steps=100, eta=1.0,\n       noisify=&lt;function ddim_noisify&gt;)\n\n\nx_0 = ddim(unet, sz=(8, 1, 32, 32), n_steps=100)\nshow_images(x_0, imsize=0.8)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:01&lt;00:00, 70.26time step/s]\n\n\n\n\n\n\n\n\n\n\nbs = 128\neval = ImageEval.fashion_mnist(bs=bs)\n\n\nx_0 = ddim(unet, sz=(bs, 1, 32, 32), n_steps=100)\neval.fid(x_0)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:03&lt;00:00, 30.48time step/s]\n\n\n764.1282958984375\n\n\nDirectly comparing this to DDPM:\n\nx_0 = ddim(unet, sz=(bs, 1, 32, 32), n_steps=100, eta=0.0)  # eta=0 makes this DDPM\neval.fid(x_0)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:03&lt;00:00, 30.55time step/s]\n\n\n818.5997314453125\n\n\nand a real batch of data\n\nxb, _ = dls.peek(\"test\")\neval.fid(xb)\n\n158.8096923828125",
    "crumbs": [
      "Cosine scheduler, revisited"
    ]
  },
  {
    "objectID": "autoencoders.html",
    "href": "autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "Adapted from:\n\nhttps://youtu.be/0Hi2r4CaHvk?si=GA9KaGAnGOlS_NJO&t=3568\n\nAutoencoders learn a bottleneck representation that can be ‚Äúreversed‚Äù to reconstruct the original image.\n\nTypically, they are not used on their own but are used to produce compressed representations.\nWe‚Äôve seen how a convolutional neural network can produce a simple representation of an image: that is, the categorical probability distribution over all the fashion classes. How do reverse this process to reconstruct the original image.\nTranspose or ‚ÄúStride \\(\\frac{1}{2}\\)‚Äù convolutions work, but this notebook focuses on the nearest neighbor upsampling. This upsamples the activations from the previous layer and applies a convolutional layer to restore detail.\n\n\ndeconv\n\n deconv (c_in, c_out, ks=3, act=True)\n\nWe need to modify the fit function because the loss function is no longer of the label.\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False):\n    \"\"\"Modified fit function for reconstruction tasks\"\"\"\n    progress = tqdm if tqdm_ else lambda x: x\n    for epoch in range(epochs):\n        model.train()\n        trn_loss, trn_count = 0.0, 0\n        for xb, _ in progress(train_dl):\n            xb = to_device(xb)\n            loss = loss_func(model(xb), xb)  # üëà\n            bs, *_ = xb.shape\n            trn_loss += loss.item() * bs\n            trn_count += bs\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tst_loss, tot_acc, tst_count = 0.0, 0.0, 0\n            for xb, _ in progress(valid_dl):\n                xb = to_device(xb)\n                pred = model(xb)\n                bs, *_ = xb.shape\n                tst_count += bs\n                tst_loss += loss_func(pred, xb).item() * bs\n\n        print(\n            f\"{epoch=}: trn_loss={trn_loss / trn_count:.3f}, tst_loss={tst_loss / tst_count:.3f}\"\n        )\n\n\nsource\n\n\nget_model\n\n get_model ()\n\n\nautoencoder = get_model()\nautoencoder\n\nSequential(\n  (0): ZeroPad2d((2, 2, 2, 2))\n  (1): Sequential(\n    (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (2): Sequential(\n    (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (3): Sequential(\n    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (4): Sequential(\n    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n    (1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): ReLU()\n  )\n  (5): Sequential(\n    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n    (1): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): ReLU()\n  )\n  (6): Sequential(\n    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n    (1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (7): ZeroPad2d((-2, -2, -2, -2))\n  (8): Sigmoid()\n)\n\n\n\nwith fashion_mnist() as (_, tst_dl):\n    xb, _ = next(iter(tst_dl))\n\n\nassert xb.shape == autoencoder(xb.to(def_device)).shape\n\n\nmodel = get_model()\nwith fashion_mnist() as dls:\n    opt = optim.AdamW(model.parameters(), lr=0.01)\n    fit(10, model, F.mse_loss, opt, *dls)\n\nepoch=0: trn_loss=0.052, tst_loss=0.028\nepoch=1: trn_loss=0.024, tst_loss=0.021\nepoch=2: trn_loss=0.020, tst_loss=0.019\nepoch=3: trn_loss=0.019, tst_loss=0.018\nepoch=4: trn_loss=0.018, tst_loss=0.018\nepoch=5: trn_loss=0.018, tst_loss=0.018\nepoch=6: trn_loss=0.018, tst_loss=0.018\nepoch=7: trn_loss=0.017, tst_loss=0.017\nepoch=8: trn_loss=0.017, tst_loss=0.018\nepoch=9: trn_loss=0.017, tst_loss=0.017\n\n\n\npred = model(xb.to(def_device))\nshow_images([xb[0, ...].squeeze(), pred[0, ...].squeeze()])\n\n\n\n\n\n\n\n\nThat looks‚Ä¶not great.\nAt this point, Jeremy pauses to go over building a framework to iterate on this problem more quickly. Continued in the next notebook.",
    "crumbs": [
      "Autoencoders"
    ]
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Calculus and Backprop",
    "section": "",
    "text": "Adapted from:\nThe better calculus pedagogy is the calculus of infintesimals, that is: assume \\(f'(x) = \\frac{f(x + \\Delta x)-f(x)}{\\Delta x}\\) like normal and ignore second order infinitesimals (i.e., infinitesimals of infinitesimals). By doing so, the main rules of arithmetic suddenly also apply to calculus. For example:",
    "crumbs": [
      "Calculus and Backprop"
    ]
  },
  {
    "objectID": "calculus.html#neural-networks",
    "href": "calculus.html#neural-networks",
    "title": "Calculus and Backprop",
    "section": "Neural Networks",
    "text": "Neural Networks\nHow does this relate to deep learning? Suppose we wanted to model this function with a neural network.\n\nx = np.linspace(0, 10, 100)\ny = (x - 2.5) ** 2\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(x, y);\n\n\n\n\n\n\n\n\nA single line wouldn‚Äôt be especially adequate for this. (The sum of lines is just a line.) But what about the sum of rectified lines?\n\ny1 = np.clip(-3 * (x - 2), a_min=0, a_max=None)\ny2 = np.clip(3 * (x - 3), a_min=0, a_max=None)\ny3 = np.clip(6 * (x - 5), a_min=0, a_max=None)\n\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(6, 3))\nax0.plot(x, y)\nax0.plot(x, y1)\nax0.plot(x, y2)\nax0.plot(x, y3)\nax1.plot(x, y)\nax1.plot(x, y1 + y2 + y3);\n\n\n\n\n\n\n\n\nThis is the idea of neural networks (except, instead of lines, we‚Äôre dealing with hyperplanes).\n\nBasic architecture\nLet‚Äôs consider a specific problem. Suppose we wanted to classify digits with a simple neural network.\n\nsource\n\n\nMNISTDataModule\n\n MNISTDataModule (bs=128)\n\nMNIST data Module\nIt‚Äôs a bit cleaner to deal with images as vectors for this exercise.\n\nX_trn = rearrange(X_trn, \"b h w -&gt; b (h w)\")\n\nSay we wanted to classify a digit as a ‚Äúseven‚Äù (or not) based on a single pixel. A trained linear model would find some coefficient and you would draw some line dividing sevens from non-sevens.\nOf course, this is pretty limiting. What if this surface had a lot of curvature?\n\nx = np.linspace(0, 1, 100)\ny_t = 1 - x**2\ny_pred = -0.95 * x + 1.1\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(x, y_t, label=\"True\")\nax.plot(x, y_pred, label=\"Pred\")\nax.set(xlabel=\"Pixel Intensity\", ylabel=\"P(Seven)\")\nfig.legend();\n\n\n\n\n\n\n\n\nHow do fit the sum of rectified lines like before?\nTo make this more interesting, let‚Äôs consider using all pixels, for all images.\nLet‚Äôs define some parameters and helpers.\n\ndef relu(x):\n    return np.clip(x, a_min=0, a_max=None)\n\n\nn, m = X_trn.shape\nnh = 50  # num. hidden dimensions\nn, m\n\n(60000, 784)\n\n\nOur results are going to be non-sense here, but this gives us the right dimensions for everything.\n\nW0 = np.random.randn(m, nh)\nb0 = np.zeros(nh)\nW1 = np.random.randn(nh, 1)\nb1 = np.zeros(1)\n\nl0 = X_trn @ W0 + b0\nl1 = relu(l0)\ny_pred = l1 @ W1 + b1\ny_pred[:5]\n\narray([[ -9.87308795],\n       [-24.17324311],\n       [ 32.42885041],\n       [  6.0755439 ],\n       [-65.67629667]])\n\n\nLet‚Äôs compute an regression loss to get the model to predict the label. (This isn‚Äôt formally apropriate but it gives us the intuition.)\n\ny_pred.shape, y_true.shape\n\n((60000, 1), (60000,))\n\n\n\ndiff = y_pred.T - y_true\nmse = (diff**2).mean()\nmse\n\n1979.1768273281134\n\n\nEventually, at the end of the forward pass, we end up with a single number. We compute the loss for each example in the batch and take the batchwise mean or sum of the loss. Then, we use this along with the gradients with respect to each parameter to update the weights,\nLet‚Äôs calculate these derivates, one by one ü§©\n\n\nMean Squared Error\nFirst, we need to determine the gradient of the loss with respect to it‚Äôs input.\n\\[MSE = \\frac{ \\sum_{i=1}^{N} ( y^i-a^i )^2 }{N}\\]\nThis function composes an inner difference function and an outer square function. By the chain rule:\n\\[\n\\frac{d}{dx} f(g(x,y)) = f'(g(x,y)) g'(x,y)\n\\]\nLet \\(g(x,y) = x-y\\) and \\(f(x) = \\frac{x^2}{n}\\)\nThus, \\(\\frac{d}{dx} g(x,y)=\\frac{dx}{dx} - \\frac{dy}{dx}=1\\) and \\[\n\\begin{align*}\nf'(g(x, y)) & = f'((x-y)^2 / n) \\\\\n            & = f'((x^2 - 2xy + y^2)/n) \\\\\n            & = (2x - 2y) / n\n\\end{align*}\n\\]\nLet‚Äôs verify:\n\nassert n == 60000\nx, y = sympy.symbols(\"x y\")\nsympy.diff(((x - y) ** 2) / n, x)\n\n\\(\\displaystyle \\frac{x}{30000} - \\frac{y}{30000}\\)\n\n\nGreat. Now, to implement this in code, we need a way to store gradients on tensors themselves.\n\n@dataclass\nclass T:\n    \"\"\"Wrapper for numpy arrays to help store a gradient\"\"\"\n\n    value: Any\n    g: Any = None\n\n    def __getattr__(self, t):\n        return getattr(self.value, t)\n\n    def __getitem__(self, i):\n        return self.value[i]\n\n    @property\n    def v(self):\n        return self.value\n\nThen, we can implement it like so:\n\ndef mse_grad(y_pred: T, y_true: np.array):\n    diff = y_pred.squeeze() - y_true\n    y_pred_g = 2 * diff / n\n    y_pred.g = y_pred_g[:, None]\n\nContinuing on with the linear transformation layer, we‚Äôll review the mathematics then implement the code.\n\n\nLinear layer\nFor a linear layer, the gradient is derived like so:\nLet \\(L = loss(Y)\\) and \\(Y = WX+B\\) be a neural network. We want to compute the partial derivates of \\(L\\) with respect to \\(W\\), \\(B\\) and \\(X\\) to, ultimately, reduce \\(L\\).\n\nFor \\(X\\)‚Ä¶\nStarting with a single, \\(j\\)th parameter of the \\(i\\)th example, \\(x_j^i\\)\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial x_j^i} &= \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial x_j^i} \\\\\n                                  &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial x_j^i}\n\\end{align*}\n\\]\nNote that \\(\\frac{\\partial y^k}{\\partial x_j^i} = 0\\) if \\(k \\neq i\\) (i.e., the output of an example passed through the network is not a function of the input of a different example). Therefore,\n\\[\n\\begin{align*}\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial x_j^i}\n&= \\frac{\\partial L}{\\partial y^i} \\cdot \\frac{\\partial y^i}{\\partial x_j^i} \\\\\n&= \\frac{\\partial L}{\\partial y^i} \\cdot \\frac{\\partial w_j x_j^i +b}{\\partial x_j^i} \\\\\n&= \\frac{\\partial L}{\\partial y^i} w_j\n\\end{align*}\n\\]\nThe matrix of derivates for all \\(d\\) parameters (\\(j \\in \\{1, ..., d\\}\\)) and \\(N\\) examples (\\(i \\in \\{1, ..., N\\}\\)) is:\n\\[\n\\frac{\\partial L}{\\partial X} = \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y^1} w_1 & \\dots & \\frac{\\partial L}{\\partial y^1} w_d \\\\\n\\frac{\\partial L}{\\partial y^2} w_1 & \\dots & \\frac{\\partial L}{\\partial y^2} w_d \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial y^N} w_1 & \\dots & \\frac{\\partial L}{\\partial y^N} w_d \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y^1} \\\\\n\\frac{\\partial L}{\\partial y^2} \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial y^N} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1 & w_2 & \\dots & w_d\n\\end{bmatrix}\n\\]\nIn code: inp.g = out.g @ w.T\n\n\nFor \\(W\\)‚Ä¶\nStarting with a single parameter, \\(w_j\\):\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} &= \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial w_j} \\\\\n                                &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial w_j} \\\\\n                                &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial w_j x_j^k + b_j}{\\partial w_j} \\\\\n                                &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_j^k\n\\end{align*}\n\\]\nFor all parameters, \\(W\\):\n\\[\n\\frac{\\partial L}{\\partial W} = \\begin{bmatrix}\n\\frac{\\partial L}{\\partial w_{1}} \\\\\n\\frac{\\partial L}{\\partial w_{2}} \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial w_{d}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_1^k \\\\\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_2^k \\\\\n\\vdots \\\\\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_d^k\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y^1} x_1^1 &+ \\dots &+ \\frac{\\partial L}{\\partial y^N} x_1^N \\\\\n\\frac{\\partial L}{\\partial y^1} x_2^1 &+ \\dots &+ \\frac{\\partial L}{\\partial y^N} x_2^N \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial L}{\\partial y^1} x_d^1 &+ \\dots &+ \\frac{\\partial L}{\\partial y^N} x_d^N \\\\\n\\end{bmatrix} = \\begin{bmatrix}\nx_1^1 & \\dots & x^{N}_1 \\\\\nx_2^1 & \\dots & x^{N}_2 \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_d^1 & \\dots & x^{N}_d\n\\end{bmatrix}  \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y_{1}} \\\\\n\\frac{\\partial L}{\\partial y_{2}} \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial y_{N}} \\\\\n\\end{bmatrix} = X^T \\frac{\\partial L}{\\partial Y}\n\\]\nIn code: w.g = inp.v.T @ out.g\n\n\nFor \\(B\\)‚Ä¶\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial B} &= \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial B} \\\\\n                              &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial B} \\\\\n                              &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} (1) \\\\\n                               &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k}\n\\end{align*}\n\\]\nIn code b.g = out.g.sum(axis=0)\nYou can see that these derivation are similar to their single-variable counterparts (e.g.¬†\\(\\frac{dL}{dm} = \\frac{dL}{dy} \\cdot \\frac{dy}{dm} = \\frac{dL}{dy} \\cdot \\frac{dmx+b}{dm}=\\frac{dL}{dy}x\\)). However, the order of operations and the transposition is dictated by the matrix mathematics.\nIn code:\n\ndef lin_grad(inp, out, w, b):\n    inp.g = out.g @ w.T\n    w.g = inp.v.T @ out.g\n    b.g = out.g.sum(axis=0)\n\nMuch credit for my understanding goes to this blog bost and this article.\n\n\n\nReLU\nFor ReLU, we pass the upstream gradient downstream for any dimensions that contributed to the upstream signal. Note that this is an element-wise operation because the layer operates only on specific elements and has no global behavior.\n\ndef relu_grad(inp, out):\n    inp.g = (inp.value &gt; 0).astype(float) * out.g\n\nPutting it all together:\n\n# \"Tensorize\" the weights, biases and outputs\ntensors = (y_pred, W1, b1, l1, W0, b0, l0, X_trn)\nty_pred, tW1, tb1, tl1, tW0, tb0, tl0, tX_trn = map(T, tensors)\nmse_grad(ty_pred, y_true)\nty_pred.g[:5]\n\narray([[-0.00049577],\n       [-0.00080577],\n       [ 0.00094763],\n       [ 0.00016918],\n       [-0.00248921]])\n\n\n\nmse_grad(ty_pred, y_true)\nlin_grad(tl1, ty_pred, tW1, tb1)\nrelu_grad(tl0, tl1)\nlin_grad(tX_trn, tl0, tW0, tb0)\n\nVerify with PyTorch\n\n# Port layers\npt_lin0 = nn.Linear(m, nh)\ndtype = pt_lin0.weight.data.dtype\npt_lin0.weight.data = torch.from_numpy(tW0.v.T).to(dtype)\npt_lin0.bias.data = torch.from_numpy(tb0.v).to(dtype)\npt_lin1 = nn.Linear(nh, 1)\npt_lin1.weight.data = torch.from_numpy(tW1.T).to(dtype)\npt_lin1.bias.data = torch.from_numpy(tb1.v).to(dtype)\n\n# Forward pass\nlogits = pt_lin0(torch.from_numpy(X_trn).to(dtype))\nlogits = F.relu(logits)\nlogits = pt_lin1(logits)\nloss = F.mse_loss(\n    logits.squeeze(),\n    torch.from_numpy(y_true).float(),\n)\n\n# Backward pass\nloss.backward()\n\nfor w, b, layer in [\n    (tW0, tb0, pt_lin0),\n    (tW1, tb1, pt_lin1),\n]:\n    assert torch.isclose(\n        torch.from_numpy(w.g.T).float(),\n        layer.weight.grad,\n        atol=1e-4,\n    ).all()\n    assert torch.isclose(\n        torch.from_numpy(b.g.T).float(),\n        layer.bias.grad,\n        atol=1e-4,\n    ).all()\n\nLet‚Äôs refactor these as classes.\n\nclass Module:\n    def __call__(self, *x):\n        self.inp = x\n        self.out = self.forward(*x)\n        if isinstance(self.out, (np.ndarray,)):\n            self.out = T(self.out)\n        return self.out\n\n\nclass ReLu(Module):\n    def forward(self, x: T):\n        return relu(x)\n\n    def backward(self):\n        (x,) = self.inp\n        relu_grad(x, self.out)\n\n\nclass Linear(Module):\n    def __init__(self, h_in, h_out):\n        self.W = T(np.random.randn(h_in, h_out))\n        self.b = T(np.zeros(h_out))\n\n    def forward(self, x):\n        return T(x @ self.W.v + self.b.v)\n\n    def backward(self):\n        (x,) = self.inp\n        lin_grad(x, self.out, self.W, self.b)\n\n\nclass MSE(Module):\n    def forward(self, y_pred, y_true):\n        return ((y_pred.squeeze() - y_true) ** 2).mean()\n\n    def backward(self):\n        y_pred, y_true = self.inp\n        mse_grad(y_pred, y_true)\n\n\nclass MLP(Module):\n    def __init__(self, layers, criterion):\n        super().__init__()\n        self.layers = layers\n        self.criterion = criterion\n\n    def forward(self, x, y_pred):\n        for l in self.layers:\n            x = l(x)\n        self.criterion(x, y_pred)\n\n    def backward(self):\n        self.criterion.backward()\n        for l in reversed(self.layers):\n            l.backward()\n\n\nmodel = MLP([Linear(m, nh), ReLu(), Linear(nh, 1)], criterion=MSE())\n\n\nmodel(tX_trn, y_true)\nmodel.backward()\n\nThis is quite a bit cleaner!\nBy the rules of FastAI, we can now use the torch.nn.Module classes which is the equivalent in PyTorch.",
    "crumbs": [
      "Calculus and Backprop"
    ]
  },
  {
    "objectID": "ddpm.html",
    "href": "ddpm.html",
    "title": "DDPM and Mixed Precision",
    "section": "",
    "text": "Adapted from:\nplt.style.use(\"ggplot\")",
    "crumbs": [
      "DDPM and Mixed Precision"
    ]
  },
  {
    "objectID": "ddpm.html#review-lesson-9b",
    "href": "ddpm.html#review-lesson-9b",
    "title": "DDPM and Mixed Precision",
    "section": "Review, lesson 9b",
    "text": "Review, lesson 9b\n‚ÄúGenerative modeling‚Äù produces some complicated probability distribution, such that sampling produces a complicated example. GAN‚Äôs and VAE‚Äôs can produce such a distribution, and iterative refinement models can do so as well.\n¬†\nThe forward process is defined recurively like so: \\[\nq(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} (x_{t-1}), \\beta_t I)\n\\]\nDuring the forward process of diffusion, the mean decreases and the variance increases. In other words, the original image is lost and is replaced by pure noise.\nIt‚Äôs easy to compute the forward process for a given \\(X\\) and \\(t\\). There is, in fact, a closed-form solution for a given \\(\\{\\beta_0,...,\\beta_T\\}\\)\n\\[\n\\begin{align*}\n\\epsilon &\\sim \\mathcal{N}(0, I) \\\\\n\\alpha_t &= 1-\\beta_t \\\\\n\\bar{\\alpha_t} &= \\prod_{i=0}^t \\alpha_i \\\\\n     x_t &= \\sqrt{\\bar{\\alpha_t}} x_0 + \\sqrt{1-\\bar{\\alpha_t}} \\epsilon\n\\end{align*}\n\\]\n\ns = LMSDiscreteScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    num_train_timesteps=1000,\n)\nfig, (a0, a1, a2) = plt.subplots(1, 3, figsize=(8, 3))\na0.plot(s.betas)\na0.set(xlabel=\"Time\", ylabel=r\"$\\beta$\")\na1.plot(s.sigmas)\na1.set(xlabel=\"Time\", ylabel=r\"$\\sigma$\")\na2.plot(s.alphas_cumprod)\na2.set(xlabel=\"Time\", ylabel=r\"$\\bar{\\alpha}$\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nTo review:\n\n\\(\\beta\\) and \\(\\sigma\\) are the mean and standard deviation (respectively) of the noise added to the image at each step\n\\(\\bar{\\alpha}\\) is the cumulative amount of noise added at a particular time\n\nTo train this model, we:\n\nRandomly select a timepoint, \\(t\\) and add the apropriate amount of noise to the image\nPredict the added noise and back-prop with MSE loss\n\nSpecifically, we need the equation from the paper Algorithm 1.\n\\[\n\\begin{align*}\n       t &\\sim Uniform(\\{1,...,T\\}) \\\\\n\\epsilon &\\sim \\mathcal{N}(0, I) \\\\\n    loss &= MSE \\left( \\epsilon, \\epsilon_\\theta ( \\sqrt{\\bar{\\alpha_t}} x_0 + \\sqrt{1-\\bar{\\alpha_t}} \\epsilon, t ) \\right) \\\\\n\\end{align*}\n\\]\nHere, \\(\\epsilon_\\theta\\) is a neural network that predicts the mean and uses a fixed variance.\n\nsource\n\nDDPM\n\n DDPM (n_steps=1000, Œ≤min=0.0001, Œ≤max=0.02)\n\nModify the training behavior\nBy iteratively predicting the noise and removing it, ultimately we end up with a high probability data point.\nThis is defined in Algorithm 2 in the DDPM paper\n\\[\n\\begin{align*}\nz &\\sim \\mathcal{N}(0, I) \\text{ if } t &gt; 1 \\text{ else } z=0 \\\\\n\\hat{x}_{t-1} &= \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n\\end{align*}\n\\]\nLet‚Äôs break this down into manageable chunks\n\n\n\n\n\n\n\nEquation\nDescription\n\n\n\n\n\\[z \\sim \\mathcal{N}(0, I) \\text{ if } t &gt; 1 \\text{ else } z=0\\]\nSamples a noise vector from a standard normal distribution if t &gt; 1, otherwise sets z to 0\n\n\n\\[\\epsilon_\\theta(x_t, t)\\]\nRepresents the neural network\n\n\n\\[\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\]\nThe predicted noise scaling factor required such that the mean of the predicted noise matches that of \\(x_t\\)\n\n\n\\[x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\]\n\\(\\hat{x}_0\\)\n\n\n\\[ \\frac{1}{\\sqrt{\\alpha_t}} \\]\nThe overall scaling factor required such that adding $ _t z $ yields a mean and variance apropriate to the schedule\n\n\n\\[\\sigma_t z\\]\nNoise to add to the predicted \\(x_0\\) to get \\(x_{t-1}\\)\n\n\n\nSee Tanishq‚Äôs post for details on the derivation of the coefficients.\n\nsource\n\n\nsample\n\n sample (model, sz=(16, 1, 32, 32), device='cpu', return_all=False)",
    "crumbs": [
      "DDPM and Mixed Precision"
    ]
  },
  {
    "objectID": "ddpm.html#speeding-up-training",
    "href": "ddpm.html#speeding-up-training",
    "title": "DDPM and Mixed Precision",
    "section": "Speeding up training",
    "text": "Speeding up training\nGPUs are extremely fast at 16-bit precision. Unfortunately, not everything can be done at such low precision. Most training is performed using ‚Äúmixed precision.‚Äù\nWe can adapt the example in the Torch docs to that of a Callback\nuse_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\n\nsource\n\nMixedPrecision\n\n MixedPrecision ()\n\nTraining specific behaviors for the Learner\nSince fp16 takes only a fraction of the original representation memory space and processing time, we can dramatically increase the batch size. However, since we have fewer opportunities to update per epoch, we need to compensate with a higher learning rate and more epochs.\n\nunet = fashion_unet()\nunet.init_()\ntrain(\n    unet,\n    lr=1e-2,\n    n_epochs=8,\n    bs=512,\n    opt_func=partial(torch.optim.Adam, eps=1e-5),\n    extra_cbs=[MixedPrecision()],\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.262\n0\ntrain\n\n\n0.028\n0\neval\n\n\n0.026\n1\ntrain\n\n\n0.026\n1\neval\n\n\n0.022\n2\ntrain\n\n\n0.021\n2\neval\n\n\n0.019\n3\ntrain\n\n\n0.019\n3\neval\n\n\n0.018\n4\ntrain\n\n\n0.017\n4\neval\n\n\n0.017\n5\ntrain\n\n\n0.016\n5\neval\n\n\n0.016\n6\ntrain\n\n\n0.016\n6\neval\n\n\n0.016\n7\ntrain\n\n\n0.016\n7\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 1min 44s, sys: 19.3 s, total: 2min 4s\nWall time: 2min 7s\n\n\n\n# Let's save a copy of this for the FID notebook\ntorch.save(unet, \"../models/fashion_unet.pt\")\n\n\nimgs = ddpm.sample(unet)\nshow_images(imgs)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:14&lt;00:00, 67.09time step/s]\n\n\n\n\n\n\n\n\n\nThis makes training for longer more feasible, ultimately resulting in a lower loss.\nI tried a lot of configurations for this loop. It would usually diverge, until I increased the number of epochs to 8. This probably spread out the learning rate scheduling.\n\n# For the homework, let's keep a copy of these\nunet_hw, ddpm_hw = unet, ddpm",
    "crumbs": [
      "DDPM and Mixed Precision"
    ]
  },
  {
    "objectID": "ddpm.html#accelerate",
    "href": "ddpm.html#accelerate",
    "title": "DDPM and Mixed Precision",
    "section": "Accelerate",
    "text": "Accelerate\nAccelerate was developed by Sylvain Gugger at Huggingface to convert existing PyTorch code (and PyTorch-framework, like ours) to use optimizations such as:\n\nZeRO-Offload: enables multi-billion parameter model training by offloading tensor storage to the CPU and reloading strategically onto the GPU before computation (Author presentation here)\nFully-sharded data parallelism: divide the compute graph into shards, such that each shard computes an intermediate activation and dispatches it to the apropriate GPU\nMixed Precision Training\n\n\nsource\n\nAccelerateCB\n\n AccelerateCB (mixed_precision='fp16')\n\nTraining specific behaviors for the Learner\n\nunet = fashion_unet()\nunet.init_()\ntrain(\n    unet,\n    lr=1e-2,\n    n_epochs=8,\n    bs=512,\n    opt_func=partial(torch.optim.Adam, eps=1e-5),\n    extra_cbs=[AccelerateCB()],\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.203\n0\ntrain\n\n\n0.032\n0\neval\n\n\n0.029\n1\ntrain\n\n\n0.026\n1\neval\n\n\n0.023\n2\ntrain\n\n\n0.021\n2\neval\n\n\n0.021\n3\ntrain\n\n\n0.020\n3\neval\n\n\n0.018\n4\ntrain\n\n\n0.018\n4\neval\n\n\n0.017\n5\ntrain\n\n\n0.017\n5\neval\n\n\n0.016\n6\ntrain\n\n\n0.016\n6\neval\n\n\n0.016\n7\ntrain\n\n\n0.016\n7\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 1min 57s, sys: 19.1 s, total: 2min 16s\nWall time: 2min 35s\n\n\n\n\nHomework\n\nDo the same training setup, but use only the final 200 steps of the noise scheduler we used in this lesson.\n\nLet‚Äôs think. This assignment assumes we only need the values right of the dashed line.\n\nfig, (a0, a1, a2) = plt.subplots(1, 3, figsize=(8, 3))\na0.plot(s.betas)\na0.set(xlabel=\"Time\", ylabel=r\"$\\beta$\")\na1.plot(s.sigmas)\na1.set(xlabel=\"Time\", ylabel=r\"$\\sigma$\")\na2.plot(s.alphas_cumprod)\na2.set(xlabel=\"Time\", ylabel=r\"$\\bar{\\alpha}$\")\nfig.tight_layout()\n\nfor y_min, y_max, ax in [(0.00085, 0.012, a0), (0, 15, a1), (0, 1, a2)]:\n    ax.plot([800, 800], [y_min, y_max], linestyle=\"--\", color=\"black\")\n\n\n\n\n\n\n\n\nWhat happens if we use the values of \\(\\beta\\) from the dashed line to 0.012.\n\ns.betas[1000 - 200]\n\ntensor(0.0087)\n\n\n\nunet = fashion_unet()\nddpm = train(unet, ddpm=DDPM(200, 0.0087, 0.012), n_epochs=5)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.093\n0\ntrain\n\n\n0.023\n0\neval\n\n\n0.021\n1\ntrain\n\n\n0.020\n1\neval\n\n\n0.018\n2\ntrain\n\n\n0.018\n2\neval\n\n\n0.017\n3\ntrain\n\n\n0.016\n3\neval\n\n\n0.016\n4\ntrain\n\n\n0.015\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 2min 46s, sys: 9.92 s, total: 2min 56s\nWall time: 2min 59s\n\n\n\nimgs = ddpm.sample(unet)\nshow_images(imgs)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:02&lt;00:00, 68.44time step/s]\n\n\n\n\n\n\n\n\n\nQuite blurry.\nLet‚Äôs try something else. Let‚Äôs use a model trained ‚Äúnormally‚Äù, but only take the last 200 steps.\n\nunet, ddpm = unet_hw, ddpm_hw\n\n\n@torch.no_grad()\ndef sample_using_only_n_steps(\n    n_steps,\n    ddpm,\n    model,\n    sz=(16, 1, 32, 32),\n    device=def_device,\n):\n    ·æ±, …ë, œÉ = ddpm.·æ±.to(device), ddpm.…ë.to(device), ddpm.œÉ.to(device)\n    x_t = torch.randn(sz, device=device) * œÉ[ddpm.n_steps - n_steps]\n    bs, *_ = sz\n    preds = []\n    iter_ = list(reversed(range(1, ddpm.n_steps)))\n    iter_ = iter_[-n_steps:]\n    for t in tqdm(iter_, unit=\"time step\"):\n        # Predict the noise for each example in the image\n        t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)\n        noise_pred = model(x_t, t_batch).sample\n\n        # Predict the image without noise\n        x_0_pred = x_t - (1 - …ë[t]) / torch.sqrt(1 - ·æ±[t]) * noise_pred\n\n        # Add noise to the predicted noiseless image such that it ulimately\n        # has slightly less noise than before\n        x_t_minus_1 = x_0_pred / …ë[t].sqrt() + (œÉ[t] * torch.randn(sz, device=device))\n\n        # Repeat\n        x_t = x_t_minus_1\n\n    # At the last step, simply rescale and do not add noise\n    x_0 = x_0_pred / …ë[0].sqrt()\n    return x_0\n\n\nimgs = sample_using_only_n_steps(200, ddpm, unet)\nshow_images(imgs)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:02&lt;00:00, 68.70time step/s]\n\n\n\n\n\n\n\n\n\nNot really working. Maybe we can take every other nth step, until step 200, and then sample normally.\n\n@torch.no_grad()\ndef sample_skip(\n    skip: int,\n    until: int,\n    ddpm,\n    model,\n    sz=(16, 1, 32, 32),\n    device=def_device,\n):\n    \"\"\"For the first n steps, reuse the noise prediction\"\"\"\n    ·æ±, …ë, œÉ = ddpm.·æ±.to(device), ddpm.…ë.to(device), ddpm.œÉ.to(device)\n    x_t = torch.randn(sz, device=device)\n    bs, *_ = sz\n    preds = []\n    ts = list(reversed(range(1, ddpm.n_steps)))\n\n    skip_ts, nonskip_ts = ts[:until][::skip], ts[until:]\n\n    try:\n        for t in tqdm(skip_ts, unit=\"time step\"):\n            # Predict the noise for each example in the image\n            t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)\n            noise_pred = model(x_t, t_batch).sample\n            epsilon = torch.randn(sz, device=device)\n\n            for t_offset in range(skip):\n                # Predict the image without noise\n                if t + t_offset &gt; until:\n                    raise StopIteration\n                K = (1 - …ë[t - t_offset]) / torch.sqrt(1 - ·æ±[t - t_offset])\n                x_0_pred = x_t - K * noise_pred\n\n                # Add noise to the predicted noiseless image such that it ulimately\n                # has slightly less noise than before\n                x_t_minus_1 = x_0_pred / …ë[t].sqrt() + (œÉ[t] * epsilon)\n\n            # Repeat\n            x_t = x_t_minus_1\n    except StopIteration:\n        ...\n\n    for t in tqdm(nonskip_ts, unit=\"time step\"):\n        t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)\n        noise_pred = model(x_t, t_batch).sample\n        x_0_pred = x_t - (1 - …ë[t]) / torch.sqrt(1 - ·æ±[t]) * noise_pred\n        x_t_minus_1 = x_0_pred / …ë[t].sqrt() + (œÉ[t] * torch.randn(sz, device=device))\n        x_t = x_t_minus_1\n\n    x_0 = x_0_pred / …ë[0].sqrt()\n    return x_0\n\n\nimgs = sample_skip(100, 300, ddpm, unet)\nshow_images(imgs)\n\n  0%|                                                                                                                                                            | 0/3 [00:00&lt;?, ?time step/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 699/699 [00:10&lt;00:00, 68.30time step/s]\n\n\n\n\n\n\n\n\n\nLet‚Äôs try one more thing where we skip more at the beginning than at the end.\n\n@torch.no_grad()\ndef sample_skip_schedule(schedule, ddpm, model, sz=(16, 1, 32, 32), device=def_device):\n    assert sum(schedule) &lt; ddpm.n_steps\n\n    ·æ±, …ë, œÉ = ddpm.·æ±.to(device), ddpm.…ë.to(device), ddpm.œÉ.to(device)\n    x_t = torch.randn(sz, device=device)\n    bs, *_ = sz\n    preds = []\n    ts = reversed(range(1, ddpm.n_steps))\n    iter_ts = iter(tqdm(ts, total=ddpm.n_steps - 1))\n\n    for block in schedule:\n        t = next(iter_ts)\n        t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)\n        noise_pred = model(x_t, t_batch).sample\n        epsilon = torch.randn(sz, device=device)\n\n        # Reuse the predicted and sampled noise\n        for _ in range(block - 1):\n            t = next(iter_ts)\n            K = (1 - …ë[t]) / torch.sqrt(1 - ·æ±[t])\n            x_0_pred = x_t - K * noise_pred\n            x_t_minus_1 = x_0_pred / …ë[t].sqrt() + (œÉ[t] * epsilon)\n\n        # Repeat\n        x_t = x_t_minus_1\n\n    for t in iter_ts:\n        t_batch = torch.full((bs,), fill_value=t, device=device, dtype=torch.long)\n        noise_pred = model(x_t, t_batch).sample\n        x_0_pred = x_t - (1 - …ë[t]) / torch.sqrt(1 - ·æ±[t]) * noise_pred\n        x_t_minus_1 = x_0_pred / …ë[t].sqrt() + (œÉ[t] * torch.randn(sz, device=device))\n        x_t = x_t_minus_1\n\n    x_0 = x_0_pred / …ë[0].sqrt()\n    return x_0\n\n\nsched = [200] + [25] * 10 + [5] * 25\nn_steps = ddpm.n_steps - sum(sched) + len(sched)\nimgs = sample_skip_schedule(sched, ddpm, unet)\nshow_images(imgs)\nn_steps\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:06&lt;00:00, 149.67it/s]\n\n\n461\n\n\n\n\n\n\n\n\n\nNone of these worked very well. I was able to get it down to 100 steps in the cosine_revisited.",
    "crumbs": [
      "DDPM and Mixed Precision"
    ]
  },
  {
    "objectID": "augmentation.html",
    "href": "augmentation.html",
    "title": "Augmentation",
    "section": "",
    "text": "Adapted from:\nset_seed(42)\nplt.style.use(\"ggplot\")\nLet‚Äôs redefine the training loop for clarity.\nsource",
    "crumbs": [
      "Augmentation"
    ]
  },
  {
    "objectID": "augmentation.html#going-wider",
    "href": "augmentation.html#going-wider",
    "title": "Augmentation",
    "section": "Going wider",
    "text": "Going wider\nCan we get a better result by increasing the width of our network?\nWe didn‚Äôt spend much time designing the Residual CNN from the previous notebook. We simply replaced the Conv blocks with Residual Conv blocks, doubling the number of parameters.\nIn principle, ResNet‚Äôs are more stable than their CNN counterparts, so we should be able to make the network wider as well as deeper.\n\nsource\n\nResNet\n\n ResNet (nfs:Sequence[int]=[16, 32, 64, 128, 256, 512], n_outputs=10)\n\nArbitrarily wide and deep residual neural network\n\nm = ResNet.kaiming()\ntrain(m)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.831\n0.474\n0\ntrain\n\n\n0.870\n0.378\n0\neval\n\n\n0.911\n0.241\n1\ntrain\n\n\n0.914\n0.233\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs create quick utility to view the shape of the model to check for areas of improvement\n\nsource\n\n\nsummarize\n\n summarize (m, mods, dls=None, xb_=None)\n\n\nsource\n\n\nhooks\n\n hooks (mods, f)\n\n\nsource\n\n\nflops\n\n flops (x, w, h)\n\nEstimate flops\n\nsummarize(ResNet(), \"ResidualConvBlock\")\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nResidualConvBlock\n(8, 1, 28, 28)\n(8, 16, 28, 28)\n6,896\n5.3\n\n\nResidualConvBlock\n(8, 16, 28, 28)\n(8, 32, 14, 14)\n14,496\n2.8\n\n\nResidualConvBlock\n(8, 32, 14, 14)\n(8, 64, 7, 7)\n57,664\n2.8\n\n\nResidualConvBlock\n(8, 64, 7, 7)\n(8, 128, 4, 4)\n230,016\n3.7\n\n\nResidualConvBlock\n(8, 128, 4, 4)\n(8, 256, 2, 2)\n918,784\n3.7\n\n\nResidualConvBlock\n(8, 256, 2, 2)\n(8, 512, 1, 1)\n3,672,576\n3.7\n\n\nResidualConvBlock\n(8, 512, 1, 1)\n(8, 10, 1, 1)\n52,150\n0.1\n\n\nTotal\n\n\n4,952,582\n\n\n\n\n\n\nOne of the important constaints of our model here is that that strides must be configured to downsample the image precisely to bs x c x 1 x 1. We can make this more flexible by taking the final feature map (regardless of its height and width) and taking the average.\n\nsource\n\n\nGlobalAveragePooling\n\n GlobalAveragePooling (*args, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nResNetWithGlobalPooling\n\n ResNetWithGlobalPooling (nfs:Sequence[int]=[16, 32, 64, 128, 256, 512],\n                          n_outputs=10)\n\nArbitrarily wide and deep residual neural network\n\nnfs = [\n    16,\n    32,\n    64,\n    128,\n    256,  # üëà notice that this leaves the feature map at 2x2...\n]\nm = ResNetWithGlobalPooling.kaiming(nfs)\nsummarize(m, \"ResidualConvBlock|Linear\")\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nResidualConvBlock\n(8, 1, 28, 28)\n(8, 16, 28, 28)\n6,896\n5.3\n\n\nResidualConvBlock\n(8, 16, 28, 28)\n(8, 32, 14, 14)\n14,496\n2.8\n\n\nResidualConvBlock\n(8, 32, 14, 14)\n(8, 64, 7, 7)\n57,664\n2.8\n\n\nResidualConvBlock\n(8, 64, 7, 7)\n(8, 128, 4, 4)\n230,016\n3.7\n\n\nResidualConvBlock\n(8, 128, 4, 4)\n(8, 256, 2, 2)\n918,784\n3.7\n\n\nLinear\n(8, 256)\n(8, 10)\n2,570\n0.0\n\n\nTotal\n\n\n1,230,426\n\n\n\n\n\n\n\n# ...but it still works!\ntrain(m)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.848\n0.581\n0\ntrain\n\n\n0.840\n0.477\n0\neval\n\n\n0.913\n0.287\n1\ntrain\n\n\n0.916\n0.271\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCan we reduce the number of parameters to save on memory? Indeed. One thing to focus on is the first ResidualConvBlock which has the most MegaFlops, because it applies the 16 kernels to each pixel. We can try replacing it with a Conv.\n\nsource\n\n\nResNetWithGlobalPoolingInitialConv\n\n ResNetWithGlobalPoolingInitialConv (nfs:Sequence[int]=[16, 32, 64, 128,\n                                     256, 512], n_outputs=10)\n\nArbitrarily wide and deep residual neural network\n\nm = ResNetWithGlobalPoolingInitialConv.kaiming()\nsummarize(m, [*m.layers, m.lin, m.norm])\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nConv\n(8, 1, 28, 28)\n(8, 16, 28, 28)\n432\n0.3\n\n\nResidualConvBlock\n(8, 16, 28, 28)\n(8, 32, 14, 14)\n14,496\n2.8\n\n\nResidualConvBlock\n(8, 32, 14, 14)\n(8, 64, 7, 7)\n57,664\n2.8\n\n\nResidualConvBlock\n(8, 64, 7, 7)\n(8, 128, 4, 4)\n230,016\n3.7\n\n\nResidualConvBlock\n(8, 128, 4, 4)\n(8, 256, 2, 2)\n918,784\n3.7\n\n\nResidualConvBlock\n(8, 256, 2, 2)\n(8, 512, 1, 1)\n3,672,576\n3.7\n\n\nLinear\n(8, 512)\n(8, 10)\n5,130\n0.0\n\n\nBatchNorm1d\n(8, 10)\n(8, 10)\n20\n0.0\n\n\nTotal\n\n\n4,899,118\n\n\n\n\n\n\n\ntrain(m)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.850\n0.565\n0\ntrain\n\n\n0.843\n0.466\n0\neval\n\n\n0.913\n0.281\n1\ntrain\n\n\n0.916\n0.265\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nThis approach yeilds a small, flexible and competitive model. What happens if we train for a while?\n\ntrain(ResNetWithGlobalPoolingInitialConv.kaiming(), n_epochs=20)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.846\n0.662\n0\ntrain\n\n\n0.876\n0.527\n0\neval\n\n\n0.898\n0.456\n1\ntrain\n\n\n0.888\n0.411\n1\neval\n\n\n0.907\n0.353\n2\ntrain\n\n\n0.888\n0.367\n2\neval\n\n\n0.912\n0.288\n3\ntrain\n\n\n0.838\n0.506\n3\neval\n\n\n0.918\n0.250\n4\ntrain\n\n\n0.856\n0.418\n4\neval\n\n\n0.925\n0.221\n5\ntrain\n\n\n0.878\n0.360\n5\neval\n\n\n0.935\n0.192\n6\ntrain\n\n\n0.901\n0.287\n6\neval\n\n\n0.943\n0.167\n7\ntrain\n\n\n0.904\n0.289\n7\neval\n\n\n0.950\n0.148\n8\ntrain\n\n\n0.902\n0.301\n8\neval\n\n\n0.955\n0.131\n9\ntrain\n\n\n0.910\n0.283\n9\neval\n\n\n0.959\n0.115\n10\ntrain\n\n\n0.910\n0.296\n10\neval\n\n\n0.966\n0.099\n11\ntrain\n\n\n0.910\n0.289\n11\neval\n\n\n0.974\n0.077\n12\ntrain\n\n\n0.913\n0.317\n12\neval\n\n\n0.978\n0.063\n13\ntrain\n\n\n0.914\n0.299\n13\neval\n\n\n0.984\n0.048\n14\ntrain\n\n\n0.919\n0.289\n14\neval\n\n\n0.991\n0.031\n15\ntrain\n\n\n0.922\n0.296\n15\neval\n\n\n0.996\n0.017\n16\ntrain\n\n\n0.927\n0.293\n16\neval\n\n\n0.999\n0.008\n17\ntrain\n\n\n0.929\n0.293\n17\neval\n\n\n1.000\n0.006\n18\ntrain\n\n\n0.928\n0.293\n18\neval\n\n\n1.000\n0.005\n19\ntrain\n\n\n0.928\n0.293\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\nThe near perfect training accuracy indicates that the model is simply memorizing the dataset and failing to generalize.\nWe‚Äôve discussed weight decay as a regularization technique. Could this help generalization?\n\n\n\n\n\n\nWeight Decay and Batchnorm do not work together\n\n\n\nWe‚Äôve posited that weight decay, as a regularization, prevents memorization. However, batch norm has a single set of coefficients that scale the layer output. Since weight decay also scales the layer weight, the model is able to ‚Äúcheat.‚Äù Jeremy says to avoid weight decay and rely on a scheduler.\n\n\nInstead, let‚Äôs try ‚ÄúAugmentation‚Äù to create pseudo-new data that the model must learn to account for.",
    "crumbs": [
      "Augmentation"
    ]
  },
  {
    "objectID": "augmentation.html#augmentation",
    "href": "augmentation.html#augmentation",
    "title": "Augmentation",
    "section": "Augmentation",
    "text": "Augmentation\nRecall, we implemented the with_transforms method on the Dataloaders class in the Learner notebook.\n\ntfms = [\n    transforms.RandomCrop(28, padding=1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.26], [0.35]),\n]\ntfmsc = transforms.Compose(tfms)\n\ndls = fashion_mnist(512).with_transforms(\n    {\"image\": batchify(tfmsc)}, lazy=True, splits=[\"train\"]\n)\n\nxb, _ = dls.peek()\nshow_images(xb[:8, ...])\n\n\n\n\n\n\n\n\n\npixels = xb.view(-1)\npixels.mean(), pixels.std()\n\n(tensor(0.0645), tensor(1.0079))\n\n\n\nm_with_augmentation = ResNetWithGlobalPoolingInitialConv.kaiming()\ntrain(m_with_augmentation, dls=dls, n_epochs=20)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.804\n0.762\n0\ntrain\n\n\n0.845\n0.568\n0\neval\n\n\n0.875\n0.515\n1\ntrain\n\n\n0.869\n0.457\n1\neval\n\n\n0.885\n0.407\n2\ntrain\n\n\n0.881\n0.369\n2\neval\n\n\n0.893\n0.339\n3\ntrain\n\n\n0.882\n0.352\n3\neval\n\n\n0.900\n0.298\n4\ntrain\n\n\n0.856\n0.388\n4\neval\n\n\n0.907\n0.268\n5\ntrain\n\n\n0.906\n0.272\n5\neval\n\n\n0.913\n0.246\n6\ntrain\n\n\n0.868\n0.380\n6\neval\n\n\n0.918\n0.229\n7\ntrain\n\n\n0.913\n0.242\n7\neval\n\n\n0.923\n0.215\n8\ntrain\n\n\n0.891\n0.306\n8\neval\n\n\n0.928\n0.202\n9\ntrain\n\n\n0.922\n0.218\n9\neval\n\n\n0.933\n0.187\n10\ntrain\n\n\n0.925\n0.215\n10\neval\n\n\n0.938\n0.174\n11\ntrain\n\n\n0.927\n0.205\n11\neval\n\n\n0.943\n0.160\n12\ntrain\n\n\n0.927\n0.206\n12\neval\n\n\n0.947\n0.149\n13\ntrain\n\n\n0.927\n0.210\n13\neval\n\n\n0.951\n0.139\n14\ntrain\n\n\n0.931\n0.199\n14\neval\n\n\n0.956\n0.124\n15\ntrain\n\n\n0.934\n0.192\n15\neval\n\n\n0.962\n0.110\n16\ntrain\n\n\n0.940\n0.178\n16\neval\n\n\n0.967\n0.096\n17\ntrain\n\n\n0.940\n0.177\n17\neval\n\n\n0.971\n0.086\n18\ntrain\n\n\n0.943\n0.177\n18\neval\n\n\n0.973\n0.080\n19\ntrain\n\n\n0.942\n0.177\n19\neval",
    "crumbs": [
      "Augmentation"
    ]
  },
  {
    "objectID": "augmentation.html#test-time-augmentation",
    "href": "augmentation.html#test-time-augmentation",
    "title": "Augmentation",
    "section": "Test Time Augmentation",
    "text": "Test Time Augmentation\nGiving the model mulitple oppertunities to see the input can further improve the output.\n\nxbf = torch.flip(xb, dims=(3,))\nshow_images([xb[0, ...], xbf[0, ...]])\n\n\n\n\n\n\n\n\n\ndef accuracy(model_predict_f, model, device=def_device):\n    dls = fashion_mnist(512)\n    n, n_correct = 0, 0\n    for xb, yb in dls[\"test\"]:\n        xb = xb.to(device)\n        yb = yb.to(device)\n        yp = model_predict_f(xb, model)\n        n += len(yb)\n        n_correct += (yp == yb).float().sum().item()\n    return n_correct / n\n\n\ndef pred_normal(xb, m):\n    return m(xb).argmax(axis=1)\n\nLet‚Äôs check the normal accuracy\n\naccuracy(pred_normal, m_with_augmentation)\n\n0.9415\n\n\nNow, we can compare that to averaging the outputs when looking at both flips\n\ndef pred_with_test_time_augmentation(xb, m):\n    yp = m(xb)\n    xbf = torch.flip(xb, dims=(3,))\n    ypf = m(xbf)\n    return (yp + ypf).argmax(axis=1)\n\n\naccuracy(pred_with_test_time_augmentation, m_with_augmentation)\n\n0.9448\n\n\nThis is a slight improvement!",
    "crumbs": [
      "Augmentation"
    ]
  },
  {
    "objectID": "augmentation.html#randcopy",
    "href": "augmentation.html#randcopy",
    "title": "Augmentation",
    "section": "RandCopy",
    "text": "RandCopy\nAnother thing to try is creating new-ish images by cutting and pasting segments of the image onto different locations. A benefit to this approach is that the image should retain its pixel brightness distribution. (Compare to, for example, adding black will push the distribution downwards)\n\nsource\n\nRandCopy\n\n RandCopy (pct=0.2, max_num=4)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\ntfmsc2 = transforms.Compose([*tfms, RandCopy()])\n\ndls2 = fashion_mnist(512).with_transforms(\n    {\"image\": batchify(tfmsc2)},\n    lazy=True,\n    splits=[\"train\"],\n)\n\nxb, _ = dls2.peek()\nshow_images(xb[:8, ...])\n\n\n\n\n\n\n\n\n\nm_with_more_augmentation = ResNetWithGlobalPoolingInitialConv.kaiming()\ntrain(m_with_more_augmentation, dls=dls2, n_epochs=20)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.782\n0.817\n0\ntrain\n\n\n0.838\n0.569\n0\neval\n\n\n0.850\n0.576\n1\ntrain\n\n\n0.858\n0.459\n1\neval\n\n\n0.865\n0.456\n2\ntrain\n\n\n0.852\n0.426\n2\neval\n\n\n0.873\n0.395\n3\ntrain\n\n\n0.877\n0.349\n3\neval\n\n\n0.880\n0.348\n4\ntrain\n\n\n0.856\n0.410\n4\neval\n\n\n0.889\n0.317\n5\ntrain\n\n\n0.903\n0.273\n5\neval\n\n\n0.896\n0.292\n6\ntrain\n\n\n0.894\n0.293\n6\neval\n\n\n0.902\n0.273\n7\ntrain\n\n\n0.888\n0.297\n7\neval\n\n\n0.907\n0.256\n8\ntrain\n\n\n0.890\n0.301\n8\neval\n\n\n0.913\n0.242\n9\ntrain\n\n\n0.921\n0.228\n9\neval\n\n\n0.917\n0.232\n10\ntrain\n\n\n0.922\n0.221\n10\neval\n\n\n0.920\n0.220\n11\ntrain\n\n\n0.924\n0.208\n11\neval\n\n\n0.927\n0.205\n12\ntrain\n\n\n0.933\n0.193\n12\neval\n\n\n0.931\n0.192\n13\ntrain\n\n\n0.925\n0.210\n13\neval\n\n\n0.935\n0.180\n14\ntrain\n\n\n0.930\n0.197\n14\neval\n\n\n0.940\n0.169\n15\ntrain\n\n\n0.937\n0.182\n15\neval\n\n\n0.943\n0.158\n16\ntrain\n\n\n0.937\n0.178\n16\neval\n\n\n0.947\n0.147\n17\ntrain\n\n\n0.937\n0.173\n17\neval\n\n\n0.949\n0.141\n18\ntrain\n\n\n0.939\n0.174\n18\neval\n\n\n0.951\n0.137\n19\ntrain\n\n\n0.938\n0.174\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\naccuracy(pred_normal, m_with_more_augmentation)\n\n0.9383\n\n\n\naccuracy(pred_with_test_time_augmentation, m_with_more_augmentation)\n\n0.9414\n\n\nWe‚Äôre so close to Jeremy‚Äôs 94.6% accuracy\n\n\nHomework: Beat Jeremy\n\nf = RandCopy()\n\n\ndef pred_with_test_time_augmentation_02(xb, m):\n    ys = m(xb)\n    ys += m(torch.flip(xb, dims=(3,)))\n    for _ in range(6):\n        ys += m(f(xb))\n    return ys.argmax(axis=1)\n\n\naccuracy(pred_with_test_time_augmentation_02, m_with_more_augmentation)\n\n0.9402\n\n\nUnfortunately, additional test time augmentation does not seem to improve the results.\nLet‚Äôs try making it deeper.\n\nmz = ResNetWithGlobalPoolingInitialConv.kaiming(nfs=[32, 64, 128, 256, 512, 512])\nsummarize(mz, [*mz.layers, mz.lin, mz.norm])\ntrain(mz, dls=dls2, n_epochs=20)\n\n\n\n\n\n\n\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nConv\n(8, 1, 28, 28)\n(8, 32, 28, 28)\n864\n0.6\n\n\nResidualConvBlock\n(8, 32, 28, 28)\n(8, 64, 14, 14)\n57,664\n11.2\n\n\nResidualConvBlock\n(8, 64, 14, 14)\n(8, 128, 7, 7)\n230,016\n11.2\n\n\nResidualConvBlock\n(8, 128, 7, 7)\n(8, 256, 4, 4)\n918,784\n14.7\n\n\nResidualConvBlock\n(8, 256, 4, 4)\n(8, 512, 2, 2)\n3,672,576\n14.7\n\n\nResidualConvBlock\n(8, 512, 2, 2)\n(8, 512, 1, 1)\n4,983,296\n5.0\n\n\nLinear\n(8, 512)\n(8, 10)\n5,130\n0.0\n\n\nBatchNorm1d\n(8, 10)\n(8, 10)\n20\n0.0\n\n\nTotal\n\n\n9,868,350\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.801\n0.767\n0\ntrain\n\n\n0.836\n0.539\n0\neval\n\n\n0.861\n0.544\n1\ntrain\n\n\n0.880\n0.399\n1\neval\n\n\n0.871\n0.441\n2\ntrain\n\n\n0.867\n0.400\n2\neval\n\n\n0.879\n0.375\n3\ntrain\n\n\n0.886\n0.346\n3\neval\n\n\n0.889\n0.330\n4\ntrain\n\n\n0.847\n0.399\n4\neval\n\n\n0.894\n0.301\n5\ntrain\n\n\n0.907\n0.259\n5\neval\n\n\n0.901\n0.277\n6\ntrain\n\n\n0.900\n0.276\n6\neval\n\n\n0.909\n0.255\n7\ntrain\n\n\n0.900\n0.280\n7\neval\n\n\n0.913\n0.241\n8\ntrain\n\n\n0.920\n0.240\n8\neval\n\n\n0.918\n0.228\n9\ntrain\n\n\n0.902\n0.275\n9\neval\n\n\n0.922\n0.217\n10\ntrain\n\n\n0.920\n0.226\n10\neval\n\n\n0.927\n0.204\n11\ntrain\n\n\n0.924\n0.215\n11\neval\n\n\n0.932\n0.190\n12\ntrain\n\n\n0.931\n0.191\n12\neval\n\n\n0.935\n0.179\n13\ntrain\n\n\n0.934\n0.187\n13\neval\n\n\n0.941\n0.164\n14\ntrain\n\n\n0.932\n0.195\n14\neval\n\n\n0.946\n0.151\n15\ntrain\n\n\n0.941\n0.172\n15\neval\n\n\n0.950\n0.138\n16\ntrain\n\n\n0.944\n0.163\n16\neval\n\n\n0.955\n0.129\n17\ntrain\n\n\n0.943\n0.166\n17\neval\n\n\n0.957\n0.121\n18\ntrain\n\n\n0.943\n0.163\n18\neval\n\n\n0.958\n0.117\n19\ntrain\n\n\n0.943\n0.163\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\naccuracy(pred_normal, mz)\n\n0.945\n\n\n\naccuracy(pred_with_test_time_augmentation, mz)\n\n0.947\n\n\nOh, that is just barely better than Jeremy.\nI noticed a bug where the initialization is NOT incorporating the GenerualRelu leak parameter. Let‚Äôs see if that helps.\n\ninit_leaky_weights??\n\n\nSignature: init_leaky_weights(module, leak=0.0)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef init_leaky_weights(module, leak=0.0):\n    if isinstance(module, (nn.Conv2d,)):\n        init.kaiming_normal_(module.weight, a=leak)  # üëà weirdly, called `a` here\nFile:      ~/Desktop/SlowAI/nbs/slowai/initializations.py\nType:      function\n\n\n\n\nResNetWithGlobalPoolingInitialConv().layers[0].act.a\n\n0.1\n\n\nLet‚Äôs fix that and see if we can improve the performance.\n\ndef init_leaky_weights_fixed(m):\n    if isinstance(m, Conv):\n        if m.act is None or not m.act.a:\n            init.kaiming_normal_(m.weight)\n        else:\n            init.kaiming_normal_(m.weight, a=m.act.a)\n\n\nclass ResNetWithGlobalPoolingInitialConv2(ResNetWithGlobalPoolingInitialConv):\n    @classmethod\n    def kaiming(cls, *args, **kwargs):\n        model = cls(*args, **kwargs)\n        model.apply(init_leaky_weights_fixed)\n        return model\n\n\nmz2 = ResNetWithGlobalPoolingInitialConv2.kaiming(nfs=[32, 64, 128, 256, 512, 512])\ntrain(mz2, dls=dls2, n_epochs=20)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.795\n0.783\n0\ntrain\n\n\n0.859\n0.528\n0\neval\n\n\n0.857\n0.555\n1\ntrain\n\n\n0.866\n0.463\n1\neval\n\n\n0.867\n0.450\n2\ntrain\n\n\n0.850\n0.445\n2\neval\n\n\n0.877\n0.379\n3\ntrain\n\n\n0.894\n0.315\n3\neval\n\n\n0.885\n0.334\n4\ntrain\n\n\n0.895\n0.298\n4\neval\n\n\n0.896\n0.301\n5\ntrain\n\n\n0.888\n0.295\n5\neval\n\n\n0.902\n0.278\n6\ntrain\n\n\n0.901\n0.273\n6\neval\n\n\n0.907\n0.261\n7\ntrain\n\n\n0.916\n0.237\n7\neval\n\n\n0.913\n0.243\n8\ntrain\n\n\n0.919\n0.227\n8\neval\n\n\n0.916\n0.233\n9\ntrain\n\n\n0.926\n0.210\n9\neval\n\n\n0.921\n0.218\n10\ntrain\n\n\n0.925\n0.206\n10\neval\n\n\n0.924\n0.207\n11\ntrain\n\n\n0.923\n0.214\n11\neval\n\n\n0.929\n0.197\n12\ntrain\n\n\n0.927\n0.198\n12\neval\n\n\n0.934\n0.181\n13\ntrain\n\n\n0.927\n0.195\n13\neval\n\n\n0.939\n0.169\n14\ntrain\n\n\n0.936\n0.183\n14\neval\n\n\n0.943\n0.158\n15\ntrain\n\n\n0.938\n0.176\n15\neval\n\n\n0.948\n0.145\n16\ntrain\n\n\n0.943\n0.164\n16\neval\n\n\n0.952\n0.133\n17\ntrain\n\n\n0.943\n0.161\n17\neval\n\n\n0.955\n0.125\n18\ntrain\n\n\n0.944\n0.160\n18\neval\n\n\n0.957\n0.122\n19\ntrain\n\n\n0.945\n0.161\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\naccuracy(pred_normal, mz2)\n\n0.9446\n\n\n\naccuracy(pred_with_test_time_augmentation, mz2)\n\n0.9473\n\n\nSadly, slightly worse for whatever reason.\nLet‚Äôs try a Fixup Initialization",
    "crumbs": [
      "Augmentation"
    ]
  },
  {
    "objectID": "augmentation.html#fixup-initialization",
    "href": "augmentation.html#fixup-initialization",
    "title": "Augmentation",
    "section": "Fixup initialization",
    "text": "Fixup initialization\n\nclass FixupResBlock(nn.Module):\n    def __init__(self, c_in, c_out, ks=3, stride=2):\n        super(FixupResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(c_in, c_out, ks, 1, padding=ks // 2, bias=False)\n        self.conv2 = nn.Conv2d(c_out, c_out, ks, stride, padding=ks // 2, bias=False)\n        self.id_conv = nn.Conv2d(c_in, c_out, stride=1, kernel_size=1)\n        self.scale = nn.Parameter(torch.ones(1))\n\n    def forward(self, x_orig):\n        x = self.conv1(x_orig)\n        x = F.relu(x)\n        x = self.conv2(x) * self.scale\n        if self.conv2.stride == (2, 2):\n            x_orig = F.avg_pool2d(x_orig, kernel_size=2, ceil_mode=True)\n        x = F.relu(x + self.id_conv(x_orig))\n        return x\n\n\nclass FixupResNet(nn.Module):\n    def __init__(self, nfs, num_classes=10):\n        super(FixupResNet, self).__init__()\n        self.conv = nn.Conv2d(1, nfs[0], 5, stride=2, padding=2, bias=False)\n        layers = []\n        for c_in, c_out in zip(nfs, nfs[1:]):\n            layers.append(FixupResBlock(c_in, c_out))\n        self.layers = nn.Sequential(*layers)\n        self.fc = nn.Linear(nfs[-1], num_classes)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.layers(x)\n        bs, c, h, w = range(4)\n        x = x.mean((h, w))  # Global Average Pooling\n        x = self.fc(x)\n        return x\n\n    @torch.no_grad()\n    def init_weights(self):\n        init.kaiming_normal_(self.conv.weight)\n        n_layers = len(self.layers)\n        for layer in self.layers:\n            (c_out, c_in, ksa, ksb) = layer.conv1.weight.shape\n            nn.init.normal_(\n                layer.conv1.weight,\n                mean=0,\n                std=sqrt(2 / (c_out * ksa * ksb)) * n_layers ** (-0.5),\n            )\n            nn.init.constant_(layer.conv2.weight, 0)\n        nn.init.constant_(self.fc.weight, 0)\n        nn.init.constant_(self.fc.bias, 0)\n\n    @classmethod\n    def random(cls, *args, **kwargs):\n        m = cls(*args, **kwargs)\n        m.init_weights()\n        return m\n\n\nm = FixupResNet.random([8, 16, 32, 64, 128, 256, 512])\nstats = StoreModuleStatsCB(m.layers)\ntrain(m, extra_cbs=[stats])\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.333\n1.663\n0\ntrain\n\n\n0.643\n0.853\n0\neval\n\n\n0.766\n0.584\n1\ntrain\n\n\n0.806\n0.507\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats.mean_std_plot()\n\n\n\n\n\n\n\n\nOkay, fixup doesn‚Äôt look too promising.\nOn the forums, some things that were successful:\n\nDropout (and test time dropout augmentation)\nCurriculum learning\nMish activation\n\nThis is how you would implement dropout\n\ndistributions.binomial.Binomial?\n\n\nInit signature:\ndistributions.binomial.Binomial(\n    total_count=1,\n    probs=None,\n    logits=None,\n    validate_args=None,\n)\nDocstring:     \nCreates a Binomial distribution parameterized by :attr:`total_count` and\neither :attr:`probs` or :attr:`logits` (but not both). :attr:`total_count` must be\nbroadcastable with :attr:`probs`/:attr:`logits`.\nExample::\n    &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterinistic\")\n    &gt;&gt;&gt; m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n    &gt;&gt;&gt; x = m.sample()\n    tensor([   0.,   22.,   71.,  100.])\n    &gt;&gt;&gt; m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n    &gt;&gt;&gt; x = m.sample()\n    tensor([[ 4.,  5.],\n            [ 7.,  6.]])\nArgs:\n    total_count (int or Tensor): number of Bernoulli trials\n    probs (Tensor): Event probabilities\n    logits (Tensor): Event log-odds\nFile:           ~/micromamba/envs/slowai/lib/python3.11/site-packages/torch/distributions/binomial.py\nType:           type\nSubclasses:     \n\n\n\n\nclass Dropout(nn.Module):\n    def __init__(self, p=0.9):\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        if self.training:\n            return x\n        else:\n            dist = distributions.binomial.Binomial(1, props=1 - self.p)\n            return x * dist.sample(x.shape) / self.p\n\nThe difference between Dropout and Dropout2D is that Dropout2D only applies to the width and height dimensions.",
    "crumbs": [
      "Augmentation"
    ]
  },
  {
    "objectID": "resnets.html",
    "href": "resnets.html",
    "title": "ResNets",
    "section": "",
    "text": "Adapted from:\nset_seed(42)\nplt.style.use(\"ggplot\")\nLet‚Äôs start by cleaning up some of the module implementations.\nsource",
    "crumbs": [
      "ResNets"
    ]
  },
  {
    "objectID": "resnets.html#going-deeper",
    "href": "resnets.html#going-deeper",
    "title": "ResNets",
    "section": "Going deeper",
    "text": "Going deeper\nAt this point, we can try to go deeper by adding a stride 1 convolution layer\n\nclass DeeperCNN(CNN):\n    \"\"\"7 layer convolutional neural network with GeneralReLU\"\"\"\n\n    def get_layers(self, nfs, n_outputs=10, C=None):\n        if C is None:\n            C = Conv\n        assert len(nfs) == 5\n        # Notice we changed the stride to 1 to fit another layer --+\n        layers = [C(1, 8, ks=5, stride=1)]  # üëà ------------------+\n        for c_in, c_out in zip(nfs, nfs[1:]):\n            layers.append(C(c_in, c_out))\n        layers.append(C(nfs[-1], n_outputs, act=False))\n        return layers\n\n\ntrain_1cycle(DeeperCNN.kaiming(nfs=(8, 16, 32, 64, 128)));\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.783\n0.764\n0\ntrain\n\n\n0.818\n0.544\n0\neval\n\n\n0.885\n0.364\n1\ntrain\n\n\n0.873\n0.366\n1\neval\n\n\n0.913\n0.271\n2\ntrain\n\n\n0.895\n0.314\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nThis gives us 90% in 3 epochs, which is the quickest we‚Äôve been able to achieve that accuracy.\nWe want to make our networks wider and deeper, but this has a limit even with an apropriate initialization. In ‚ÄúDeep Residual Learning for Image Recognition,‚Äù Kaiming observed that a 56 layer network had worse performance than a 20 layer network. Why?\nNotice, if the 36 extra layers were \\(I\\), it should have the same performance of the smaller network. In other words, it‚Äôs a superset of the small network. We should be able to table advantage of the initial training dynamics of the shallower network with deeper networks with Skip Connections.\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, inner):\n        self.inner = inner\n\n    def forward(self, x):\n        x_orig = x\n        x = self.inner(x)\n        assert x.shape == x_orig.shape\n        return x + x_orig\n\nNote that the shape must not change after the inner transformation. To do so with a Convolutional Neural Network, we need a very simple ‚ÄúIdentity‚Äù convolution that does the same transformation.\n\nF.avg_pool2d?\n\n\nDocstring:\navg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -&gt; Tensor\nApplies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n:math:`sH \\times sW` steps. The number of output features is equal to the number of\ninput planes.\nSee :class:`~torch.nn.AvgPool2d` for details and output shape.\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n    divisor_override: if specified, it will be used as divisor, otherwise\n         size of the pooling region will be used. Default: None\nType:      builtin_function_or_method\n\n\n\n\nsource\n\nResidualConvBlock\n\n ResidualConvBlock (c_in, c_out, stride=2, ks=3, act=True, norm=True)\n\nConvolutional block with residual links\n\nm = DeeperCNN.kaiming(nfs=(8, 16, 32, 64, 128), block=ResidualConvBlock)\n_ = train_1cycle(m)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.807\n0.583\n0\ntrain\n\n\n0.869\n0.384\n0\neval\n\n\n0.894\n0.289\n1\ntrain\n\n\n0.893\n0.288\n1\neval\n\n\n0.926\n0.201\n2\ntrain\n\n\n0.915\n0.228\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nRecall, the previous best was %90.9 after 5 epochs ü•≥\nLet‚Äôs take a closer look at how the parameters are allocated\n\nsource\n\n\nSummaryCB\n\n SummaryCB (mods=None, mod_filter=&lt;function noop&gt;)\n\nSummarize the model\n\nsource\n\n\nsummarize\n\n summarize (m, mods, dls=&lt;slowai.learner.DataLoaders object at\n            0x7f52347e6350&gt;)\n\n\nfor block in [Conv, ResidualConvBlock]:\n    print(block.__name__)\n    m = DeeperCNN.kaiming(nfs=(8, 16, 32, 64, 128), block=block)\n    summarize(m, m.layers)\n\nConv\n\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nConv\n(512, 1, 28, 28)\n(8, 28, 28)\n216\n0.2\n\n\nConv\n(512, 8, 28, 28)\n(16, 14, 14)\n1,184\n0.2\n\n\nConv\n(512, 16, 14, 14)\n(32, 7, 7)\n4,672\n0.2\n\n\nConv\n(512, 32, 7, 7)\n(64, 4, 4)\n18,560\n0.3\n\n\nConv\n(512, 64, 4, 4)\n(128, 2, 2)\n73,984\n0.3\n\n\nConv\n(512, 128, 2, 2)\n(10, 1, 1)\n11,540\n0.0\n\n\nTotal\n\n\n110,156\n\n\n\n\n\n\nResidualConvBlock\n\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nResidualConvBlock\n(512, 1, 28, 28)\n(8, 28, 28)\n1,848\n1.4\n\n\nResidualConvBlock\n(512, 8, 28, 28)\n(16, 14, 14)\n3,664\n0.7\n\n\nResidualConvBlock\n(512, 16, 14, 14)\n(32, 7, 7)\n14,496\n0.7\n\n\nResidualConvBlock\n(512, 32, 7, 7)\n(64, 4, 4)\n57,664\n0.9\n\n\nResidualConvBlock\n(512, 64, 4, 4)\n(128, 2, 2)\n230,016\n0.9\n\n\nResidualConvBlock\n(512, 128, 2, 2)\n(10, 1, 1)\n13,750\n0.0\n\n\nTotal\n\n\n321,438\n\n\n\n\n\n\nIndeed, we have almost 3x as many paramters and the training dynamics are quite stable!\nHow does this compare to a standard implementation?\n\ndef train_timm(id):\n    m = timm.create_model(id, in_chans=1, num_classes=10)\n    m.layers = []  # Because we're not recording anything\n    np = sum(p.numel() for p in m.parameters())\n    print(f\"N. parameters: {np:,}\")\n    train_1cycle(m)\n\n\ntrain_timm(\"resnet18\")\n\nN. parameters: 11,175,370\n\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.776\n0.663\n0\ntrain\n\n\n0.729\n0.935\n0\neval\n\n\n0.884\n0.312\n1\ntrain\n\n\n0.890\n0.316\n1\neval\n\n\n0.914\n0.230\n2\ntrain\n\n\n0.906\n0.260\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nSlightly better! Of course, this model has 30x more parameters, so it‚Äôs not surprising.\nHow does this compare to a network without these links?\n\nclass DoubleConvBlock(nn.Module):\n    \"\"\"Convolutional block with residual links\"\"\"\n\n    def __init__(self, c_in, c_out, stride=2, ks=3, act=True, norm=True):\n        super().__init__()\n        self.conv_a = Conv(c_in, c_out, stride=1, ks=ks, act=act, norm=norm)\n        self.conv_b = Conv(c_out, c_out, stride=stride, ks=ks, act=act, norm=norm)\n\n    def forward(self, x):\n        x = self.conv_a(x)\n        x = self.conv_b(x)\n        return x\n\n\nm = DeeperCNN.kaiming(nfs=(8, 16, 32, 64, 128), block=DoubleConvBlock)\n\n\nshape(m)\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00&lt;?]\n    \n    \n\n\n    \n      \n      0.00% [0/118 00:00&lt;?]\n    \n    \n\n\n\n\n\nType\nInput\nOutput\nN. params\n\n\n\n\nDoubleConvBlock\n(512, 1, 28, 28)\n(8, 28, 28)\n1,832\n\n\nDoubleConvBlock\n(512, 8, 28, 28)\n(16, 14, 14)\n3,520\n\n\nDoubleConvBlock\n(512, 16, 14, 14)\n(32, 7, 7)\n13,952\n\n\nDoubleConvBlock\n(512, 32, 7, 7)\n(64, 4, 4)\n55,552\n\n\nDoubleConvBlock\n(512, 64, 4, 4)\n(128, 2, 2)\n221,696\n\n\nDoubleConvBlock\n(512, 128, 2, 2)\n(10, 1, 1)\n12,460\n\n\nTotal\n\n\n309,012\n\n\n\n\n\n\n_ = train_1cycle(m)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.793\n0.730\n0\ntrain\n\n\n0.806\n0.599\n0\neval\n\n\n0.889\n0.348\n1\ntrain\n\n\n0.898\n0.304\n1\neval\n\n\n0.920\n0.247\n2\ntrain\n\n\n0.910\n0.275\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nInterestingly, only the slightest bit worse",
    "crumbs": [
      "ResNets"
    ]
  },
  {
    "objectID": "learner.html",
    "href": "learner.html",
    "title": "Learner",
    "section": "",
    "text": "Adapted from:\nAt this point, Jeremy points out that copying and pasting code leads to bottlenecks in modeling velocity. We need to start to build a framework to:",
    "crumbs": [
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#data",
    "href": "learner.html#data",
    "title": "Learner",
    "section": "Data",
    "text": "Data\nWe‚Äôll start with a wrapper around datasets to make it simpler to work with raw PyTorch.\n\nsource\n\nDataLoaders\n\n DataLoaders (splits, nworkers:int=2, bs=32, collate_fn=&lt;function\n              default_collate&gt;, tdir='/tmp/tmpriwe92on')\n\nWrapper around huggingface datasets to facilitate raw pytorch work\n\ndls = DataLoaders.from_hf(\"fashion_mnist\", nworkers=2)\n\n\ndls.splits.set_format(\"torch\")  # This will be overwritten in a second\nbatch = dls.peek()\nbatch[\"image\"].shape, batch[\"label\"].shape\n\n(torch.Size([32, 28, 28]), torch.Size([32]))\n\n\nWe should also add some helpers to facilitate processing images.\n\nsource\n\n\ntensorize_images\n\n tensorize_images (dls, feature='image', normalize=True,\n                   pipe=[PILToTensor(), ConvertImageDtype()])\n\nTensorize and normalize the image feature\n\nsource\n\n\nbatchify\n\n batchify (f)\n\nConvert a function that processes a single feature to processing a list of features\n\nT.Normalize?\n\n\nInit signature: T.Normalize(mean, std, inplace=False)\nDocstring:     \nNormalize a tensor image with mean and standard deviation.\nThis transform does not support PIL Image.\nGiven mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\nchannels, this transform will normalize each channel of the input\n``torch.*Tensor`` i.e.,\n``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n.. note::\n    This transform acts out of place, i.e., it does not mutate the input tensor.\nArgs:\n    mean (sequence): Sequence of means for each channel.\n    std (sequence): Sequence of standard deviations for each channel.\n    inplace(bool,optional): Bool to make this operation in-place.\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/micromamba/envs/slowai/lib/python3.11/site-packages/torchvision/transforms/transforms.py\nType:           type\nSubclasses:     \n\n\n\n\ndls = DataLoaders.from_hf(\"fashion_mnist\", nworkers=0)\ndls = tensorize_images(dls)\nxb = dls.peek()[\"image\"]\nshow_images(xb[:8, ...], figsize=(8, 4))\n\n\n\n\n\n\n\n\n\nxb.min(), xb.max()\n\n(tensor(-0.8286), tensor(2.0066))\n\n\nNotice that this unit-normalized\n\nplt.hist(xb.view(-1))\n\n(array([13418.,   607.,   687.,  1014.,  1057.,  1076.,  1408.,  2054.,\n         2393.,  1374.]),\n array([-0.82863587, -0.5451138 , -0.26159173,  0.02193036,  0.30545244,\n         0.58897448,  0.8724966 ,  1.15601861,  1.43954074,  1.72306275,\n         2.00658488]),\n &lt;BarContainer object of 10 artists&gt;)",
    "crumbs": [
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#learner-and-callbacks",
    "href": "learner.html#learner-and-callbacks",
    "title": "Learner",
    "section": "Learner and callbacks",
    "text": "Learner and callbacks\nNext, we‚Äôll add a learner with callbacks. Recall, this was our earlier fit function:\n\nfit??\n\n\nSignature: fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False):\n    progress = tqdm if tqdm_ else lambda x: x\n    for epoch in range(epochs):\n        model.train()\n        for batch in progress(train_dl):\n            xb, yb = map(to_device, batch)\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        model.eval()\n        with torch.no_grad():\n            tot_loss, tot_acc, count = 0.0, 0.0, 0\n            for batch in progress(valid_dl):\n                xb, yb = map(to_device, batch)\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred, yb).item() * n\n                tot_acc += accuracy(pred, yb).item() * n\n        print(\n            f\"{epoch=}, validation loss={tot_loss / count:.3f}, validation accuracy={tot_acc / count:.2f}\"\n        )\n    return tot_loss / count, tot_acc / count\nFile:      ~/Desktop/SlowAI/nbs/slowai/convs.py\nType:      function\n\n\n\nTo add callbacks, we need a few clever Exception control flow signals\n\nsource\n\nCancelEpochException\nSkip to the next epoch\n\nsource\n\n\nCancelBatchException\nSkip to the next batch\n\nsource\n\n\nCancelFitException\nExit fit context\nThen, we define the learner and callback classes\n\nsource\n\n\nCallback\n\n Callback ()\n\nModify the training behavior\n\nsource\n\n\nwith_cbs\n\n with_cbs (nm)\n\nRun the callbacks lifecycle at the apropriate time\n\nsource\n\n\nonly\n\n only (f)\n\nIf the lifecycle hook is decorated as such, only run this hook and not other callbacks‚Äô hooks\n\nsource\n\n\nLearner\n\n Learner (model, dls, loss_func=&lt;function mse_loss&gt;, lr=0.1, cbs=None,\n          opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nFlexible training loop\nThis learner delegates all aspects of model training to callbacks, so something like this is neccesary.\n\nsource\n\n\nTrainCB\n\n TrainCB ()\n\nTraining specific behaviors for the Learner\nNow that we have the basic scaffolding, we‚Äôll add metrics. Updating and storing state will be handled by torchmetrics, but we‚Äôll define a callback to orchestrate the torchmetrics instances.\n\nsource\n\n\nMetricsCB\n\n MetricsCB (*ms, **metrics)\n\nUpdate and print metrics\nFinally, we can define a Trainer callback specifically for the autoencoder objective.\n\nclass TrainAutoencoderCB(TrainCB):\n    \"\"\"Modify the training loop for the ELBO objective\"\"\"\n\n    def predict(self, learn):\n        xb, *_ = learn.batch\n        learn.preds = learn.model(xb)\n\n    def get_loss(self, learn):\n        xb, *_ = learn.batch\n        learn.loss = learn.loss_func(learn.preds, xb)\n\nLet‚Äôs also define some additional useful callbacks and dataset helpers:\n\nsource\n\n\nProgressCB\n\n ProgressCB (plot=False, periodicity=10)\n\nReport the progress\n\nsource\n\n\nbefore\n\n before (callback_cls:Union[Sequence[Type[__main__.Callback]],Type[__main_\n         _.Callback]])\n\nRun a callback before another callback\n\nsource\n\n\nafter\n\n after (callback_cls:Union[Sequence[Type[__main__.Callback]],Type[__main__\n        .Callback]])\n\nRun a callback after another callback\n\nsource\n\n\nDeviceCB\n\n DeviceCB (device='cpu')\n\nMove tensors and model to the CPU/GPU/etc\n\nsource\n\n\nto_cpu\n\n to_cpu (x)\n\n\nsource\n\n\nfashion_mnist\n\n fashion_mnist (bs=2048, **kwargs)\n\nHelper to use fashion MNIST\n\nDataLoaders??\n\n\nInit signature:\nDataLoaders(\n    splits,\n    nworkers: int = 6,\n    bs=32,\n    collate_fn=&lt;function default_collate at 0x7f88ed959120&gt;,\n    tdir='/tmp/tmpmsi_fg04',\n)\nDocstring:      Wrapper around huggingface datasets to facilitate raw pytorch work\nType:           type\nSubclasses:     \n\n\n\nPutting it all together\n\nmodel = get_ae_model()\ndls = fashion_mnist()\nprint(dls.splits[\"train\"].format)\ncbs = [\n    MetricsCB(),\n    DeviceCB(),\n    TrainAutoencoderCB(),\n    ProgressCB(plot=True),\n]\nlearn = Learner(\n    model,\n    dls,\n    F.mse_loss,\n    lr=0.01,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\n\n{'type': 'custom', 'format_kwargs': {'transform': &lt;function DataLoaders.with_transforms.&lt;locals&gt;.map_ at 0x7f885457b1a0&gt;}, 'columns': ['image', 'label'], 'output_all_columns': False}\n\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.071\n0\ntrain\n\n\n0.955\n0\neval\n\n\n0.908\n1\ntrain\n\n\n0.854\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 3.98 s, sys: 2.42 s, total: 6.4 s\nWall time: 11.2 s\n\n\n\ndef viz(model, xb):\n    xb = xb.to(def_device)\n    pred = model(xb)\n    paired = []\n    for i in range(min(xb.shape[0], 8)):\n        paired.append(xb[i, ...])\n        paired.append(pred[i, ...])\n    show_images(paired, figsize=(8, 8))\n\n\nxbt, _ = dls.peek(\"test\")\nviz(model, xbt)\n\n\n\n\n\n\n\n\nStill not good, but less code!\nI don‚Äôt really like the idea of delegating the core training functions to callbacks, so we can just implement them here:\n\nsource\n\n\nTrainLearner\n\n TrainLearner (model, dls, loss_func=&lt;function mse_loss&gt;, lr=0.1,\n               cbs=None, opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nSane training loop\nThis works pretty similarly\n\nclass AutoencoderTrainer(TrainLearner):\n    def predict(self):\n        xb, *_ = self.batch\n        self.preds = self.model(xb)\n\n    def get_loss(self):\n        xb, *_ = self.batch\n        self.loss = self.loss_func(self.preds, xb)\n\n\ncbs = [MetricsCB(), DeviceCB(), ProgressCB(plot=True)]\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=0.01,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.950\n0\ntrain\n\n\n0.585\n0\neval\n\n\n0.566\n1\ntrain\n\n\n0.556\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 1.24 s, sys: 1.37 s, total: 2.61 s\nWall time: 8.12 s\n\n\nCan we improve the reconstruction? Let‚Äôs implement a simple momentum.\n\nsource\n\n\nMomentumCB\n\n MomentumCB (momentum=0.85)\n\nModify the training behavior\n\ncbs = [MetricsCB(), DeviceCB(), ProgressCB(plot=True), MomentumCB()]\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=0.01,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\nviz(model, xbt)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.254\n0\ntrain\n\n\n1.209\n0\neval\n\n\n1.184\n1\ntrain\n\n\n1.148\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 1.46 s, sys: 1.41 s, total: 2.87 s\nWall time: 8.6 s\n\n\n\n\n\n\n\n\n\nNot especially impressive.\nWhat about using the automated learning rate finder?\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\nsource\n\n\nLRFinderCB\n\n LRFinderCB (gamma=1.3, max_mult=3)\n\nFind an apopriate learning rate by increasing it by a constant factor for each batch until the loss diverges\n\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=1e-5,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).lr_find()\n\n\n\n\n\n\n    \n      \n      20.00% [2/10 00:07&lt;00:29]\n    \n    \n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.294\n0\ntrain\n\n\n1.068\n1\ntrain\n\n\n\n\n\n    \n      \n      56.67% [17/30 00:01&lt;00:01 1.987]\n    \n    \n\n\n/home/jeremy/micromamba/envs/slowai/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered `nan` values in tensor. Will be removed.\n  warnings.warn(*args, **kwargs)  # noqa: B028\n\n\n\n\n\n\n\n\n\nIt looks like 1e-2 is a good learning rate.\n\ncbs = [MetricsCB(), DeviceCB(), ProgressCB(plot=True), MomentumCB()]\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=1e-2,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\nviz(model, xbt)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.905\n0\ntrain\n\n\n0.625\n0\neval\n\n\n0.592\n1\ntrain\n\n\n0.563\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, not especially impressive.\nWe‚Äôll write some tools to diagnose model issues in the next notebook.",
    "crumbs": [
      "Learner"
    ]
  },
  {
    "objectID": "attention.html",
    "href": "attention.html",
    "title": "Attention and Conditionality",
    "section": "",
    "text": "Adapted from\n\nhttps://youtu.be/DH5bp6zTPB4?si=ziAq_45vkifFx1R4&t=2942\n\nAttention is a ‚Äúhigh-pass‚Äù filter that can help with the limitations of a ‚Äúlow-pass‚Äù filter such as a convolution.\nHuggingface diffusers implements a 1D-attention by rasterizing the image, which is how we will implement this (although this is known to be ‚Äúsuboptimal,‚Äù accourding to Howard.) Then, each pixel has a \\(C\\)-dimensional embedding.\nJohno mentions that softmax tends to assign all weight to a single dimension, but this is often undesirable. Multi-headedness compensates for this by creating many orthoganol subspaces within which attention is assigned.\nSee jer.fish/posts/notes-on-self-attention for more details.\n\nsource\n\nMultiheadSelfAttention1D\n\n MultiheadSelfAttention1D (nc, nh)\n\nMulti-head self-attention\n\nattn = MultiheadSelfAttention1D(nc=32, nh=4)\n\n\nx = torch.randn(8, 32, 16, 16)\nattn(x).shape\n\ntorch.Size([8, 32, 16, 16])\n\n\nLet‚Äôs use this in a Diffusion UNet. Note that we cannot use attention near the beginning or head due to the quadratic time and space capacity (there are more time steps at these points). Typically, attention is only used when the feature map is 16x16 or 32x32 or higher.\n\nsource\n\n\nTAResBlock\n\n TAResBlock (t_embed, c_in, c_out, ks=3, stride=2, nh=None)\n\nRes-block with attention\n\nsource\n\n\nTADownblock\n\n TADownblock (t_embed, c_in, c_out, downsample=True, n_layers=1, nh=None)\n\nResdownblock with attention\n\nsource\n\n\nTAUpblock\n\n TAUpblock (t_embed, c_in, c_out, upsample=True, n_layers=1, nh=None)\n\nResupblock with attention\n\nsource\n\n\nTAUnet\n\n TAUnet (nfs=(224, 448, 672, 896), attention_heads=(0, 8, 8, 8),\n         n_blocks=(3, 2, 2, 1, 1), color_channels=3)\n\nU-net with attention up/down-blocks\n\ndls = get_fashion_dls(512)\n\n\nun = train(\n    TAUnet(\n        color_channels=1,\n        nfs=(32, 64, 128, 256, 384),\n        n_blocks=(3, 2, 1, 1, 1, 1),\n        attention_heads=(0, 8, 8, 8, 8, 8),\n    ),\n    dls,\n    lr=1e-3,\n    n_epochs=25,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.282\n0\ntrain\n\n\n0.114\n0\neval\n\n\n0.084\n1\ntrain\n\n\n0.073\n1\neval\n\n\n0.059\n2\ntrain\n\n\n0.055\n2\neval\n\n\n0.048\n3\ntrain\n\n\n0.045\n3\neval\n\n\n0.042\n4\ntrain\n\n\n0.045\n4\neval\n\n\n0.039\n5\ntrain\n\n\n0.064\n5\neval\n\n\n0.037\n6\ntrain\n\n\n0.043\n6\neval\n\n\n0.035\n7\ntrain\n\n\n0.040\n7\neval\n\n\n0.034\n8\ntrain\n\n\n0.037\n8\neval\n\n\n0.032\n9\ntrain\n\n\n0.050\n9\neval\n\n\n0.033\n10\ntrain\n\n\n0.036\n10\neval\n\n\n0.031\n11\ntrain\n\n\n0.032\n11\neval\n\n\n0.030\n12\ntrain\n\n\n0.031\n12\neval\n\n\n0.030\n13\ntrain\n\n\n0.030\n13\neval\n\n\n0.029\n14\ntrain\n\n\n0.030\n14\neval\n\n\n0.029\n15\ntrain\n\n\n0.030\n15\neval\n\n\n0.028\n16\ntrain\n\n\n0.028\n16\neval\n\n\n0.028\n17\ntrain\n\n\n0.030\n17\neval\n\n\n0.028\n18\ntrain\n\n\n0.028\n18\neval\n\n\n0.028\n19\ntrain\n\n\n0.029\n19\neval\n\n\n0.028\n20\ntrain\n\n\n0.028\n20\neval\n\n\n0.027\n21\ntrain\n\n\n0.027\n21\neval\n\n\n0.027\n22\ntrain\n\n\n0.027\n22\neval\n\n\n0.027\n23\ntrain\n\n\n0.027\n23\neval\n\n\n0.027\n24\ntrain\n\n\n0.028\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 16min 3s, sys: 2min 6s, total: 18min 9s\nWall time: 18min 16s\n\n\n\nx_0, _ = ddpm(un, (8, 1, 32, 32), n_steps=100)\nshow_images(x_0, imsize=0.8)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 249/249 [00:01&lt;00:00, 130.57time step/s]\n\n\n\n\n\n\n\n\n\n\ndel un\n\n\n\nAdding conditionality\n\nsource\n\nConditionalTAUnet\n\n ConditionalTAUnet (n_classes, nfs=(224, 448, 672, 896),\n                    attention_heads=(0, 8, 8, 8), n_blocks=(3, 2, 2, 1,\n                    1), color_channels=3)\n\nU-net with attention up/down-blocks\n\nsource\n\n\nConditionalFashionDDPM\n\n ConditionalFashionDDPM ()\n\nTraining specific behaviors for the Learner\n\nsource\n\n\nconditional_train\n\n conditional_train (model, dls, lr=0.004, n_epochs=25, extra_cbs=[],\n                    loss_fn=&lt;function mse_loss&gt;)\n\n\nun = conditional_train(\n    ConditionalTAUnet(\n        n_classes=10,\n        color_channels=1,\n        nfs=(32, 64, 128, 256, 384),\n        n_blocks=(3, 2, 1, 1, 1, 1),\n        attention_heads=(0, 8, 8, 8, 8, 8),\n    ),\n    dls,\n    lr=4e-3,\n    n_epochs=25,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.158\n0\ntrain\n\n\n0.087\n0\neval\n\n\n0.058\n1\ntrain\n\n\n0.056\n1\neval\n\n\n0.045\n2\ntrain\n\n\n0.054\n2\neval\n\n\n0.039\n3\ntrain\n\n\n0.042\n3\neval\n\n\n0.036\n4\ntrain\n\n\n0.050\n4\neval\n\n\n0.034\n5\ntrain\n\n\n0.043\n5\neval\n\n\n0.033\n6\ntrain\n\n\n0.035\n6\neval\n\n\n0.031\n7\ntrain\n\n\n0.039\n7\neval\n\n\n0.030\n8\ntrain\n\n\n0.035\n8\neval\n\n\n0.030\n9\ntrain\n\n\n0.050\n9\neval\n\n\n0.030\n10\ntrain\n\n\n0.039\n10\neval\n\n\n0.028\n11\ntrain\n\n\n0.035\n11\neval\n\n\n0.028\n12\ntrain\n\n\n0.029\n12\neval\n\n\n0.028\n13\ntrain\n\n\n0.029\n13\neval\n\n\n0.027\n14\ntrain\n\n\n0.028\n14\neval\n\n\n0.027\n15\ntrain\n\n\n0.027\n15\neval\n\n\n0.026\n16\ntrain\n\n\n0.027\n16\neval\n\n\n0.026\n17\ntrain\n\n\n0.026\n17\neval\n\n\n0.026\n18\ntrain\n\n\n0.026\n18\neval\n\n\n0.026\n19\ntrain\n\n\n0.026\n19\neval\n\n\n0.026\n20\ntrain\n\n\n0.026\n20\neval\n\n\n0.025\n21\ntrain\n\n\n0.026\n21\neval\n\n\n0.025\n22\ntrain\n\n\n0.026\n22\neval\n\n\n0.025\n23\ntrain\n\n\n0.026\n23\neval\n\n\n0.025\n24\ntrain\n\n\n0.025\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 16min 1s, sys: 2min 6s, total: 18min 8s\nWall time: 18min 16s\n\n\n\nsource\n\n\nconditional_ddpm\n\n conditional_ddpm (model, c, sz=(16, 1, 32, 32), device='cpu',\n                   n_steps=100)\n\n\nc = torch.arange(0, 9)\nx_0, _ = conditional_ddpm(un, c, (9, 1, 32, 32))\nshow_images(x_0, imsize=0.8);\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00&lt;00:00, 119.07time step/s]\n\n\n\n\n\n\n\n\n\n\nx_0, _ = conditional_ddpm(un, c, (9, 1, 32, 32))\nshow_images(x_0, imsize=0.8);\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00&lt;00:00, 131.05time step/s]",
    "crumbs": [
      "Attention and Conditionality"
    ]
  },
  {
    "objectID": "investigate_kink.html",
    "href": "investigate_kink.html",
    "title": "Investigating the loss kink while training diffusion U-Net",
    "section": "",
    "text": "aesthetics()\n\n\ndls = get_fashion_dls(bs=512)\n\n\nm = TUnet(\n    color_channels=1,\n    nfs=(32, 64, 128, 256, 384),\n    n_blocks=(3, 2, 1, 1, 1, 1),\n)\nblocks = [*m.downblocks, *m.upblocks]\nstats = StoreModuleStatsCB(\n    sum([b.convs for b in blocks], nn.ModuleList()),\n    hook_kwargs={\"periodicity\": 1},\n)\ntrain(\n    m,\n    dls,\n    lr=4e-3,\n    n_epochs=2,\n    extra_cbs=[stats],\n)\nstats.mean_std_plot()\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.121\n0\ntrain\n\n\n0.074\n0\neval\n\n\n0.044\n1\ntrain\n\n\n0.040\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 19min 6s, sys: 4min 29s, total: 23min 35s\nWall time: 6min 56s\n\n\n\n\n\n\n\n\n\n\nclass KTUnet(TUnet, KaimingMixin):\n    ...\n\n\nm = KTUnet.kaiming(\n    color_channels=1,\n    nfs=(32, 64, 128, 256, 384),\n    n_blocks=(3, 2, 1, 1, 1, 1),\n)\nblocks = [*m.downblocks, *m.upblocks]\nstats = StoreModuleStatsCB(\n    sum([b.convs for b in blocks], nn.ModuleList()),\n    hook_kwargs={\"periodicity\": 1},\n)\ntrain(\n    m,\n    dls,\n    lr=1e-3,\n    n_epochs=2,\n    extra_cbs=[stats],\n)\nstats.mean_std_plot()\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.866\n0\ntrain\n\n\n0.754\n0\neval\n\n\n0.128\n1\ntrain\n\n\n0.097\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 17min 40s, sys: 7.59 s, total: 17min 48s\nWall time: 4min 10s\n\n\n\n\n\n\n\n\n\nThis experiment demonstrates that the issue is with Kaiming initialization, which I analyze further here in the FastAI forums.",
    "crumbs": [
      "Investigating the loss kink while training diffusion U-Net"
    ]
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "VAE",
    "section": "",
    "text": "Adapted from: - https://www.youtube.com/watch?v=8AgZ9jcQ9v8&list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP&index=17\n\nsource\n\nPerceptron\n\n Perceptron (c_in, c_out, bias=True, act=&lt;class\n             'torch.nn.modules.activation.SiLU'&gt;)\n\n*A sequential container.\nModules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module it contains. It then ‚Äúchains‚Äù outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are each a registered submodule of the Sequential).\nWhat‚Äôs the difference between a Sequential and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like‚Äìa list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))*\n\nsource\n\n\nKaimingMixin\n\n KaimingMixin ()\n\nHelper to initialize the network using Kaiming\n\nsource\n\n\nVAE\n\n VAE (c_in, c_hidden, c_bottleneck, layers=1)\n\nVariational autoencoder\nSigma can go to 0 to preserve data in the activations, so we need a new loss function to make sure that the hidden distribution is normal. This is known as ‚ÄúKullback‚ÄìLeibler divergence‚Äù or ‚ÄúKLD‚Äù loss. This reaches a minimum when Œº is 0 and œÉ is 1.\n\nsource\n\n\nkld_loss\n\n kld_loss (Œº, log_œÉ, eps=0, dim=None)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(4, 4))\nax.set(xlabel=r\"$\\mu$\", ylabel=\"KL divergence loss\")\nfor log_sigma_ in torch.linspace(0, 3, 5):\n    mu = torch.linspace(-3, 3, 100).unsqueeze(0)\n    log_sigma = torch.full((1, 1), log_sigma_)\n    loss = kld_loss(mu, log_sigma, dim=0)\n    ax.plot(mu.squeeze(), loss, label=r\"$\\sigma=${}\".format(log_sigma_.item()))\nfig.legend();\n\n\n\n\n\n\n\n\nThis is added to a normal reconstruction loss.\n\nsource\n\n\nvae_loss\n\n vae_loss (inputs, x_pred, Œº, log_œÉ)\n\nWe want to be able to keep track of the KLD loss over time, so let‚Äôs track it in a metric.\n\nsource\n\n\nMeanlikeMetric\n\n MeanlikeMetric (**kwargs)\n\n*Base class for all metrics present in the Metrics API.\nThis class is inherited by all metrics and implements the following functionality: 1. Handles the transfer of metric states to correct device 2. Handles the synchronization of metric states across processes\nThe three core methods of the base class are * add_state() * forward() * reset()\nwhich should almost never be overwritten by child classes. Instead, the following methods should be overwritten * update() * compute()\nArgs: kwargs: additional keyword arguments, see :ref:Metric kwargs for more info.\n    - compute_on_cpu: If metric state should be stored on CPU during computations. Only works for list states.\n    - dist_sync_on_step: If metric state should synchronize on ``forward()``. Default is ``False``\n    - process_group: The process group on which the synchronization is called. Default is the world.\n    - dist_sync_fn: Function that performs the allgather option on the metric state. Default is an custom\n      implementation that calls ``torch.distributed.all_gather`` internally.\n    - distributed_available_fn: Function that checks if the distributed backend is available. Defaults to a\n      check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n    - sync_on_compute: If metric state should synchronize when ``compute`` is called. Default is ``True``\n    - compute_with_cache: If results from ``compute`` should be cached. Default is ``True``*\n\nsource\n\n\nKLDMetric\n\n KLDMetric (**kwargs)\n\n*Base class for all metrics present in the Metrics API.\nThis class is inherited by all metrics and implements the following functionality: 1. Handles the transfer of metric states to correct device 2. Handles the synchronization of metric states across processes\nThe three core methods of the base class are * add_state() * forward() * reset()\nwhich should almost never be overwritten by child classes. Instead, the following methods should be overwritten * update() * compute()\nArgs: kwargs: additional keyword arguments, see :ref:Metric kwargs for more info.\n    - compute_on_cpu: If metric state should be stored on CPU during computations. Only works for list states.\n    - dist_sync_on_step: If metric state should synchronize on ``forward()``. Default is ``False``\n    - process_group: The process group on which the synchronization is called. Default is the world.\n    - dist_sync_fn: Function that performs the allgather option on the metric state. Default is an custom\n      implementation that calls ``torch.distributed.all_gather`` internally.\n    - distributed_available_fn: Function that checks if the distributed backend is available. Defaults to a\n      check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n    - sync_on_compute: If metric state should synchronize when ``compute`` is called. Default is ``True``\n    - compute_with_cache: If results from ``compute`` should be cached. Default is ``True``*\n\nsource\n\n\nBCEMetric\n\n BCEMetric (**kwargs)\n\n*Base class for all metrics present in the Metrics API.\nThis class is inherited by all metrics and implements the following functionality: 1. Handles the transfer of metric states to correct device 2. Handles the synchronization of metric states across processes\nThe three core methods of the base class are * add_state() * forward() * reset()\nwhich should almost never be overwritten by child classes. Instead, the following methods should be overwritten * update() * compute()\nArgs: kwargs: additional keyword arguments, see :ref:Metric kwargs for more info.\n    - compute_on_cpu: If metric state should be stored on CPU during computations. Only works for list states.\n    - dist_sync_on_step: If metric state should synchronize on ``forward()``. Default is ``False``\n    - process_group: The process group on which the synchronization is called. Default is the world.\n    - dist_sync_fn: Function that performs the allgather option on the metric state. Default is an custom\n      implementation that calls ``torch.distributed.all_gather`` internally.\n    - distributed_available_fn: Function that checks if the distributed backend is available. Defaults to a\n      check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n    - sync_on_compute: If metric state should synchronize when ``compute`` is called. Default is ``True``\n    - compute_with_cache: If results from ``compute`` should be cached. Default is ``True``*\n\nsource\n\n\nMetricsCBWithKLDAndBCE\n\n MetricsCBWithKLDAndBCE (*ms, **metrics)\n\nUpdate and print metrics\n\nsource\n\n\nVAETrainCB\n\n VAETrainCB ()\n\nTraining specific behaviors for the Learner\n\nsource\n\n\ntrain\n\n train (model, dls, lr=0.004, n_epochs=4, extra_cbs=[], loss_fn=&lt;function\n        vae_loss&gt;)\n\nSet up the training run\n\ndls = fashion_mnist(normalize=False, bs=256)\n\n\nx, _ = dls.peek()\nx.shape\n\ntorch.Size([256, 1, 28, 28])\n\n\n\nx.min(), x.max()\n\n(tensor(0.), tensor(1.))\n\n\n\nsource\n\n\nFashionMNISTForReconstruction\n\n FashionMNISTForReconstruction ()\n\nModify the training behavior\n\nvae = train(\n    VAE.kaiming((28**2), 400, 200, layers=2),\n    dls,\n    extra_cbs=[\n        FashionMNISTForReconstruction(),\n    ],\n    n_epochs=20,\n    lr=3e-2,\n)\n\n\n\n\n\n\n\n\nloss\nkld\nbce\nepoch\ntrain\n\n\n\n\n0.984\n0.427\n0.557\n0\ntrain\n\n\n0.723\n0.234\n0.489\n0\neval\n\n\n0.546\n0.097\n0.448\n1\ntrain\n\n\n0.453\n0.046\n0.408\n1\neval\n\n\n0.419\n0.034\n0.385\n2\ntrain\n\n\n0.401\n0.035\n0.365\n2\neval\n\n\n0.393\n0.034\n0.359\n3\ntrain\n\n\n0.390\n0.039\n0.350\n3\neval\n\n\n0.381\n0.033\n0.348\n4\ntrain\n\n\n0.377\n0.032\n0.345\n4\neval\n\n\n0.371\n0.031\n0.341\n5\ntrain\n\n\n0.369\n0.029\n0.340\n5\neval\n\n\n0.362\n0.029\n0.333\n6\ntrain\n\n\n0.359\n0.028\n0.330\n6\neval\n\n\n0.354\n0.028\n0.325\n7\ntrain\n\n\n0.354\n0.028\n0.325\n7\neval\n\n\n0.350\n0.029\n0.321\n8\ntrain\n\n\n0.352\n0.029\n0.323\n8\neval\n\n\n0.348\n0.029\n0.319\n9\ntrain\n\n\n0.350\n0.029\n0.320\n9\neval\n\n\n0.347\n0.029\n0.318\n10\ntrain\n\n\n0.348\n0.029\n0.319\n10\neval\n\n\n0.346\n0.030\n0.316\n11\ntrain\n\n\n0.347\n0.030\n0.316\n11\neval\n\n\n0.345\n0.030\n0.315\n12\ntrain\n\n\n0.345\n0.030\n0.315\n12\neval\n\n\n0.344\n0.030\n0.314\n13\ntrain\n\n\n0.345\n0.030\n0.314\n13\neval\n\n\n0.343\n0.030\n0.313\n14\ntrain\n\n\n0.343\n0.030\n0.313\n14\neval\n\n\n0.343\n0.030\n0.312\n15\ntrain\n\n\n0.342\n0.030\n0.312\n15\neval\n\n\n0.342\n0.030\n0.312\n16\ntrain\n\n\n0.342\n0.030\n0.311\n16\neval\n\n\n0.341\n0.030\n0.311\n17\ntrain\n\n\n0.341\n0.030\n0.310\n17\neval\n\n\n0.341\n0.031\n0.311\n18\ntrain\n\n\n0.341\n0.030\n0.310\n18\neval\n\n\n0.341\n0.031\n0.310\n19\ntrain\n\n\n0.341\n0.030\n0.310\n19\neval\n\n\n\n\n\n\n\n\n\n\n\n\nIt took quite a bit of work to ensure that these results matched Howards‚Äô:\n\nUse SiLU instead of ReLU\nInitialize leakily\nNormalize the output before visualizing\nEnsure there were the number of encoder layers as decoder layers\nUse the original FashionMNIST, not the one upsampled to 32x32 for DDPM.\n\n\nwith torch.no_grad():\n    xb = rearrange(x, \"b c h w -&gt; b (c h w)\")\n    xb_pred, _, _ = to_cpu(vae(xb.cuda()))\nxb_pred = rearrange(xb_pred.sigmoid(), \"b (c h w) -&gt; b c h w\", c=1, h=28, w=28)\nxb_pred = xb_pred.float()\n\n\nshow_images(x[:8], imsize=0.8);\n\n\n\n\n\n\n\n\n\nshow_images(xb_pred[:8], imsize=0.8);",
    "crumbs": [
      "VAE"
    ]
  },
  {
    "objectID": "stable_sgd.html",
    "href": "stable_sgd.html",
    "title": "Optimizers and Schedulers",
    "section": "",
    "text": "Adapted from:\nset_seed(42)\nplt.style.use(\"ggplot\")\nsource",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "stable_sgd.html#implementing-stochastic-gradient-descent",
    "href": "stable_sgd.html#implementing-stochastic-gradient-descent",
    "title": "Optimizers and Schedulers",
    "section": "Implementing Stochastic Gradient Descent",
    "text": "Implementing Stochastic Gradient Descent\nLet‚Äôs start to implement our own optimizer class. Remember, the learner class interfaces with the optimizer solely through the constructor and the step function.\n@with_cbs(\"batch\")\ndef _one_batch(self):\n    self.predict()\n    self.callback(\"after_predict\")\n    self.get_loss()\n    self.callback(\"after_loss\")\n    if self.training:\n        self.backward()\n        self.callback(\"after_backward\")\n        self.step() # üëà\n        self.callback(\"after_step\")\n        self.zero_grad()\n# ...\n\ndef fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n    with tempfile.TemporaryDirectory() as tdir:\n        self.dls.tdir = tdir\n        cbs = fc.L(cbs)\n        # `add_cb` and `rm_cb` were added in lesson 18\n        for cb in cbs:\n            self.cbs.append(cb)\n        try:\n            self.n_epochs = n_epochs\n            self.epochs = range(n_epochs)\n            if lr is None:\n                lr = self.lr\n            if self.opt_func:\n                self.opt = self.opt_func(self.model.parameters(), lr) # üëà\n            self._fit(train, valid)\n        finally:\n            for cb in cbs:\n                self.cbs.remove(cb)\nTherefore, we only need to implement those two interfaces. Let‚Äôs also add (and discuss) weight decay.\n\nclass SGD:\n    def __init__(self, params, lr, wd=0):\n        params = list(params)\n        fc.store_attr()\n        self.i = 0\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params:\n                self.reg_step(p)\n                self.opt_step(p)\n        self.i += 1\n\n    def opt_step(self, p):\n        p -= p.grad * self.lr\n\n    def reg_step(self, p):\n        if self.wd != 0:\n            p *= 1 - self.lr * self.wd\n\n    def zero_grad(self):\n        for p in self.params:\n            p.grad.data.zero_()",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "stable_sgd.html#weight-decay-regularization",
    "href": "stable_sgd.html#weight-decay-regularization",
    "title": "Optimizers and Schedulers",
    "section": "Weight Decay Regularization",
    "text": "Weight Decay Regularization\nWe‚Äôve added weight decay regularization here.\nSuppose we want to apply a loss on the weights themselves as a regularization? (Where \\(k\\) is the weight decay parameter.) \\[\nL = loss(y) + loss(w) = loss(y) + \\sum_{i=1}^d k (w_{i})^2\n\\]\nWe need the derivative to compute the backwards gradient.\n\\[\n\\frac{\\partial L}{\\partial w} =  \\frac{\\partial loss(y)}{\\partial w} +  \\sum_{i=1}^d 2k (w_{i})\n\\]\nLet‚Äôs assume that torch has already computed \\(\\frac{\\partial loss(y)}{\\partial w}\\). Then, we could do:\nweight.grad += wd * weight\nOf course, we are about that perform opt_step anyways, which subtracts lr * weight.grad from the parameters. Therefore,\np *= 1 - self.lr * self.wd\nis equivalent.",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "stable_sgd.html#momentum",
    "href": "stable_sgd.html#momentum",
    "title": "Optimizers and Schedulers",
    "section": "Momentum",
    "text": "Momentum\nWe already have MomentumCB, but its hacky. This really deserves to be its own class.\nLet‚Äôs get a sense of the parameters of momentum.\n\nxs = torch.linspace(-4, 4, 100)\nys = 1 - (xs / 3) ** 2 + torch.randn(100) * 0.1\n\n\nfig, axs = plt.subplots(2, 2, figsize=(6, 3))\nbetas = [0.5, 0.7, 0.9, 0.99]\nfor beta, ax in zip(betas, axs.flatten()):\n    ax.scatter(xs, ys)\n    avg, res = 0, []\n    for yi in ys:\n        avg = beta * avg + (1 - beta) * yi\n        res.append(avg)\n    ax.plot(xs, np.array(res), color=\"purple\")\n    ax.set_title(f\"beta={beta}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThis demonstrates how momentum can smooth out the loss surface.\n\nclass MomentumSGD(SGD):\n    def __init__(self, params, lr, wd=0, mom=0.9):\n        super().__init__(params, lr, wd)\n        self.mom = mom\n\n    def opt_step(self, p):\n        grad_avg = getattr(p, \"grad_avg\", torch.zeros_like(p.grad))\n\n        # This is slight more sophisticated than the MomentumCB because\n        # it wasn't apropriately weighting the existing gradients\n        p.grad_avg = grad_avg * self.mom + p.grad * (1 - self.mom)\n\n        p -= p.grad_avg * self.lr\n\nLet‚Äôs try this on FashionMNIST\n\ntrain(\n    get_kaiming_initalized_model(),\n    lr=1.25,  # üëà learning rate can be WAY up here\n    opt_func=MomentumSGD,\n)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.803\n0.549\n0\ntrain\n\n\n0.590\n1.788\n0\neval\n\n\n0.857\n0.395\n1\ntrain\n\n\n0.851\n0.411\n1\neval\n\n\n0.880\n0.325\n2\ntrain\n\n\n0.866\n0.383\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTangent\n\n\n\nJeremy goes on a tangent here and discusses batch sizes. In theory, larger batch sizes represent a more accurate reprentation of the loss surface. But they also imply that there are fewer update steps, which is bad! Supposedly, Yan LeCunn (originally) thought that the ideal batch size is one, whereas the norm these days is to have a batch size of millions.",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "stable_sgd.html#rmsprop",
    "href": "stable_sgd.html#rmsprop",
    "title": "Optimizers and Schedulers",
    "section": "RMSProp",
    "text": "RMSProp\nMomentum can be ‚Äúaggressive‚Äù for some finicky architectures. RMSProp is a historically interesting architecture that can be used instead. This was an optimization algorithm debuted in a Coursera by Hinton. It was never published.\nThe algorithm trains models by updating weights with the gradient divided by an exponentially weighted average of the square of the gradients. Large squared gradients imply large variances, so this ensure gives each parameter a chance to shine üåü In other words, this allows the network to take larger steps in directions where gradients are consistently small, and smaller steps in directions where gradients are fluctuating or large.\n\nclass RMSProp(SGD):\n    def __init__(self, params, lr, wd=0.0, sqr_mom=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.sqr_mom, self.eps = sqr_mom, eps\n\n    def opt_step(self, p):\n        # Note that Jeremy initializes the squared average as that of the current\n        # gradient, instead of 0. Otherwise, p.sqr_avg.sqrt() + self.eps is a\n        # very large value and the initial learning rate is very high\n        sqr_avg = getattr(p, \"sqr_avg\", p.grad**2)\n        #                                    vvvvvvvvv\n        p.sqr_avg = sqr_avg * self.sqr_mom + p.grad**2 * (1 - self.sqr_mom)\n        p -= self.lr * p.grad / (p.sqr_avg.sqrt() + self.eps)\n\nHow does this do on FashionMNIST?\n\ntrain(get_kaiming_initalized_model(), lr=1e-2, opt_func=RMSProp)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.780\n0.635\n0\ntrain\n\n\n0.814\n0.536\n0\neval\n\n\n0.855\n0.387\n1\ntrain\n\n\n0.827\n0.513\n1\neval\n\n\n0.869\n0.348\n2\ntrain\n\n\n0.822\n0.521\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nHonestly, not that good in practice ‚Äì at least on its own. That‚Äôs why we usually see:",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "stable_sgd.html#adam",
    "href": "stable_sgd.html#adam",
    "title": "Optimizers and Schedulers",
    "section": "Adam",
    "text": "Adam\nRMSProp and momentum are usually seen together in the Adam optimizer.\nKeep in mind that \\(\\beta\\) terms beta1 and beta2 are just momentum and squared momentum!\n\nclass Adam(SGD):\n    def __init__(self, params, lr, wd=0.0, beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n\n    def opt_step(self, p):\n        if not hasattr(p, \"avg\"):\n            p.avg = torch.zeros_like(p.grad.data)\n        if not hasattr(p, \"sqr_avg\"):\n            p.sqr_avg = torch.zeros_like(p.grad.data)\n        p.avg = self.beta1 * p.avg + (1 - self.beta1) * p.grad\n\n        # For the first minibatch, the momentum is 0, so the gradient\n        # should be quite small. However, we know this is the case and\n        # we can adjust for it because we know how far from 0 it should\n        # be -- i.e., $(beta_1)^2$. Therefore, we can divide by 1 minus\n        # this term to increase the average. Note that as i increases,\n        # the unbiased avg approaches the original avg\n        unbias_avg = p.avg / (1 - (self.beta1 ** (self.i + 1)))\n\n        p.sqr_avg = self.beta2 * p.sqr_avg + (1 - self.beta2) * (p.grad**2)\n\n        # Same idea as above\n        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2 ** (self.i + 1)))\n\n        # Finally, we perform the learning rate modulation for momentum and\n        # RMSProp\n        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()\n\n\ntrain(get_kaiming_initalized_model(), lr=1e-2, opt_func=Adam)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.824\n0.490\n0\ntrain\n\n\n0.852\n0.430\n0\neval\n\n\n0.874\n0.346\n1\ntrain\n\n\n0.867\n0.386\n1\neval\n\n\n0.888\n0.302\n2\ntrain\n\n\n0.868\n0.371\n2\neval",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "stable_sgd.html#schedulers",
    "href": "stable_sgd.html#schedulers",
    "title": "Optimizers and Schedulers",
    "section": "Schedulers",
    "text": "Schedulers\nDecreasing the learning rate over time is thought to help with convergence by allowing the network to explore grooves in the loss surface.\nHere are the built-in optimizers:\n\n[m for m in dir(lr_scheduler) if m != \"EPOCH_DEPRECATION_WARNING\" and m[0].isupper()]\n\n['ChainedScheduler',\n 'ConstantLR',\n 'CosineAnnealingLR',\n 'CosineAnnealingWarmRestarts',\n 'Counter',\n 'CyclicLR',\n 'ExponentialLR',\n 'LRScheduler',\n 'LambdaLR',\n 'LinearLR',\n 'MultiStepLR',\n 'MultiplicativeLR',\n 'OneCycleLR',\n 'Optimizer',\n 'PolynomialLR',\n 'ReduceLROnPlateau',\n 'SequentialLR',\n 'StepLR']\n\n\nWe‚Äôll need to tweak the Learner API to incorporate schedulers.\n\nopt = torch.optim.SGD(model.parameters(), lr=1e-1)\nparam = next(iter(model.parameters()))\nst = opt.state[param]\nst\n\n{}\n\n\nPyTorch stores parameter state in the optimizer, similarly to how we stored state directly in the tensors.\ngrad_avg = getattr(p, \"grad_avg\", torch.zeros_like(p.grad))\np.grad_avg = grad_avg * self.mom + p.grad * (1 - self.mom)\np -= p.grad_avg * self.lr\nWe can also group state in ‚Äúparameter groups‚Äù\n\nopt\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.1\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\n\nopt.param_groups[0].keys()\n\ndict_keys(['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov', 'maximize', 'foreach', 'differentiable'])\n\n\nWe can configure out learning rate scheduler like so\n\nlr_scheduler.CosineAnnealingLR?\n\n\nInit signature:\nlr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max,\n    eta_min=0,\n    last_epoch=-1,\n    verbose=False,\n)\nDocstring:     \nSet the learning rate of each parameter group using a cosine annealing\nschedule, where :math:`\\eta_{max}` is set to the initial lr and\n:math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n.. math::\n    \\begin{aligned}\n        \\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1\n        + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right),\n        & T_{cur} \\neq (2k+1)T_{max}; \\\\\n        \\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\n        \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right),\n        & T_{cur} = (2k+1)T_{max}.\n    \\end{aligned}\nWhen last_epoch=-1, sets initial lr as lr. Notice that because the schedule\nis defined recursively, the learning rate can be simultaneously modified\noutside this scheduler by other operators. If the learning rate is set\nsolely by this scheduler, the learning rate at each step becomes:\n.. math::\n    \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n    \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)\nIt has been proposed in\n`SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\nimplements the cosine annealing part of SGDR, and not the restarts.\nArgs:\n    optimizer (Optimizer): Wrapped optimizer.\n    T_max (int): Maximum number of iterations.\n    eta_min (float): Minimum learning rate. Default: 0.\n    last_epoch (int): The index of last epoch. Default: -1.\n    verbose (bool): If ``True``, prints a message to stdout for\n        each update. Default: ``False``.\n.. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n    https://arxiv.org/abs/1608.03983\nFile:           ~/micromamba/envs/slowai/lib/python3.11/site-packages/torch/optim/lr_scheduler.py\nType:           type\nSubclasses:     \n\n\n\n\nn_batches = 100\nsched = lr_scheduler.CosineAnnealingLR(opt, n_batches)\n\n\ndef plot_scheduler(sched, n_steps):\n    fig, ax = plt.subplots()\n    lrs = []\n    lrs.append(sched.get_last_lr())\n    for _ in range(n_batches):\n        sched.optimizer.step()\n        sched.step()\n        lrs.append(sched.get_last_lr())\n        ax.plot(lrs)\n    ax.set(xlabel=\"Time\", ylabel=\"LR\");\n\n\nplot_scheduler(sched, n_steps=n_batches)\n\n\n\n\n\n\n\n\nLet‚Äôs write this as a callback\n\nsource\n\nBaseSchedulerCB\n\n BaseSchedulerCB (scheduler_f, **kwargs)\n\nBase callback class for schedulers\n\nsource\n\n\nBatchSchedulerCB\n\n BatchSchedulerCB (scheduler_f, **kwargs)\n\nStep the scheduler every batch\n\nsource\n\n\nRecorderCB\n\n RecorderCB (**d)\n\nRecord internal state values at each batch.\n\nn_epochs = 3\ndls = fashion_mnist(512)\nT_max = len(dls[\"train\"]) * n_epochs\nscheduler = BatchSchedulerCB(lr_scheduler.CosineAnnealingLR, T_max=T_max)\nrecorder = RecorderCB(lr=g(\"pg.lr\"))\ntrain(\n    get_kaiming_initalized_model(),\n    1e-2,\n    n_epochs,\n    opt_func=torch.optim.Adam,\n    cbs=[scheduler, recorder],\n)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.822\n0.493\n0\ntrain\n\n\n0.843\n0.430\n0\neval\n\n\n0.878\n0.332\n1\ntrain\n\n\n0.868\n0.359\n1\neval\n\n\n0.896\n0.283\n2\ntrain\n\n\n0.877\n0.335\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nrecorder.plot()\n\n\n\n\n\n\n\n\nAnother trick to improve training dynamics is to start with a warmup, as in the 1cycle policy. This is because high learning rate in the initial phase of poorly initialized models can lead to the dead units issue explored earlier.\n\nlr_scheduler.OneCycleLR?\n\n\nInit signature:\nlr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr,\n    total_steps=None,\n    epochs=None,\n    steps_per_epoch=None,\n    pct_start=0.3,\n    anneal_strategy='cos',\n    cycle_momentum=True,\n    base_momentum=0.85,\n    max_momentum=0.95,\n    div_factor=25.0,\n    final_div_factor=10000.0,\n    three_phase=False,\n    last_epoch=-1,\n    verbose=False,\n)\nDocstring:     \nSets the learning rate of each parameter group according to the\n1cycle learning rate policy. The 1cycle policy anneals the learning\nrate from an initial learning rate to some maximum learning rate and then\nfrom that maximum learning rate to some minimum learning rate much lower\nthan the initial learning rate.\nThis policy was initially described in the paper `Super-Convergence:\nVery Fast Training of Neural Networks Using Large Learning Rates`_.\nThe 1cycle learning rate policy changes the learning rate after every batch.\n`step` should be called after a batch has been used for training.\nThis scheduler is not chainable.\nNote also that the total number of steps in the cycle can be determined in one\nof two ways (listed in order of precedence):\n#. A value for total_steps is explicitly provided.\n#. A number of epochs (epochs) and a number of steps per epoch\n   (steps_per_epoch) are provided.\n   In this case, the number of total steps is inferred by\n   total_steps = epochs * steps_per_epoch\nYou must either provide a value for total_steps or provide a value for both\nepochs and steps_per_epoch.\nThe default behaviour of this scheduler follows the fastai implementation of 1cycle, which\nclaims that \"unpublished work has shown even better results by using only two phases\". To\nmimic the behaviour of the original paper instead, set ``three_phase=True``.\nArgs:\n    optimizer (Optimizer): Wrapped optimizer.\n    max_lr (float or list): Upper learning rate boundaries in the cycle\n        for each parameter group.\n    total_steps (int): The total number of steps in the cycle. Note that\n        if a value is not provided here, then it must be inferred by providing\n        a value for epochs and steps_per_epoch.\n        Default: None\n    epochs (int): The number of epochs to train for. This is used along\n        with steps_per_epoch in order to infer the total number of steps in the cycle\n        if a value for total_steps is not provided.\n        Default: None\n    steps_per_epoch (int): The number of steps per epoch to train for. This is\n        used along with epochs in order to infer the total number of steps in the\n        cycle if a value for total_steps is not provided.\n        Default: None\n    pct_start (float): The percentage of the cycle (in number of steps) spent\n        increasing the learning rate.\n        Default: 0.3\n    anneal_strategy (str): {'cos', 'linear'}\n        Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for\n        linear annealing.\n        Default: 'cos'\n    cycle_momentum (bool): If ``True``, momentum is cycled inversely\n        to learning rate between 'base_momentum' and 'max_momentum'.\n        Default: True\n    base_momentum (float or list): Lower momentum boundaries in the cycle\n        for each parameter group. Note that momentum is cycled inversely\n        to learning rate; at the peak of a cycle, momentum is\n        'base_momentum' and learning rate is 'max_lr'.\n        Default: 0.85\n    max_momentum (float or list): Upper momentum boundaries in the cycle\n        for each parameter group. Functionally,\n        it defines the cycle amplitude (max_momentum - base_momentum).\n        Note that momentum is cycled inversely\n        to learning rate; at the start of a cycle, momentum is 'max_momentum'\n        and learning rate is 'base_lr'\n        Default: 0.95\n    div_factor (float): Determines the initial learning rate via\n        initial_lr = max_lr/div_factor\n        Default: 25\n    final_div_factor (float): Determines the minimum learning rate via\n        min_lr = initial_lr/final_div_factor\n        Default: 1e4\n    three_phase (bool): If ``True``, use a third phase of the schedule to annihilate the\n        learning rate according to 'final_div_factor' instead of modifying the second\n        phase (the first two phases will be symmetrical about the step indicated by\n        'pct_start').\n    last_epoch (int): The index of the last batch. This parameter is used when\n        resuming a training job. Since `step()` should be invoked after each\n        batch instead of after each epoch, this number represents the total\n        number of *batches* computed, not the total number of epochs computed.\n        When last_epoch=-1, the schedule is started from the beginning.\n        Default: -1\n    verbose (bool): If ``True``, prints a message to stdout for\n        each update. Default: ``False``.\nExample:\n    &gt;&gt;&gt; # xdoctest: +SKIP\n    &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)\n    &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    &gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n    &gt;&gt;&gt; for epoch in range(10):\n    &gt;&gt;&gt;     for batch in data_loader:\n    &gt;&gt;&gt;         train_batch(...)\n    &gt;&gt;&gt;         optimizer.step()\n    &gt;&gt;&gt;         scheduler.step()\n.. _Super-Convergence\\: Very Fast Training of Neural Networks Using Large Learning Rates:\n    https://arxiv.org/abs/1708.07120\nFile:           ~/micromamba/envs/slowai/lib/python3.11/site-packages/torch/optim/lr_scheduler.py\nType:           type\nSubclasses:     \n\n\n\n\nplot_scheduler(lr_scheduler.OneCycleLR(opt, max_lr=0.1, total_steps=100), 100)\n\n\n\n\n\n\n\n\nUltimately, tricks like batch norm and OneCycle are unneccesary if the model is initialized apropriately. These papers are excellent guides to doing so:\n\nFor ResNets: Fixup Initialization: Residual Learning Without Normalization\n\n\nNormalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization ‚Äì even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.\n\n\nFor Transformers: Improving Transformer Optimization Through Better Initialization\n\n\nThe Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, re- cent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justi- fication, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 atten- tion/MLP blocks) without difficulty.\n\n\n\n\n\n\n\nWarning\n\n\n\nOver time, the community learn to initialize models correctly. It‚Äôs difficult! Most researchers don‚Äôt realize they don‚Äôt need batch norm or warmup if the initialization is correct. Don‚Äôt be like most researchers!\n\n\n\nsource\n\n\ntrain_1cycle\n\n train_1cycle (model, lr=0.01, n_epochs=3, extra_cbs=[])\n\n\nrecorder, _ = train_1cycle(get_kaiming_initalized_model())\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.727\n0.777\n0\ntrain\n\n\n0.837\n0.482\n0\neval\n\n\n0.870\n0.357\n1\ntrain\n\n\n0.868\n0.361\n1\neval\n\n\n0.895\n0.285\n2\ntrain\n\n\n0.884\n0.320\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the learning rate schedule is negatively correlated to the momentum. The idea is that as the network approaches a stable training dynamic, the momentum is less neccesary.\n\nrecorder.plot()",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "stable_sgd.html#achieving-90-accuracy",
    "href": "stable_sgd.html#achieving-90-accuracy",
    "title": "Optimizers and Schedulers",
    "section": "Achieving 90% accuracy",
    "text": "Achieving 90% accuracy\nLet‚Äôs simply increase the maximum learning rate and the training duration with all the improvements we‚Äôve explored in this notebook\n\ntrain_1cycle(get_kaiming_initalized_model(), lr=5e-2, n_epochs=6);\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.792\n0.578\n0\ntrain\n\n\n0.798\n0.559\n0\neval\n\n\n0.843\n0.428\n1\ntrain\n\n\n0.831\n0.479\n1\neval\n\n\n0.874\n0.339\n2\ntrain\n\n\n0.866\n0.377\n2\neval\n\n\n0.893\n0.288\n3\ntrain\n\n\n0.883\n0.328\n3\neval\n\n\n0.909\n0.247\n4\ntrain\n\n\n0.894\n0.294\n4\neval\n\n\n0.922\n0.211\n5\ntrain\n\n\n0.899\n0.276\n5\neval\n\n\n\n\n\n\n\n\n\n\n\n\n90% accuracy üéâüéâüéâ",
    "crumbs": [
      "Optimizers and Schedulers"
    ]
  },
  {
    "objectID": "activations.html",
    "href": "activations.html",
    "title": "Activation Statistics",
    "section": "",
    "text": "Adapted from:\nWe need to have a way of looking inside models and diagnosing issues.\nsource",
    "crumbs": [
      "Activation Statistics"
    ]
  },
  {
    "objectID": "activations.html#baseline",
    "href": "activations.html#baseline",
    "title": "Activation Statistics",
    "section": "Baseline",
    "text": "Baseline\nLet‚Äôs look at a fashion MNIST classification problem.\n\nsource\n\nConv2dWithReLU\n\n Conv2dWithReLU (*args, nonlinearity=&lt;function relu&gt;,\n                 stride:Union[int,Tuple[int,int]]=1,\n                 padding:Union[str,int,Tuple[int,int]]=0,\n                 dilation:Union[int,Tuple[int,int]]=1, groups:int=1,\n                 bias:bool=True, padding_mode:str='zeros', device=None,\n                 dtype=None)\n\nConvolutional neural network with a built in activation\n\nsource\n\n\nCNN\n\n CNN ()\n\nSix layer convolutional neural network\nGenerally, we want a high learning rate to come up with generalizable algorithms. Let‚Äôs start with the relatively high 0.6.\n\ndef train(model, extra_cbs=None):\n    cbs = [\n        MetricsCB(MulticlassAccuracy(num_classes=10)),\n        DeviceCB(),\n        ProgressCB(plot=True),\n    ]\n    if extra_cbs:\n        cbs.extend(extra_cbs)\n    learn = TrainLearner(\n        model,\n        fashion_mnist(),\n        F.cross_entropy,\n        lr=0.6,\n        cbs=cbs,\n    ).fit()\n    return learn\n\n\ntrain(model=CNN())\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.159\n2.910\n0\ntrain\n\n\n0.100\n2.386\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs look at the underlying activations",
    "crumbs": [
      "Activation Statistics"
    ]
  },
  {
    "objectID": "activations.html#hooks",
    "href": "activations.html#hooks",
    "title": "Activation Statistics",
    "section": "Hooks",
    "text": "Hooks\nJeremy‚Äôs implementation is kind of a mess so I did a bit of refactoring. Hooks are just another kind of callback in the PyTorch universe, so we can adopt our Callback conventions.\n\nsource\n\nHook\n\n Hook (m, f)\n\nWrapper for a PyTorch hook, facilitating adding instance state\n\nsource\n\n\nHooksCallback\n\n HooksCallback (hook_cls, mods=None, mod_filter=&lt;function noop&gt;,\n                on_train=True, on_valid=False)\n\nContainer for hooks with clean up and and options to target certain modules\nThat being implemented, we can subclass these for adding hook behaviors.\n\nsource\n\n\nStoreModuleStats\n\n StoreModuleStats (m, on_train=True, on_valid=False, periodicity=1)\n\nA hook for storing the activation statistics\n\nsource\n\n\nStoreModuleStatsCB\n\n StoreModuleStatsCB (mods=None, mod_filter=&lt;function noop&gt;, on_train=True,\n                     on_valid=False, hook_kwargs=None)\n\nCallback for plotting the layer-wise activation statistics\nNow, we can rerun while keeping track of the activation stats\n\nmodel = CNN()\ncb = StoreModuleStatsCB(mods=model.layers)\ntrain(model=model, extra_cbs=[cb])\ncb.mean_std_plot()\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.169\n2.274\n0\ntrain\n\n\n0.199\n2.096\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncb.hist_plot()\n\n\n\n\n\n\n\n\nJeremy makes the point that his network isn‚Äôt training because the weights are close to 0, which makes them ‚Äúdead units.‚Äù\n‚ö†Ô∏è Generally, the mean should be 0 and the standard deviation should be close to 1.\nUltimately, Jeremy recommends simply abandoning any training run where the activation variance increases and crashes.",
    "crumbs": [
      "Activation Statistics"
    ]
  },
  {
    "objectID": "diving_deeper_homework.html",
    "href": "diving_deeper_homework.html",
    "title": "Diving Deeper, homework",
    "section": "",
    "text": "Negative prompts are an extension of the Classifier Free Guidance Module. Recall this is part of the pred_noise method of StableDiffusion\n\nStableDiffusion.pred_noise?\n\nSignature: StableDiffusion.pred_noise(self, prompt_embedding, l, t, guidance_scale)\nDocstring: &lt;no docstring&gt;\nFile:      ~/Desktop/SlowAI/nbs/slowai/overview.py\nType:      function\n\n\nLet‚Äôs define a helper method to load StableDiffusion, as in the ‚ÄúOverview‚Äù notebook\n\nsource\n\nget_stable_diffusion\n\n get_stable_diffusion (cls=&lt;class 'slowai.overview.StableDiffusion'&gt;)\n\n\nsource\n\n\nget_simple_pipe\n\n get_simple_pipe ()\n\n\nsd = get_stable_diffusion()\n\n\nsd(\n    prompt=\"a photo of a giraffe in Paris\",\n    guidance_scale=7.5,\n    as_pil=True,\n)\n\n  0%|          | 0/30 [00:00&lt;?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:04&lt;00:00,  7.38it/s]\n\n\n\n\n\n\n\n\n\nprompt_embedding is a tensor four-rank tensor of batch_size x seq_len x channels, where the batch size is 2 because its the concatenated unconditional prompt and the conditional prompt.\n\nsd.embed_prompt(\"a photo of a giraffe in paris\").shape\n\ntorch.Size([2, 77, 768])\n\n\nWe want to add the negative prompt and run this through the denoising unet at the same time. This should make the batch size into 3.\n\nsource\n\n\nStableDiffusionWithNegativePromptA\n\n StableDiffusionWithNegativePromptA\n                                     (tokenizer:transformers.models.clip.t\n                                     okenization_clip.CLIPTokenizer, text_\n                                     encoder:transformers.models.clip.mode\n                                     ling_clip.CLIPTextModel,\n                                     scheduler:Any, unet:diffusers.models.\n                                     unets.unet_2d_condition.UNet2DConditi\n                                     onModel, vae:diffusers.models.autoenc\n                                     oders.autoencoder_kl.AutoencoderKL)\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptA)\nembedding = sd.embed_prompt(\"a photo of a giraffe in paris\", \"blurry\")\nembedding.shape\n\ntorch.Size([3, 77, 768])\n\n\nNow, we need to pretty much totally rewrite the denoising method to incorporate this negative guidance.\n\nsource\n\n\nStableDiffusionWithNegativePromptB\n\n StableDiffusionWithNegativePromptB\n                                     (tokenizer:transformers.models.clip.t\n                                     okenization_clip.CLIPTokenizer, text_\n                                     encoder:transformers.models.clip.mode\n                                     ling_clip.CLIPTextModel,\n                                     scheduler:Any, unet:diffusers.models.\n                                     unets.unet_2d_condition.UNet2DConditi\n                                     onModel, vae:diffusers.models.autoenc\n                                     oders.autoencoder_kl.AutoencoderKL)\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptB)\nembedding = sd.embed_prompt(\"a photo of a giraffe in paris\", \"blurry\")\nl = sd.init_latents()\nepsilon = sd.pred_noise(embedding, l, t=0, guidance_scale_pos=7.5, guidance_scale_neg=2)\nepsilon.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nFinally, we incorporate the negative prompt into the class API.\n\nsource\n\n\nStableDiffusionWithNegativePromptC\n\n StableDiffusionWithNegativePromptC\n                                     (tokenizer:transformers.models.clip.t\n                                     okenization_clip.CLIPTokenizer, text_\n                                     encoder:transformers.models.clip.mode\n                                     ling_clip.CLIPTextModel,\n                                     scheduler:Any, unet:diffusers.models.\n                                     unets.unet_2d_condition.UNet2DConditi\n                                     onModel, vae:diffusers.models.autoenc\n                                     oders.autoencoder_kl.AutoencoderKL)\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptC)\nsd(\n    prompt=\"a photo of a labrador dog\",\n    negative_prompt=\"park, greenery, plants, flowers\",\n    guidance_scale=7.5,\n    neg_guidance_scale=5,\n    as_pil=True,\n)\n\n  0%|          | 0/30 [00:00&lt;?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06&lt;00:00,  4.98it/s]\n\n\n\n\n\n\n\n\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptC)\nsd(\n    prompt=\"a photo of a labrador dog in a park\",\n    negative_prompt=\"greenery, plants, flowers\",\n    guidance_scale=7.5,\n    neg_guidance_scale=5,\n    as_pil=True,\n)\n\n  0%|          | 0/30 [00:00&lt;?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06&lt;00:00,  4.97it/s]",
    "crumbs": [
      "Diving Deeper, homework"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utilities",
    "section": "",
    "text": "source\n\nclean_ipython_hist\n\n clean_ipython_hist ()\n\n\nsource\n\n\nclean_tb\n\n clean_tb ()\n\n\nsource\n\n\nclean_mem\n\n clean_mem ()\n\n\nsource\n\n\nsubplots\n\n subplots (nrows:int=1, ncols:int=1, figsize:tuple=None, imsize:int=3,\n           suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None, **kwargs)\n\nA figure and set of subplots to display images of imsize inches\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nshow_images\n\n show_images (ims:list, nrows:Optional[int]=None,\n              ncols:Optional[int]=None, titles:Optional[list]=None,\n              ax_kwargs=None, figsize:tuple=None, imsize:int=3,\n              suptitle:str=None,\n              sharex:\"bool|Literal['none','all','row','col']\"=False,\n              sharey:\"bool|Literal['none','all','row','col']\"=False,\n              squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n              height_ratios:Sequence[float]|None=None,\n              subplot_kw:dict[str,Any]|None=None,\n              gridspec_kw:dict[str,Any]|None=None)\n\nShow all images ims as subplots with rows using titles\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nims\nlist\n\nImages to show\n\n\nnrows\nOptional\nNone\nNumber of rows in grid\n\n\nncols\nOptional\nNone\nNumber of columns in grid (auto-calculated if None)\n\n\ntitles\nOptional\nNone\nOptional list of titles for each image\n\n\nax_kwargs\nNoneType\nNone\n\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\n\n/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Other Parameters\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section See Also\n  else: warn(msg)\n\nsource\n\n\nshow_image\n\n show_image (im, ax=None, figsize=None, title=None, noframe=True,\n             cmap=None, norm=None, aspect=None, interpolation=None,\n             alpha=None, vmin=None, vmax=None, origin=None, extent=None,\n             interpolation_stage=None, filternorm=True, filterrad=4.0,\n             resample=None, url=None, data=None)\n\nShow a PIL or PyTorch image on ax.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nim\n\n\n\n\n\nax\nNoneType\nNone\n\n\n\nfigsize\nNoneType\nNone\n\n\n\ntitle\nNoneType\nNone\n\n\n\nnoframe\nbool\nTrue\n\n\n\ncmap\nNoneType\nNone\nThe Colormap instance or registered colormap name used to map scalar datato colors.This parameter is ignored if X is RGB(A).\n\n\nnorm\nNoneType\nNone\nThe normalization method used to scale scalar data to the [0, 1] rangebefore mapping to colors using cmap. By default, a linear scaling isused, mapping the lowest value to 0 and the highest to 1.If given, this can be one of the following:- An instance of .Normalize or one of its subclasses (see :ref:colormapnorms).- A scale name, i.e.¬†one of ‚Äúlinear‚Äù, ‚Äúlog‚Äù, ‚Äúsymlog‚Äù, ‚Äúlogit‚Äù, etc. For a list of available scales, call matplotlib.scale.get_scale_names(). In that case, a suitable .Normalize subclass is dynamically generated and instantiated.This parameter is ignored if X is RGB(A).\n\n\naspect\nNoneType\nNone\nThe aspect ratio of the Axes. This parameter is particularlyrelevant for images since it determines whether data pixels aresquare.This parameter is a shortcut for explicitly calling.Axes.set_aspect. See there for further details.- ‚Äòequal‚Äô: Ensures an aspect ratio of 1. Pixels will be square (unless pixel sizes are explicitly made non-square in data coordinates using extent).- ‚Äòauto‚Äô: The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels.Normally, None (the default) means to use :rc:image.aspect. However, ifthe image uses a transform that does not contain the axes data transform,then None means to not modify the axes aspect at all (in that case, directlycall .Axes.set_aspect if desired).\n\n\ninterpolation\nNoneType\nNone\nThe interpolation method used.Supported values are ‚Äònone‚Äô, ‚Äòantialiased‚Äô, ‚Äònearest‚Äô, ‚Äòbilinear‚Äô,‚Äòbicubic‚Äô, ‚Äòspline16‚Äô, ‚Äòspline36‚Äô, ‚Äòhanning‚Äô, ‚Äòhamming‚Äô, ‚Äòhermite‚Äô,‚Äòkaiser‚Äô, ‚Äòquadric‚Äô, ‚Äòcatrom‚Äô, ‚Äògaussian‚Äô, ‚Äòbessel‚Äô, ‚Äòmitchell‚Äô,‚Äòsinc‚Äô, ‚Äòlanczos‚Äô, ‚Äòblackman‚Äô.The data X is resampled to the pixel size of the image on thefigure canvas, using the interpolation method to either up- ordownsample the data.If interpolation is ‚Äònone‚Äô, then for the ps, pdf, and svgbackends no down- or upsampling occurs, and the image data ispassed to the backend as a native image. Note that different ps,pdf, and svg viewers may display these raw pixels differently. Onother backends, ‚Äònone‚Äô is the same as ‚Äònearest‚Äô.If interpolation is the default ‚Äòantialiased‚Äô, then ‚Äònearest‚Äôinterpolation is used if the image is upsampled by more than afactor of three (i.e.¬†the number of display pixels is at leastthree times the size of the data array). If the upsampling rate issmaller than 3, or the image is downsampled, then ‚Äòhanning‚Äôinterpolation is used to act as an anti-aliasing filter, unless theimage happens to be upsampled by exactly a factor of two or one.See:doc:/gallery/images_contours_and_fields/interpolation_methodsfor an overview of the supported interpolation methods, and:doc:/gallery/images_contours_and_fields/image_antialiasing fora discussion of image antialiasing.Some interpolation methods require an additional radius parameter,which can be set by filterrad. Additionally, the antigrain imageresize filter is controlled by the parameter filternorm.\n\n\nalpha\nNoneType\nNone\nThe alpha blending value, between 0 (transparent) and 1 (opaque).If alpha is an array, the alpha blending values are applied pixelby pixel, and alpha must have the same shape as X.\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\nPlace the [0, 0] index of the array in the upper left or lowerleft corner of the Axes. The convention (the default) ‚Äòupper‚Äô istypically used for matrices and images.Note that the vertical axis points upward for ‚Äòlower‚Äôbut downward for ‚Äòupper‚Äô.See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\nextent\nNoneType\nNone\nThe bounding box in data coordinates that the image will fill.These values may be unitful and match the units of the Axes.The image is stretched individually along x and y to fill the box.The default extent is determined by the following conditions.Pixels have unit size in data coordinates. Their centers are oninteger coordinates, and their center coordinates range from 0 tocolumns-1 horizontally and from 0 to rows-1 vertically.Note that the direction of the vertical axis and thus the defaultvalues for top and bottom depend on origin:- For origin == 'upper' the default is (-0.5, numcols-0.5, numrows-0.5, -0.5).- For origin == 'lower' the default is (-0.5, numcols-0.5, -0.5, numrows-0.5).See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\ninterpolation_stage\nNoneType\nNone\nIf ‚Äòdata‚Äô, interpolationis carried out on the data provided by the user. If ‚Äòrgba‚Äô, theinterpolation is carried out after the colormapping has beenapplied (visual interpolation).\n\n\nfilternorm\nbool\nTrue\nA parameter for the antigrain image resize filter (see theantigrain documentation). If filternorm is set, the filternormalizes integer values and corrects the rounding errors. Itdoesn‚Äôt do anything with the source floating point values, itcorrects only integers according to the rule of 1.0 which meansthat any sum of pixel weights must be equal to 1.0. So, thefilter function must produce a graph of the proper shape.\n\n\nfilterrad\nfloat\n4.0\nThe filter radius for filters that have a radius parameter, i.e.when interpolation is one of: ‚Äòsinc‚Äô, ‚Äòlanczos‚Äô or ‚Äòblackman‚Äô.\n\n\nresample\nNoneType\nNone\nWhen True, use a full resampling method. When False, onlyresample when the output image is larger than the input image.\n\n\nurl\nNoneType\nNone\nSet the url of the created .AxesImage. See .Artist.set_url.\n\n\ndata\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nget_grid\n\n get_grid (n:int, nrows:int=None, ncols:int=None, title:str=None,\n           weight:str='bold', size:int=14, figsize:tuple=None,\n           imsize:int=3, suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None)\n\nReturn a grid of n axes, rows by cols\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n\nNumber of axes\n\n\nnrows\nint\nNone\nNumber of rows, defaulting to int(math.sqrt(n))\n\n\nncols\nint\nNone\nNumber of columns, defaulting to ceil(n/rows)\n\n\ntitle\nstr\nNone\nIf passed, title set to the figure\n\n\nweight\nstr\nbold\nTitle font weight\n\n\nsize\nint\n14\nTitle font size\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\n\n\nsource\n\n\nsubplots\n\n subplots (nrows:int=1, ncols:int=1, figsize:tuple=None, imsize:int=3,\n           suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None, **kwargs)\n\nA figure and set of subplots to display images of imsize inches\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nglomf\n\n glomf (spec)\n\n\nsource\n\n\nSuppressor\n\n Suppressor ()\n\nStolen from: https://stackoverflow.com/questions/2828953/silence-the-stdout-of-a-function-in-python-without-trashing-sys-stdout-and-resto\n\nsource\n\n\ndownload_image\n\n download_image (image_url, resize=None)\n\n\nurl = \"https://static01.nyt.com/images/2024/01/18/arts/16lilnasx-notebook/16lilnasx-notebook-threeByTwoSmallAt2X.jpg?format=pjpg&quality=75&auto=webp&disable=upscale\"\nimg = download_image(url, (128, 128))\nshow_image(img.permute(1, 2, 0));",
    "crumbs": [
      "Utilities"
    ]
  },
  {
    "objectID": "coco_b.html",
    "href": "coco_b.html",
    "title": "COCO: Super-resolution",
    "section": "",
    "text": "try:\n    with open(\"data/coco_color_fps.json\") as f:\n        color_fps = json.load(f)[\"fps_color\"]\nexcept FileNotFoundError:\n    fps = [str(fi) for fi in Path(\"data/train2017\").glob(\"**/*.jpg\")]\n    color_fps = []\n    with multiprocessing.Pool() as p:\n        iter_ = zip(p.imap(black_and_white, fps), fps)\n        for b_w, fp in tqdm(iter_, total=len(fps)):\n            if not b_w:\n                color_fps.append(fp)\n    with open(\"data/coco_color_fps.json\", \"wt\") as f:\n        color_fps = json.dump({\"fps_color\": fps_color}, f)\n\n\nsource\n\ncoco_2017_trn\n\n coco_2017_trn (fps=None, n=None, remove_bw=True)\n\n\nds = coco_2017_trn(fps_color)\n\n\nsource\n\n\ncrop_to_box\n\n crop_to_box (img:&lt;module'PIL.Image'from'/opt/hostedtoolcache/Python/3.10.\n              14/x64/lib/python3.10/site-packages/PIL/Image.py'&gt;)\n\n\nsource\n\n\npreprocess_ddpm\n\n preprocess_ddpm (examples, pipe, extra_blur=False)\n\n\nrows = ds[\"train\"][:6]\nfig, axes = plt.subplots(4, 5, figsize=(10, 8))\nfor ax in axes.flatten():\n    ax.set_xticks([])\n    ax.set_yticks([])\ntrn = preprocess_ddpm(rows, pipe=trn_preprocess_super_rez)\ntst = preprocess_ddpm(rows, pipe=tst_preprocess_super_rez)\nfor im_trn_hi, im_trn_lo, im_test, im_org, ax_col in zip(\n    trn[\"image_high_rez\"],\n    trn[\"image_low_rez\"],\n    tst[\"image_high_rez\"],\n    rows[\"image\"],\n    axes.T,\n):\n    for ax, im in zip(ax_col, (im_trn_lo, im_trn_hi, im_test)):\n        ax.imshow(denorm(im).permute(1, 2, 0))\n    ax_col[3].imshow(im_org)\naxes[0, 0].set(title=\"Train (Low Rez)\")\naxes[1, 0].set(title=\"Train (Hi Rez)\")\naxes[2, 0].set(title=\"Test\")\naxes[3, 0].set(title=\"Original\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_coco_dataset\n\n get_coco_dataset (fac, trn, tst, fp='data/train2017', bs=512, n=None,\n                   columns=['image_low_rez', 'image_high_rez'])\n\n\ndls = get_coco_dataset_super_rez(n=100)\n\n\n\n\n\n\n\nCPU times: user 320 ms, sys: 35.5 ms, total: 355 ms\nWall time: 173 ms\n\n\nWe also want to do colorization\n\nsource\n\n\npreprocess_colorization\n\n preprocess_colorization (examples, pipe)\n\n\ndls = get_coco_dataset_colorization(n=100)\n\n\n\n\n\n\n\nCPU times: user 319 ms, sys: 25 ms, total: 344 ms\nWall time: 156 ms\n\n\n\nxb, yb = dls.peek()\n\n\ndenorm(xb).max()\n\ntensor(0.9961)\n\n\n\nshow_images(denorm(xb[:6, ...]), imsize=(1.6))\n\n\n\n\n\n\n\n\n\nshow_images(denorm(yb[:6, ...]), imsize=(1.6))",
    "crumbs": [
      "COCO: Super-resolution"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Course overview",
    "section": "",
    "text": "Adapted from:",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "overview.html#motivation",
    "href": "overview.html#motivation",
    "title": "Course overview",
    "section": "Motivation",
    "text": "Motivation\nSay we want to generate images. Ideally, we would have a probability distribution of the pixels, \\(P\\). Let‚Äôs say that the probability distribution is for each \\(28^2\\) pixels. Note tha teach pixel is variables of the probability density function, or PDF for short).\nWe don‚Äôt have this, but say we had the derivative of this PDF. Recall that mutlivariate calculus is concerned with partial derivatives. For example, the partial derivatives of \\(f(x,y)=x^2 + y^2\\) are:\n\\[\n\\begin{align}\n\\frac{\\partial f(x,y)}{\\partial x} &= 2x \\\\\n\\frac{\\partial f(x,y)}{\\partial y} &= 2y \\\\\n\\end{align}\n\\]\nIn the case of an image, imagine the partial derivative of the function \\(P\\) with respect to a single pixel:\n\\[\n\\frac{\\partial P(X)}{\\partial X_{i,j}} = \\frac{P(X)-P(X + \\partial X_{i,j})}{\\partial X_{i,j}}\n\\]\nWe can use this idea to generate images.\nWe could start from a blurry image and ‚Äúmarch upwards‚Äù in the direction of the partial derivative.\n\nsource\n\nshow_image\n\n show_image (img, title=None, K=2)\n\n\nsource\n\n\nshow_images\n\n show_images (imgs, titles=None, K=2)\n\n\nmnist = load_dataset(\"mnist\")\nimg = np.array(mnist[\"train\"][0][\"image\"])\nimg = normalize(img)\nimg_blurry = img + np.random.normal(size=img.size).reshape(28, 28) * 0.2\nimg_very_blurry = img + np.random.normal(size=img.size).reshape(28, 28) * 0.8\nimgs = [img, img_blurry, img_very_blurry]\nshow_images(imgs)\n\nFound cached dataset mnist (/Users/jeremiahfisher/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 104.60it/s]\n\n\n\n\n\n\n\n\n\nAn algorithm could look something like this:\n\nFor all pixel values \\(X_{i,j} \\in X\\), evaluate the partial derivative \\(\\frac{P(X)-P(X + \\partial X_{i,j})}{\\partial X_{i,j}}\\) or \\(\\frac{\\partial P(X)}{\\partial X_{i,j}}\\). This can also be expressed as \\(\\nabla_X P\\)\nFor some hyperparameter constant \\(C\\) and for all pixel values \\(X_{i,j} \\in X\\), \\(X_{i,j} := X_{i,j} + C \\frac{\\partial P(X)}{\\partial X_{i,j}}\\) or, equivalently, \\(X := X + C \\cdot \\nabla_X P\\)\nRepeat until satisfied\n\nIn PyTorch, this would look something like:\nX = get_image()\n# Somewhat unusually, you would incorporate the image as a model\n# parameter in order to get auto-differentiation\nmodel = get_nabla_X_of_P_model(X=X)\nfor _ in range(n_timesteps):\n    p_grad = model.forward(X)\n    p_grad.backward()\n    model.X += C * model.X.grad\nIn fact, we don‚Äôt have \\(P(X)\\) or \\(\\nabla_X P(X)\\) in real life. But we can solve a related problem.\nNotice that \\(\\nabla_X P(X)\\) provides a direction from blurrier to sharper images. We can train a neural network to de-blur by adding the blur ourselves. The input-output pair would be \\(\\langle image + \\epsilon, \\epsilon \\rangle\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\) and \\(\\Sigma \\in \\mathbb{R}^{28^2 \\times 28^2}\\)",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "overview.html#an-aside-the-mathematics",
    "href": "overview.html#an-aside-the-mathematics",
    "title": "Course overview",
    "section": "An aside, the mathematics",
    "text": "An aside, the mathematics\nIn mathematical parlance, we seek to fit a set of reverse Markov transitions to maximize the likelihood of the model on the training data. Or ‚Äúminimizing the variational upper bound on the negative log likelihood.‚Äù Not sure what that mean exactly, but I‚Äôll be targetting the mathematical level of myself 6 months ago.\nFirst, we define some terms:\n\n\\(X^{(0)}\\) is the input data distribution; for example, MNIST digits\n\\(Q\\) and \\(P\\) are probability density functions\n\\(\\beta_t\\) is the ‚Äúnoise value‚Äù at time t\n\nWe can look to physics for inspiration in AI. This paper draws from the thermodynamics to imagine the opposite of a diffusion process: that is, evolving for a high-entropy distribution (like a noisy image) to a clear one.\nIn the diffusion process, we have a ‚Äúforward‚Äù Gaussian Markov process called \\(Q\\) that governs the transition to a noisier distribution. In nature, this is a Guassian distribution: \\[\nQ(X^{(t)} | X^{(t-1)}) = \\mathcal{N}(X^{(t-1)}\\sqrt{1-\\beta_t}, I\\beta_t)\n\\] Note that at \\(t=0\\), we haven‚Äôt added any noise and \\(B_0=0\\). In fact, there is a simple expected value:\n\\[\n\\begin{align}\nE\\left[\\mathcal{N}(X^{(t-1)}\\sqrt{1-\\beta_t}, I\\beta_t)\\right]\n&= E\\left[\\mathcal{N}(X^{(t-1)}\\sqrt{1-(0)}, I(0)\\right]\\\\\n&= E\\left[\\mathcal{N}(X^{(t-1)}, 0)\\right] \\\\\n&= X^{(t-1)}\n\\end{align}\n\\]\nFurthermore, note that adding noise is a simple process and we generally work with the ‚Äúanalytic‚Äù conditional distribution where we all the noise all at once. We‚Äôll go over this function later.\nThe important process to consider, however, is the ‚Äúbackward‚Äù Guassian Markov process called \\(P\\)\n\\[\nQ(X^{(t)} | X^{(t-1)}) = \\mathcal{N}(\\square(X^{(t-1)}), \\triangle(X^{(t-1)}))\n\\]\nWe want a function that maximizes the log-likelihood of this probability distribution on the data. Mathematically, this would involve the integral over the parameters and likelihood. This is mathematically intractable.\n(An aside, we use log-likelihood instead of likelihood because it increases monotonically and sums are more numerically stable on computers than products.)\nInstead of solving the integral directly, we use the Evidence Lower Bound. This is a score function of the model that balances maximizing the likelihood of the data under the model with the complexity of the model. \\[\nELBO = E\\left[log(P_\\theta(X))\\right] - KL(q(\\theta) || p(\\theta))\n\\] The expectation maximization term calculates the probability of the data using the PDF \\(P\\). The likelier the data under the model, the lower the loss. Simple.\nIn general, KL divergence measures the difference between distributions. In our case, it is a loss that should be minimized between the expected (or ‚Äúvariational‚Äù) weight distribution (\\(p\\)) and the actual (‚Äútrue posterior‚Äù) distribution (\\(q\\)) of the model weights themselves. Basically, we want to see the same distributions in the reverse process that we would see in the forward process.\nThe \\(||\\) is a notation for the computation: \\[\nKL(q || p) = \\int q(\\theta)log\\left( \\frac{q(\\theta)}{{p(\\theta)}} \\right) d\\theta\n\\] Let‚Äôs break this down.\n\nThe log ratio of the point values of \\(p\\) and \\(q\\) at \\(\\theta\\) is a measure of the difference at that point\n\nIf the probability distributions give the same likelihood for \\(\\theta\\), the resulting value is 0\nAs \\(q(\\theta)\\) goes 0 and \\(p(\\theta)\\) goes to 1, the limit of the resulting value diverges to negative infinity\nAs \\(q(\\theta)\\) goes 1 and \\(p(\\theta)\\) goes to 0, the limit diverges to positive infinity\n\n\\(q(\\theta)\\) is a weight for the integrand\n\nSo, in sum, the KL diverenge is a sum of the difference in probability weighted by the probability values of the variational distribution. If the distributions are similar, the KL divergence is small; otherwise, they are large.\nSee: https://www.assemblyai.com/blog/content/media/2022/05/KL_Divergence.mp4\nInterestingly, Ho et al fixes the variance schedule. In turns out, this makes it so that the covariance matrix is constant, the KL divergence term goes to 0 and we only need to estimate the mean.\nWe can train a model to do so with MSE loss.\nThis course will demonstrate how to do so from scratch.\nLet‚Äôs begin by running the algorithm.\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n# Use a simple noising scheduler for the initial draft\npipe.scheduler = LMSDiscreteScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    num_train_timesteps=1000,\n)\npipe = pipe.to(TORCH_DEVICE)\npipe.enable_attention_slicing()\nprompt = \"a photo of a giraffe in Paris\"\npipe(prompt).images[0]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:10&lt;00:00,  1.41s/it]\n\n\n\n\n\n\n\n\n\nNow that we see what Stable Diffusion is capable of, we note its three components:\n\nVariational Autoencoder\nCLIP\nUnet",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "overview.html#model-components",
    "href": "overview.html#model-components",
    "title": "Course overview",
    "section": "Model components",
    "text": "Model components\n\nVariational Autoencoder\nStable Diffusion is a latent diffusion model. That means that the model manipulates vectors within the latent space manifold of another model. In this case, that model is a Variational Autoencoder.\n\nvae = pipe.vae\n\nVariational autoencoders are trained to compress vector information into a normal distribution manifold and decompress it with minimal reconstruction loss.\n\n\n\nimage.png\n\n\nWe can visualize the information from a trained VAE.\n\nsource\n\n\nimage_from_url\n\n image_from_url (url:str)\n\n\nsource\n\n\ndecompress\n\n decompress (latents:torch.Tensor, vae:torch.nn.modules.module.Module,\n             as_pil=True, no_grad=True)\n\nProject latents into pixel space\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatents\nTensor\n\nVAE latents\n\n\nvae\nModule\n\nVAE\n\n\nas_pil\nbool\nTrue\nReturn a PIL image\n\n\nno_grad\nbool\nTrue\nDiscard forward gradientss\n\n\n\n\nsource\n\n\ncompress\n\n compress (img:PIL.Image.Image, vae:torch.nn.modules.module.Module)\n\nProject pixels into latent space\n\n\n\n\nType\nDetails\n\n\n\n\nimg\nImage\nInput image\n\n\nvae\nModule\nVAE\n\n\n\n\ninput_ = image_from_url(\n    \"https://www.perfectduluthday.com/wp-content/uploads/2021/12/Weird_Al_Yankovic_profile.jpg\"\n)\nlatents = compress(input_, vae)\noutput = decompress(latents, vae)\nshow_images([input_, *latents.squeeze().cpu(), output])\nn_params_img = np.product(input_.shape)\nn_params_latents = np.product(latents.shape)\nf\"Compression rate: {n_params_img/n_params_latents:.2f}x\"\n\n'Compression rate: 49.13x'\n\n\n\n\n\n\n\n\n\nWe get 50x compression with no loss in detail.\nNote that, since we‚Äôre dealing with noise, that noise in the latent space does not correspond to Gaussian noise in the pixel space.\n\nnoise = torch.randn(latents.shape, device=latents.device)\nnoised_latents = latents * 0.6 + noise * 0.4\nimg = decompress(noised_latents, vae)\nimg\n\n\n\n\n\n\n\n\n\n\nU-net\nU-nets are the model used for denoising\n\nunet = pipe.unet\n\nWe‚Äôll get much more into the architecture of the U-net later. For now, know that it is a model with a strong translational equivariance property.\nThere are two inputs to the model: the prompt and the time.\nThe time is an index of how noisy the image is. This helps the model to determine how much noise to remove, because the amount of noise added at any given step is non-linear.\n\nscheduler = pipe.scheduler\nscheduler\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.17.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null,\n  \"use_karras_sigmas\": false\n}\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nX = timesteps = scheduler.timesteps.to(torch.uint8).cpu().numpy()\nY = scheduler.betas.cpu().numpy()[timesteps]\nax.scatter(X, Y, marker=\"+\")\nax.set(xlabel=\"Time\", ylabel=r\"$\\beta$\")\n\n\n\n\n\n\n\n\n\n\nCLIP\nThe other thing we could add to make the problem of de-noising easier is to indicate the image class (1, 2, 3, etc). For a simple class distribution, we could just one hot-encode it. The input-output pair would then be \\(\\langle \\left( image + \\epsilon, t, class \\right), \\epsilon \\rangle\\).\nBut we cannot one-hot encode the distribution of images on the internet √† la Stable Diffusion. Therefore, we need a more sophisticated encoder: CLIP (Constrastively Learned Image Pairs). This works on the idea that the dot-product between image encoding and text encoding of the same thing should be large, while the image encoding and text encoding for different things should be small.\nWe learn this with a neural network contrastively. For a given batch, \\(B\\), of (image, language) pairs from html alt tags:\n\nCompute the encoding \\(f_{image}(I_i)\\) and \\(f_{language}(L_i)\\) for all \\(i \\in |B|\\)\nCompute the sum \\(\\text{correctly paried loss} := \\Sigma_i^{|B|} f_{image}(I_i) \\cdot f_{language}(L_i)\\)\nCompute the sum\n\n\\[\n\\text{incorrectly paired loss} := \\Sigma_i^{|B|} \\Sigma_j^{|B|}  \\begin{cases}\n    f_{image}(I_i) \\cdot f_{language}(L_j) & i \\neq j\\\\\n    0 & i = j\n  \\end{cases}\n\\]\n\nFinal loss = incorrectly paired loss - correctly paired loss. Note that want the overall loss to be small or negative, so we take the negative of the sum of the correctly paired dot products. This pushes the vectors for correctly paired language image to be in the same subspace, and incorrectly paired counterparts into different subspaces.\n\n\nThe transformers library has a module for this\n\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\ntokens\n\ntensor([[49406,   320,  1125,   539,   320, 22826,   530,  3445, 49407]])\n\n\nWe can go backwards like so:\n\nfor t in tokens[0, :8]:\n    print(f\"{t}: {tokenizer.decoder.get(int(t))}\")\n\n49406: &lt;|startoftext|&gt;\n320: a&lt;/w&gt;\n1125: photo&lt;/w&gt;\n539: of&lt;/w&gt;\n320: a&lt;/w&gt;\n22826: giraffe&lt;/w&gt;\n530: in&lt;/w&gt;\n3445: paris&lt;/w&gt;\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(tokens.to(TORCH_DEVICE)).last_hidden_state\ntext_embeddings.shape\n\ntorch.Size([1, 9, 768])\n\n\nIn fact, this isn‚Äôt used to directly denoise the image during inference. We use a hack called Classifier Free Guidance (CFG), where ‚Äì in addition to the latents ‚Äì the model is prompted with a null prompt (an unconditional prompt) and and the original prompt.\nSee more here: https://www.youtube.com/watch?v=344w5h24-h8",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "overview.html#putting-it-together",
    "href": "overview.html#putting-it-together",
    "title": "Course overview",
    "section": "Putting it together",
    "text": "Putting it together\nWith these components in mind, we can start to put them together.\n\nsource\n\nStableDiffusion\n\n StableDiffusion\n                  (tokenizer:transformers.models.clip.tokenization_clip.CL\n                  IPTokenizer, text_encoder:transformers.models.clip.model\n                  ing_clip.CLIPTextModel, scheduler:Any, unet:diffusers.mo\n                  dels.unets.unet_2d_condition.UNet2DConditionModel, vae:d\n                  iffusers.models.autoencoders.autoencoder_kl.AutoencoderK\n                  L)\n\n\nsd = StableDiffusion(\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler,\n    unet=unet,\n    vae=vae,\n)\nsd(prompt, n_inference_steps=30, as_pil=True)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:40&lt;00:00,  1.35s/it]",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "overview.html#sampling",
    "href": "overview.html#sampling",
    "title": "Course overview",
    "section": "Sampling",
    "text": "Sampling\nThere are many ways to sample from Stable Diffusion. The original sampling was a simple numerical differential equation solver (known as the linear multistep solver or LMS): where we take the steps that the gradient suggests with the magnitude of the solver.\nIt should be noted that this is similar to solving for neural network weights. Therefore, we take a step accourding to the optimizer and wait for convergence.\nWe can also use the tricks associated with successful neural network training. It is well-understood that the derivative isn‚Äôt always a good indicator of the loss curvature. We can improve the quality of the right direction by incorporating previous computations of the gradient (i.e., momentum).\nRead more about that here: https://stable-diffusion-art.com/samplers/",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "overview.html#hacking",
    "href": "overview.html#hacking",
    "title": "Course overview",
    "section": "Hacking",
    "text": "Hacking\n\nOptimizing pixels directly\nWe are not constrained to using the algorithms as presented to us. This is all just calculus, so we can optimize with respect to arbirary loss functions.\n\n@dataclass\nclass StableDiffusionWithArbitraryLoss(StableDiffusion):\n    loss_f: Callable\n    k: float\n    periodicity: int = 5\n\n    def denoise(self, prompt_embedding, l, t, guidance_scale, i):\n        if i % self.periodicity == 0 and i != 0:\n            # Calculate noise as per usual\n            noise_pred = self.pred_noise(prompt_embedding, l, t, guidance_scale)\n\n            # Create a copy of the latents that keeps track of the gradients\n            l = l.detach().requires_grad_()\n\n            # Take a step all the way towards a predicted x0 and use this to\n            # compute the loss\n            l_x0 = self.scheduler.step(noise_pred, t, l).pred_original_sample\n            image_x0 = decompress(l_x0, self.vae, as_pil=False, no_grad=False)\n            loss = self.loss_f(image_x0) * self.k\n            print(f\"{i}: arbritrary loss: {loss}\")\n\n            # Compute the loss gradient with respect to the latents and take\n            # a step in that direction\n            (grad,) = torch.autograd.grad(loss, l)\n            sigma = self.scheduler.sigmas[i]\n            l = l.detach() - grad * sigma**2\n        else:\n            with torch.no_grad():\n                noise_pred = self.pred_noise(prompt_embedding, l, t, guidance_scale)\n\n        l = self.scheduler.step(noise_pred, t, l).prev_sample\n\n        return l\n\n\ndef blue_loss(images):\n    return torch.abs(images[:, 2] - 0.9).mean()\n\n\nStableDiffusionWithArbitraryLoss(\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler,\n    unet=unet,\n    vae=vae,\n    loss_f=blue_loss,\n    k=75,\n    periodicity=5,\n)(\n    \"a photo of an octopus, national geographic, dlsr\",\n    n_inference_steps=30,\n    as_pil=True,\n)\n\n 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                        | 5/30 [00:06&lt;00:33,  1.33s/it]\n\n\n5: arbritrary loss: 40.17461013793945\n\n\n/Users/jeremiahfisher/miniforge3/envs/slowai/lib/python3.9/site-packages/torch/autograd/__init__.py:303: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                               | 10/30 [00:40&lt;01:21,  4.09s/it]\n\n\n10: arbritrary loss: 27.197010040283203\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                       | 15/30 [01:18&lt;01:09,  4.65s/it]\n\n\n15: arbritrary loss: 21.870607376098633\n\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                               | 20/30 [02:02&lt;00:53,  5.37s/it]\n\n\n20: arbritrary loss: 21.79729652404785\n\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 25/30 [02:42&lt;00:24,  4.96s/it]\n\n\n25: arbritrary loss: 21.87038230895996\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [03:27&lt;00:00,  6.91s/it]\n\n\n\n\n\n\n\n\n\nLook! Less blue than you would expect for an underwater scene. Cool üòé\n\n\nTinkering with the prompt embedding\nWe are also not constrained to the text embeddings directly from CLIP. We can take embeddings from different tokens and hack them apart.\n\nprompt = \"an adorable photo of a puppy\"\ntokens_a = tokenizer(prompt, return_tensors=\"pt\").input_ids\nfor t in tokens_a[0, ...]:\n    print(f\"{t}: {tokenizer.decoder.get(int(t))}\")\ntext_embeddings_a = text_encoder(tokens_a.to(TORCH_DEVICE)).last_hidden_state\ntext_embeddings_a.shape\n\n49406: &lt;|startoftext|&gt;\n550: an&lt;/w&gt;\n6298: adorable&lt;/w&gt;\n1125: photo&lt;/w&gt;\n539: of&lt;/w&gt;\n320: a&lt;/w&gt;\n6829: puppy&lt;/w&gt;\n49407: &lt;|endoftext|&gt;\n\n\ntorch.Size([1, 8, 768])\n\n\n\nprompt = \"a adorable photo of a koala\"\ntokens_b = tokenizer(prompt, return_tensors=\"pt\").input_ids\nfor t in tokens_b[0, ...]:\n    print(f\"{t}: {tokenizer.decoder.get(int(t))}\")\nwith torch.no_grad():\n    text_embeddings_b = text_encoder(tokens_b.to(TORCH_DEVICE)).last_hidden_state\ntext_embeddings_b.shape\n\n49406: &lt;|startoftext|&gt;\n320: a&lt;/w&gt;\n6298: adorable&lt;/w&gt;\n1125: photo&lt;/w&gt;\n539: of&lt;/w&gt;\n320: a&lt;/w&gt;\n36654: koala&lt;/w&gt;\n49407: &lt;|endoftext|&gt;\n\n\ntorch.Size([1, 8, 768])\n\n\n\ncombined_text_embeddings = (text_embeddings_a + text_embeddings_b) / 2\n\n\nclass PuppyKoala(StableDiffusion):\n    def embed_prompt(self, _: str) -&gt; torch.tensor:\n        global combined_text_embeddings\n        _, max_length, _ = combined_text_embeddings.shape\n        uncond_input = self.tokenizer(\n            [\"\"],\n            padding=\"max_length\",\n            max_length=max_length,\n            return_tensors=\"pt\",\n        )\n        with torch.no_grad():\n            uncond_embeddings = self.text_encoder(\n                uncond_input.input_ids.to(TORCH_DEVICE)\n            )\n            uncond_embeddings = uncond_embeddings[0]\n        return torch.cat([uncond_embeddings, combined_text_embeddings])\n\n\nsd = PuppyKoala(\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler,\n    unet=unet,\n    vae=vae,\n)\n\nsd(prompt, n_inference_steps=30, as_pil=True)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:58&lt;00:00,  1.96s/it]",
    "crumbs": [
      "Course overview"
    ]
  },
  {
    "objectID": "initializations.html",
    "href": "initializations.html",
    "title": "Initializations",
    "section": "",
    "text": "Adapted from:\nset_seed(42)\nplt.style.use(\"ggplot\")",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#probability-misc",
    "href": "initializations.html#probability-misc",
    "title": "Initializations",
    "section": "Probability, misc",
    "text": "Probability, misc\n\n\n\n\n\n\n\n\nTopic\nDescription\nFormula\n\n\n\n\nAverage Error\nTypically 0 because positive and negative errors cancel out, making this metric not useful.\n$ = y_{true} - y_{pred} $\n\n\nVariance\nThe expected value of the squared deviation from the mean of a random variable.\n\\(\\sigma^2 = \\sum \\frac{(x-\\mathbb{E}[X])^2}{N}\\)  \\(\\sigma^2 = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\)\n\n\nCovariance\nA measure of the joint variability of two random variables.\n\\(\\operatorname{cov}(X,Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}\\)\n\n\nPearson Correlation Coefficient\nA normalized measure of the linear relationship between two variables.\n\\(\\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y}\\)",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#baseline",
    "href": "initializations.html#baseline",
    "title": "Initializations",
    "section": "Baseline",
    "text": "Baseline\nLet‚Äôs look at a fashion MNIST classification problem. Our goal is to get accuracy to at least 90%.\n\nclass CNN(nn.Module):\n    \"\"\"Six layer convolutional neural network\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            Conv2dWithReLU(1, 8, kernel_size=5, stride=2, padding=2),  # 14x14\n            Conv2dWithReLU(8, 16, kernel_size=3, stride=2, padding=1),  # 7x7\n            Conv2dWithReLU(16, 32, kernel_size=3, stride=2, padding=1),  # 4x4\n            Conv2dWithReLU(32, 64, kernel_size=3, stride=2, padding=1),  # 2x2\n            nn.Conv2d(64, 10, kernel_size=3, stride=2, padding=1),  # 1x1\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        return rearrange(x, \"bs c w h -&gt; bs (c w h)\")\n\nLet‚Äôs start by finding the learning rate.\nThis diverges very quickly\n\nTrainLearner(\n    CNN(),\n    fashion_mnist(),\n    F.cross_entropy,\n    lr=0.6,\n    cbs=[LRFinderCB(), MomentumCB()],\n).fit()\n\n\n\n\n\n\n\n\nThat‚Äôs unusable. Let‚Äôs dial down the learning rate increase and start at a lower learning rate.\n\nLRFinderCB?\n\n\nInit signature: LRFinderCB(gamma=1.3, max_mult=3)\nDocstring:     \nFind an apopriate learning rate by increasing it by a constant factor for each batch\nuntil the loss diverges\nFile:           ~/Desktop/SlowAI/nbs/slowai/learner.py\nType:           type\nSubclasses:     \n\n\n\n\nTrainLearner(\n    CNN(),\n    fashion_mnist(64),\n    F.cross_entropy,\n    lr=1e-2,\n    cbs=[LRFinderCB(gamma=1.05), MomentumCB()],\n).fit()\n\n\n\n\n\n\n\n\n\nmodel = CNN()\nstats = StoreModuleStatsCB(mods=model.layers)\ncbs = [\n    MomentumCB(),\n    MetricsCB(MulticlassAccuracy(num_classes=10)),\n    DeviceCB(),\n    ProgressCB(plot=True),\n    stats,\n]\nTrainLearner(\n    model,\n    fashion_mnist(),\n    F.cross_entropy,\n    lr=0.6,\n    cbs=cbs,\n).fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.168\n2.378\n0\ntrain\n\n\n0.100\n2.301\n0\neval\n\n\n0.203\n2.170\n1\ntrain\n\n\n0.489\n1.624\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats.hist_plot()\n\n\n\n\n\n\n\n\nThe issue with neural networks in real life is that they must be represented by discrete data structures. These can overflow‚Ä¶\n\nx = torch.randn(200, 100)\nfor i in range(50):\n    x = x @ torch.randn(100, 100)\nx[0:5, 0:5]\n\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])\n\n\n‚Ä¶or underflow..\n\nx = torch.randn(200, 100)\nfor i in range(50):\n    x = x @ torch.randn(100, 100) * 0.01\nx[0:5, 0:5]\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nWe should scale our weights such that, throughout the training process, the mean remains near 0 and the standard deviation remains near 1. Otherwise, we are suseptible to these ‚Äúdead units‚Äù",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#glorotxavier",
    "href": "initializations.html#glorotxavier",
    "title": "Initializations",
    "section": "Glorot/Xavier",
    "text": "Glorot/Xavier\nNormally, the mean variance of a layer is approximately the number of output dimensions\n\nfor d in [1, 100]:\n    means = []\n    sqrs = []\n\n    for _ in range(500):\n        x = torch.randn(d)\n        a = torch.randn(512, d)\n        y = a @ x\n        means.append(y.mean())\n        sqrs.append(y.pow(2).mean())\n\n    res = torch.stack(means).mean(), torch.stack(sqrs).mean()\n    print(res)\n\n(tensor(-0.0011), tensor(1.1372))\n(tensor(-0.0029), tensor(100.2205))\n\n\nTo encourage of variance of 1, the paper from Xavier Glorot and Yoshua Bengio proposed that each layer have a coefficient of \\(\\frac{1}{\\sqrt{d}}\\) where \\(d\\) is the number of inputs\n\nd = 100\nx = torch.randn(200, d)\nfor i in range(50):\n    layer = torch.randn(d, d) * (1 / math.sqrt(d))\n    x = x @ layer\nx[0:5, 0:5]\n\ntensor([[ 1.7367, -0.3418, -2.1874, -0.9467,  1.0451],\n        [ 0.7802,  0.3702, -0.9278,  0.1607,  0.7578],\n        [-2.0552,  1.0210,  1.7889,  0.1260, -1.0092],\n        [-0.4922, -0.1349,  0.0443, -0.3037, -0.1711],\n        [ 0.0103,  0.0468, -0.6201, -1.1773,  0.1258]])\n\n\nIndeed, this gives us reasonable numbers! However, this doesn‚Äôt work for modern deep learning because of the activation layers.",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#kaiminghe-init",
    "href": "initializations.html#kaiminghe-init",
    "title": "Initializations",
    "section": "Kaiming/He init",
    "text": "Kaiming/He init\nNotice that the first layer of a neural network has a favorable activation distribution\n\nd1, d2 = 100, 100\nx = torch.randn(200, d1)\ny = torch.randn(200)\nw = torch.randn(d1, d2) / math.sqrt(d1)\nb = torch.zeros(d2)\n\n\nl1 = x @ w + b\nl1.mean(), l1.std()\n\n(tensor(0.0034), tensor(0.9888))\n\n\nBut what happens when we add non-linearities?\n\ndef relu(x):\n    return x.clamp_min(0.0)\n\n\nl1r = relu(l1)\nl1r.mean(), l1r.std()\n\n(tensor(0.3943), tensor(0.5795))\n\n\nIndeed, this property is lost‚Ä¶thus goes the path to ruin‚Ä¶\n\nl = l1r\nfor _ in range(50):\n    w = torch.randn(d2, d2) * math.sqrt(1 / d2)\n    l = relu(l @ w)\nl[0:5, 0:5]\n\ntensor([[3.6994e-08, 0.0000e+00, 9.4013e-09, 1.5577e-08, 1.0590e-08],\n        [5.5068e-08, 0.0000e+00, 1.1871e-08, 2.4266e-08, 1.3567e-08],\n        [9.3161e-08, 0.0000e+00, 2.2955e-08, 4.0294e-08, 2.4063e-08],\n        [4.9314e-08, 0.0000e+00, 1.1283e-08, 2.1542e-08, 1.3132e-08],\n        [3.8813e-08, 0.0000e+00, 9.7021e-09, 1.6069e-08, 1.1153e-08]])\n\n\nTo rectify this situation, Kaiming proposed using \\(\\sqrt{\\frac{2}{d}}\\)\n\nl = l1r\nfor _ in range(50):\n    w = torch.randn(d2, d2) * math.sqrt(2 / d2)\n    l = relu(l @ w)\nl[0:5, 0:5]\n\ntensor([[0.0218, 0.0350, 0.0000, 0.4040, 0.0000],\n        [0.1481, 0.0000, 0.0000, 0.5991, 0.0000],\n        [0.0582, 0.1006, 0.0000, 0.7180, 0.0162],\n        [0.1623, 0.0000, 0.0000, 0.9592, 0.0000],\n        [0.1217, 0.0169, 0.0000, 0.6792, 0.0000]])\n\n\nNotice, there are still positive numbers even after 50 layers!\nThis is the best way to initialize a network. But how do we do this on a code level? We use the .apply() method.\n\ndef init_weights(module):\n    # `kaiming_normal_` is an in-place operation, unlike `kaiming_normal`\n    if not isinstance(module, (nn.Sequential, Conv2dWithReLU, CNN)):\n        init.kaiming_normal_(module.weight)\n\n\nmodel = CNN()\nmodel.apply(init_weights)\nlearn = TrainLearner(\n    CNN(),\n    fashion_mnist(64),\n    F.cross_entropy,\n    lr=1e-2,\n    cbs=[LRFinderCB(gamma=1.05), MomentumCB()],\n).fit()\n\n\n\n\n\n\n\n\nBetter! Let‚Äôs try training with 0.25.\n\nmodel = CNN()\nmodel.apply(init_weights)\nstats = StoreModuleStatsCB(mods=model.layers)\ncbs = [\n    MetricsCB(MulticlassAccuracy(num_classes=10)),\n    DeviceCB(),\n    ProgressCB(plot=True),\n    MomentumCB(),\n    stats,\n]\nTrainLearner(\n    model,\n    fashion_mnist(),\n    F.cross_entropy,\n    lr=0.25,\n    cbs=cbs,\n).fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.549\n1.430\n0\ntrain\n\n\n0.673\n0.951\n0\neval\n\n\n0.749\n0.665\n1\ntrain\n\n\n0.752\n0.646\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats.mean_std_plot()\n\n\n\n\n\n\n\n\nThat‚Äôs okay! Keep in mind, I did globally normalize the inputs.\nLet‚Äôs try normalizing batchwise.\n\nsource\n\nBatchTransformCB\n\n BatchTransformCB (tfm, on_train=True, on_val=True)\n\nArbitrarily transform a batch\n\nsource\n\n\nNormalizeBatchCB\n\n NormalizeBatchCB (on_train=True, on_val=True)\n\nUnit normalize a batch\n\nmodel = CNN()\nmodel.apply(init_weights)\nstats = StoreModuleStatsCB(mods=model.layers)\ndls = fashion_mnist()\ncbs = [\n    MetricsCB(MulticlassAccuracy(num_classes=10)),\n    DeviceCB(),\n    ProgressCB(plot=True),\n    NormalizeBatchCB(),\n    MomentumCB(),\n    stats,\n]\nlearn = TrainLearner(\n    model,\n    dls,\n    F.cross_entropy,\n    lr=0.25,\n    cbs=cbs,\n)\nlearn.fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.549\n1.398\n0\ntrain\n\n\n0.706\n0.823\n0\neval\n\n\n0.758\n0.665\n1\ntrain\n\n\n0.769\n0.799\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats.mean_std_plot()\n\n\n\n\n\n\n\n\nThis isn‚Äôt much better, unsurprisingly since it was already normalized.",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#why-arent-these-activation-distributions-unit-normal",
    "href": "initializations.html#why-arent-these-activation-distributions-unit-normal",
    "title": "Initializations",
    "section": "Why aren‚Äôt these activation distributions unit normal?",
    "text": "Why aren‚Äôt these activation distributions unit normal?\nThe problem is that StoreModuleStatsCB looks at the output of the ReLU + Convolutional blocks, which cannot output negative numbers (except for the last layer which does not have an non-linearity). Therefore, their output cannot look unit normal!\nWhat if we modified ReLU to allow unit normality?\n\\[\n\\text{GeneralReLU}_{l,r,m}(x) = min(\\{m,\n\\left.\n  \\begin{cases}\n    x & \\text{if } x &gt; 0 \\\\\n    l \\cdot x & \\text{otherwise}\n  \\end{cases}\n  \\right\\} - r\n\\})\n\\]\n\nsource\n\nGeneralReLU\n\n GeneralReLU (leak=None, sub=None, max_=None)\n\nGeneralized ReLU activation function with normalization and leakiness\nThis should output a roughly unit normal distribution by pushing all the values down and allowing some contribution from negative values.\n\nx = torch.linspace(-5, 5, steps=100)\ngr = GeneralReLU(leak=0.1, sub=0.4)\nplt.plot(x, gr(x));\n\n\n\n\n\n\n\n\n\nC = Conv2dWithReLU\n\n\nclass CNNWithGeneralReLU(nn.Module):\n    \"\"\"Six layer convolutional neural network with GeneralRelU\"\"\"\n\n    def __init__(self, gr=lambda: GeneralReLU(leak=0.1, sub=0.4)):\n        super().__init__()\n        layers = [  #\n            C(1, 8, kernel_size=5, stride=2, padding=2, nonlinearity=gr()),  # 14x14\n            C(8, 16, 3, 2, 1, nonlinearity=gr()),  # 7x7\n            C(16, 32, 3, 2, 1, nonlinearity=gr()),  # 4x4\n            C(32, 64, 3, 2, 1, nonlinearity=gr()),  # 2x2\n            nn.Conv2d(64, 10, 3, 2, 1),  # 1x1\n        ]\n        self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        *_, w, h = x.shape\n        assert w == h == 1\n        return rearrange(x, \"bs c w h -&gt; bs (c w h)\")\n\n‚ö†Ô∏è Now that we are no longer using ReLU outputs, so Kaiming initialization is not acceptable! Luckily, there is an adjustment for this leaky property in the pytorch script\n\nsource\n\n\ninit_leaky_weights\n\n init_leaky_weights (module, leak=0.0)\n\n\nleak = 0.1\nmodel = CNNWithGeneralReLU(gr=lambda: GeneralReLU(leak=leak, sub=0.4))\nmodel.apply(partial(init_leaky_weights, leak=leak))\nstats = StoreModuleStatsCB(mods=model.layers)\ncbs = [\n    MetricsCB(MulticlassAccuracy(num_classes=10)),\n    DeviceCB(),\n    ProgressCB(plot=True),\n    NormalizeBatchCB(),\n    MomentumCB(),\n    stats,\n]\nlearn = TrainLearner(\n    model,\n    fashion_mnist(),\n    F.cross_entropy,\n    lr=0.25,\n    cbs=cbs,\n)\nlearn.fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.656\n0.996\n0\ntrain\n\n\n0.762\n0.640\n0\neval\n\n\n0.789\n0.573\n1\ntrain\n\n\n0.761\n0.663\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats.mean_std_plot()\n\n\n\n\n\n\n\n\nThat looks GREAT ü§© What happens if we keep on training that?\n\nlearn.fit(10)\n\n86% accuracy! Not bad.\nThis goes to show that initialization is underappreciated.",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#general-initializations",
    "href": "initializations.html#general-initializations",
    "title": "Initializations",
    "section": "General Initializations",
    "text": "General Initializations\n‚ÄúAll You Need Is a Good Init‚Äù proposed that there is a generic strategy to initialize any model.\n\nSetup your model any way you like\nPass a batch of data through the model and record the layer wise activation statistics\nApply a unit normalization\nRepeat step 2 until the activation statistics are unit normal within an acceptable tolerance\n\nIn code:\n\nsource\n\nLSUVHook\n\n LSUVHook (m, tol=0.001)\n\nAn abstract hook for performing LSUV for a single layer. .normalize() needs to be implemented for the specific layers\n\nclass CNNLSUVHook(LSUVHook):\n    def normalize(self):\n        if isinstance(self.m, (nn.Conv2d)):\n            # Final layer\n            self.m.bias -= self.mean\n            self.m.weight.data /= self.std\n        else:\n            self.m.conv.bias -= self.mean\n            self.m.conv.weight.data /= self.std\n\n\nsource\n\n\nLSUVInitialization\n\n LSUVInitialization (mods=None, mod_filter=&lt;function noop&gt;, on_train=True,\n                     on_valid=False, hook_cls=&lt;class '__main__.LSUVHook'&gt;)\n\nLayer wise sequential unit variance initialization\n\nleak = 0.1\nmodel = CNNWithGeneralReLU(gr=partial(GeneralReLU, leak=leak))\n\n# Note that we're removing the `model.apply` in favor of LSUVInitialization\n\nstats = StoreModuleStatsCB(mods=model.layers)\ncbs = [\n    MetricsCB(MulticlassAccuracy(num_classes=10)),\n    DeviceCB(),\n    ProgressCB(plot=True),\n    NormalizeBatchCB(),\n    MomentumCB(),\n    LSUVInitialization(mods=model.layers, hook_cls=CNNLSUVHook),\n    stats,\n]\nlearn = TrainLearner(\n    model,\n    dls,\n    F.cross_entropy,\n    lr=0.25,\n    cbs=cbs,\n)\nlearn.fit(2)\n\nLayer 0 normalized after 36 batches (0.00, 1.00)\nLayer 1 normalized after 37 batches (0.00, 1.00)\nLayer 2 normalized after 35 batches (0.00, 1.00)\nLayer 3 normalized after 37 batches (0.00, 1.00)\nLayer 4 normalized after 2 batches (-0.00, 1.00)\n\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.515\n1.553\n0\ntrain\n\n\n0.611\n1.258\n0\neval\n\n\n0.710\n0.928\n1\ntrain\n\n\n0.749\n0.779\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats.mean_std_plot()\n\n\n\n\n\n\n\n\nThat‚Äôs the most accurate yet! The moments look perfect.",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#batch-and-layer-normalization",
    "href": "initializations.html#batch-and-layer-normalization",
    "title": "Initializations",
    "section": "Batch and layer normalization",
    "text": "Batch and layer normalization\nWe have been normalizing the inputs before training, but the input distribution can change during training. This is known as internal covariate shift. Can we compensate?\nIndeed, we can simply normalize as part of the model architecture\n\nLayerNorm\nProposed one year after BatchNorm by Hinton, but simpler so we‚Äôll start with that. Quite simply implemented like so\n\nclass LayerNorm(nn.Module):\n    def __init__(self, eps=1e-5):\n        super().__init__()\n        self.k = nn.Parameter(torch.tensor(1.0))\n        self.a = nn.Parameter(torch.tensor(0.0))\n        self.eps = eps\n\n    def forward(self, x):\n        m = x.mean((1, 2, 3), keepdim=True)\n        v = x.var((1, 2, 3), keepdim=True)\n        x = (x - m) / ((v + self.eps).sqrt())\n        return x\n\nJeremy notes that:\n\nAlthough epsilon is used commonly to correct for the ‚Äúdead unit‚Äù issue, you should not rely on the defaults. Often, they are too small.\nThis is not really a ‚Äúnormalization‚Äù because k and a can be anything; the network is learning the apropriate distribution moments ‚Äì and perhaps this is easier for the network\n\n\nsource\n\n\nConv2dGeneral\n\n Conv2dGeneral (*args, nonlinearity=&lt;function relu&gt;, norm=None,\n                stride:Union[int,Tuple[int,int]]=1,\n                padding:Union[str,int,Tuple[int,int]]=0,\n                dilation:Union[int,Tuple[int,int]]=1, groups:int=1,\n                bias:bool=True, padding_mode:str='zeros', device=None,\n                dtype=None)\n\nConvolutional neural network with a built in activation\n\nclass CNNWithGeneralReLUAndLayerNorm(nn.Module):\n    \"\"\"Six layer convolutional neural network with GeneralRelU\"\"\"\n\n    def __init__(self, gr=GeneralReLU, norm=LayerNorm):\n        super().__init__()\n        layers = [\n            C(1, 8, kernel_size=5, stride=2, padding=2, nonlinearity=gr(), norm=norm()),\n            C(8, 16, 3, 2, 1, nonlinearity=gr(), norm=norm()),\n            C(16, 32, 3, 2, 1, nonlinearity=gr(), norm=norm()),\n            C(32, 64, 3, 2, 1, nonlinearity=gr(), norm=norm()),\n            nn.Conv2d(64, 10, 3, 2, 1),  # 1x1\n        ]\n        self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        *_, w, h = x.shape\n        assert w == h == 1\n        return rearrange(x, \"bs c w h -&gt; bs (c w h)\")\n\n\nleak = 0.1\nmodel = CNNWithGeneralReLUAndLayerNorm(gr=partial(GeneralReLU, leak=leak))\nmodel.apply(partial(init_leaky_weights, leak=leak))\nstats = StoreModuleStatsCB(mods=model.layers)\ncbs = [\n    MetricsCB(MulticlassAccuracy(num_classes=10)),\n    DeviceCB(),\n    ProgressCB(plot=True),\n    NormalizeBatchCB(),\n    MomentumCB(),\n    stats,\n]\nlearn = TrainLearner(\n    model,\n    dls,\n    F.cross_entropy,\n    lr=0.25,\n    cbs=cbs,\n)\nlearn.fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.666\n0.985\n0\ntrain\n\n\n0.779\n0.621\n0\neval\n\n\n0.819\n0.501\n1\ntrain\n\n\n0.801\n0.561\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nEven more accurate, by a bit.\nWhile this does help, it turns out that BatchNorm and LayerNorm introduces complixities that have led to me lose favor recently. This just means that the initialization is even more important, as a skill.\nThat being said, here‚Äôs the code for BatchNorm.\n\nsource\n\n\nBatchNorm\n\n BatchNorm (num_filters, mom=0.1, eps=1e-05)\n\nBatch normalization\nWhat does lerp_ do here?\n\nmomentum = 0.5\ntorch.lerp(tensor(1.0), tensor(2.0), tensor(momentum))\n\ntensor(1.5000)\n\n\n\nmomentum = 0.3333\ntorch.lerp(tensor(1.0), tensor(2.0), tensor(momentum))\n\ntensor(1.3333)\n\n\nThe basic idea is to normalize the batches accourding to the weighted average of the means and variances of the previous batches. Hopefully, this makes it such that if the model encounters a ‚Äúweird‚Äù batch, it‚Äôs not thrown off too much.\n\nsource\n\n\nCNNWithGeneralReLUAndBatchNorm\n\n CNNWithGeneralReLUAndBatchNorm (gr=&lt;class '__main__.GeneralReLU'&gt;)\n\nSix layer convolutional neural network with GeneralRelU\n\nleak = 0.1\nmodel = CNNWithGeneralReLUAndBatchNorm(gr=partial(GeneralReLU, leak=leak))\nmodel.apply(partial(init_leaky_weights, leak=leak))\nstats = StoreModuleStatsCB(mods=model.layers)\ncbs = [\n    MetricsCB(MulticlassAccuracy(num_classes=10)),\n    DeviceCB(),\n    ProgressCB(plot=True),\n    NormalizeBatchCB(),\n    MomentumCB(),\n    stats,\n]\nlearn = TrainLearner(\n    model,\n    dls,\n    F.cross_entropy,\n    lr=0.25,\n    cbs=cbs,\n)\nlearn.fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.702\n0.845\n0\ntrain\n\n\n0.769\n0.666\n0\neval\n\n\n0.817\n0.503\n1\ntrain\n\n\n0.806\n0.556\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nJust a tiny bit better, but that looks very smooth. Let‚Äôs see if we can increase the learning rate.\n\nmodel = CNNWithGeneralReLUAndBatchNorm(gr=partial(GeneralReLU, leak=leak))\nmodel.apply(partial(init_leaky_weights, leak=leak))\nlearn = TrainLearner(\n    model,\n    dls,\n    F.cross_entropy,\n    lr=0.4,\n    cbs=cbs,\n)\nlearn.fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.419\n1.934\n0\ntrain\n\n\n0.172\n193.459\n0\neval\n\n\n0.399\n2.227\n1\ntrain\n\n\n0.100\n2.383\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\nNope‚Ä¶",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "initializations.html#towards-90",
    "href": "initializations.html#towards-90",
    "title": "Initializations",
    "section": "Towards 90%",
    "text": "Towards 90%\nLet‚Äôs try a few different things:\n\nLower batch size\nDecrease learning rate (smaller batch sizes neccesitate lower learning rates)\nTrain for longer\nFinetune with an even lower learning rate\n\n\nmodel = CNNWithGeneralReLUAndBatchNorm(gr=partial(GeneralReLU, leak=leak))\nmodel.apply(partial(init_leaky_weights, leak=leak))\nlearn = TrainLearner(\n    model,\n    fashion_mnist(256),\n    F.cross_entropy,\n    lr=0.2,\n    cbs=cbs,\n)\nlearn.fit(3)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.817\n0.504\n0\ntrain\n\n\n0.848\n0.426\n0\neval\n\n\n0.870\n0.356\n1\ntrain\n\n\n0.862\n0.385\n1\neval\n\n\n0.883\n0.319\n2\ntrain\n\n\n0.870\n0.362\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn = TrainLearner(\n    model,\n    fashion_mnist(256),\n    F.cross_entropy,\n    lr=0.05,\n    cbs=cbs,\n)\nlearn.fit(2)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.900\n0.276\n0\ntrain\n\n\n0.882\n0.328\n0\neval\n\n\n0.903\n0.267\n1\ntrain\n\n\n0.886\n0.324\n1\neval\n\n\n\n\n\n\n\n\n\n\n\n\n88%! So close. We‚Äôll get there in the next notebook.",
    "crumbs": [
      "Initializations"
    ]
  },
  {
    "objectID": "predicting_t.html",
    "href": "predicting_t.html",
    "title": "Predicting \\(t\\) as a function of \\(x_t\\)",
    "section": "",
    "text": "Adapted from\naesthetics()\nWe need to be empirical and understand the success conditions. We need to know if our model is good ‚Äì or if its even better than random!\nNote that, to use our training loop, our torch modules need at least one trainable parameter and a trainable output.\nclass Dummy(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # We must have at least one parameter\n        self._ = nn.Linear(1, 1)\n\n    def forward(self, x_t):\n        bs, *_ = x_t.shape\n        # We must always output a trainable parameter\n        return torch.full((bs,), 0.5, device=x_t.device, requires_grad=True)\nLet‚Äôs define our dataset.\npreprocess_ = torchvision.transforms.Compose(pipe)\ndef preprocess(examples):\n    x_0 = [preprocess_(img) for img in examples[\"image\"]]\n    x_0 = torch.stack(x_0)\n    ((x_t, t), _) = noisify(x_0)\n    return {\"x_t\": x_t, \"t\": t.float()}\ndef get_dls(bs=512):\n    dsd = load_dataset(\"fashion_mnist\")\n    dsd[\"train\"].set_transform(preprocess)\n    dsd[\"test\"] = (\n        dsd[\"test\"]\n        .map(preprocess, batched=True, remove_columns=[\"image\", \"label\"])\n        .with_format(\"torch\")\n    )\n    return DataLoaders.from_dsd(dsd, bs=bs).listify([\"x_t\", \"t\"])\nThis was a bit annoying to set up, I had to learn the following:\ntest = load_dataset(\"fashion_mnist\")[\"test\"]\nptest = test.map(preprocess, batched=True)\nxb = ptest[:2][\"x_t\"]\ncxb = default_collate(xb)\nassert type(cxb) == type(cxb[0]) == type(cxb[0][0]) == list\nprint(f\"inner most type: {type(cxb[0][0][0])}\")\nprint(f\"shape: {len(cxb), len(cxb[0]), len(cxb[0][0]), cxb[0][0][0].shape}\")\n\n\n\n\ninner most type: &lt;class 'torch.Tensor'&gt;\nshape: (1, 32, 32, torch.Size([2]))\nTo fix this, we need to convert arrow to torch before sending the batch to the collate function by adding .with_format(\"torch\") like so:\nxb = ptest.with_format(\"torch\")[:2][\"x_t\"]\nxb.shape\n\ntorch.Size([2, 1, 32, 32])\nThis is what it looks like:\ndls = get_dls()\n\n\n\n\nCPU times: user 2.23 s, sys: 229 ms, total: 2.46 s\nWall time: 2.73 s\nr = dls.splits[\"test\"][96]\nxt, t = r[\"x_t\"], r[\"t\"]\nshow_images(xt, titles=[t])\n(xb, t) = dls.peek()\nn = 8\nshow_images(\n    xb[:n].squeeze(),\n    titles=[f\"{tt.item():.2f}\" for tt in t[:n]],\n)\nNow, we can train the dummy model.\ndef train(model, dls, n_epochs=20, lr=1e-2, log_periodicity=10, extra_cbs=None):\n    T_max = len(dls[\"train\"]) * n_epochs\n    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)\n    cbs = [\n        DeviceCB(),\n        MetricsCB(),\n        ProgressCB(plot=True, periodicity=log_periodicity),\n        scheduler,\n    ]\n    if extra_cbs:\n        cbs.extend(extra_cbs)\n    learner = TrainLearner(model, dls, cbs=cbs)\n    learner.fit(n_epochs=n_epochs)\ntrain(Dummy(), dls, n_epochs=3)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.083\n0\ntrain\n\n\n0.083\n0\neval\n\n\n0.083\n1\ntrain\n\n\n0.083\n1\neval\n\n\n0.083\n2\ntrain\n\n\n0.083\n2\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 3.94 s, sys: 1.82 s, total: 5.76 s\nWall time: 11 s\nNotice that loss does not change during the second epoch. (It does change a tiny bit, for whatever reason.) This is our baseline. Anything worse than this is totally useless.\nWe can use the same model for classification from before, just changing the number of outputs and flattening it.\nclass RegressionResNet(ResNetWithGlobalPoolingInitialConv):\n    def __init__(self, nfs=[16, 32, 64, 128, 256, 512]):\n        return super().__init__(nfs, 1)\n\n    def forward(self, x):\n        x = super().forward(x)\n        assert x.shape[-1] == 1\n        return x.squeeze()\nt_predictor = RegressionResNet.kaiming()\nsummarize(t_predictor, \"ResidualConvBlock|Linear\")\ntrain(t_predictor, dls, n_epochs=5)\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nResidualConvBlock\n(8, 16, 28, 28)\n(8, 32, 14, 14)\n14,496\n2.8\n\n\nResidualConvBlock\n(8, 32, 14, 14)\n(8, 64, 7, 7)\n57,664\n2.8\n\n\nResidualConvBlock\n(8, 64, 7, 7)\n(8, 128, 4, 4)\n230,016\n3.7\n\n\nResidualConvBlock\n(8, 128, 4, 4)\n(8, 256, 2, 2)\n918,784\n3.7\n\n\nResidualConvBlock\n(8, 256, 2, 2)\n(8, 512, 1, 1)\n3,672,576\n3.7\n\n\nLinear\n(8, 512)\n(8, 1)\n513\n0.0\n\n\nTotal\n\n\n4,894,049\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.203\n0\ntrain\n\n\n0.003\n0\neval\n\n\n0.003\n1\ntrain\n\n\n0.003\n1\neval\n\n\n0.003\n2\ntrain\n\n\n0.002\n2\neval\n\n\n0.003\n3\ntrain\n\n\n0.002\n3\neval\n\n\n0.003\n4\ntrain\n\n\n0.002\n4\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 16.1 s, sys: 6.03 s, total: 22.1 s\nWall time: 27.6 s\n(xb, t) = dls.peek(split=\"test\")\nxb, t = xb[:8, ...], t[:8, ...]\nt_pred = t_predictor(xb.to(def_device))\ntitles = [f\"True: {tt.item():.2f}, Pred: {tp.item():.2f}\" for tt, tp in zip(t, t_pred)]\nshow_images(xb.squeeze(), titles=titles)\nThis is worse than what Jeremy reports, but he was using a training batch without augmentation, so he was likely learning the training data directly. Note that Jeremy uses a batch of training data and that our discrepancy is in-line with what we‚Äôre getting in the evaluation (unlike Jeremy‚Äôs, where the discrepancy is quite large at \\(\\approx\\sqrt{0.08}\\).)\nwith torch.no_grad():\n    mse = torch.nn.functional.mse_loss(t_pred.to(def_device), t.to(def_device))\nmse\n\ntensor(0.0037, device='cuda:0')\nNice to know that I‚Äôm at the point where I can spot bugs in Jeremy‚Äôs code üêõ\neval = ImageEval.fashion_mnist(bs=256)",
    "crumbs": [
      "Predicting $t$ as a function of $x_t$"
    ]
  },
  {
    "objectID": "predicting_t.html#applying-t-predictor-to-the-diffusion-model",
    "href": "predicting_t.html#applying-t-predictor-to-the-diffusion-model",
    "title": "Predicting \\(t\\) as a function of \\(x_t\\)",
    "section": "Applying \\(t\\)-predictor to the diffusion model",
    "text": "Applying \\(t\\)-predictor to the diffusion model\nAt this point, we can add the \\(t\\) predicting model to the diffusion process.\n\nclass ContinuousDDPMWithTPred(ContinuousDDPM):\n    @only\n    def predict(self, learn):\n        (x_t, t), _ = learn.batch\n        with torch.no_grad():\n            t = t_predictor(x_t).clip(0.01, 0.99)\n        learn.preds = learn.model(x_t, t).sample\n\n\nunet = UNet2DModel(\n    sample_size=(32, 32),\n    in_channels=1,\n    out_channels=1,\n    block_out_channels=(16, 32, 64, 128),\n    norm_num_groups=8,\n)\nddpm = ContinuousDDPMWithTPred(Œ≤max=0.01)\ntrain_ddpm(\n    unet,\n    lr=1e-2,\n    n_epochs=25,\n    opt_func=partial(torch.optim.Adam, eps=1e-5),\n    ddpm=ddpm,\n    bs=512,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.212\n0\ntrain\n\n\n0.085\n0\neval\n\n\n0.064\n1\ntrain\n\n\n0.054\n1\neval\n\n\n0.047\n2\ntrain\n\n\n0.044\n2\neval\n\n\n0.042\n3\ntrain\n\n\n0.042\n3\neval\n\n\n0.040\n4\ntrain\n\n\n0.039\n4\neval\n\n\n0.037\n5\ntrain\n\n\n0.040\n5\neval\n\n\n0.035\n6\ntrain\n\n\n0.034\n6\neval\n\n\n0.034\n7\ntrain\n\n\n0.034\n7\neval\n\n\n0.033\n8\ntrain\n\n\n0.032\n8\neval\n\n\n0.032\n9\ntrain\n\n\n0.034\n9\neval\n\n\n0.031\n10\ntrain\n\n\n0.032\n10\neval\n\n\n0.031\n11\ntrain\n\n\n0.030\n11\neval\n\n\n0.031\n12\ntrain\n\n\n0.030\n12\neval\n\n\n0.030\n13\ntrain\n\n\n0.030\n13\neval\n\n\n0.029\n14\ntrain\n\n\n0.028\n14\neval\n\n\n0.029\n15\ntrain\n\n\n0.030\n15\neval\n\n\n0.029\n16\ntrain\n\n\n0.029\n16\neval\n\n\n0.028\n17\ntrain\n\n\n0.028\n17\neval\n\n\n0.028\n18\ntrain\n\n\n0.028\n18\neval\n\n\n0.028\n19\ntrain\n\n\n0.028\n19\neval\n\n\n0.028\n20\ntrain\n\n\n0.028\n20\neval\n\n\n0.027\n21\ntrain\n\n\n0.028\n21\neval\n\n\n0.027\n22\ntrain\n\n\n0.027\n22\neval\n\n\n0.027\n23\ntrain\n\n\n0.027\n23\neval\n\n\n0.027\n24\ntrain\n\n\n0.027\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 10min 1s, sys: 3min 49s, total: 13min 50s\nWall time: 11min 29s\n\n\n\nsource\n\nddim_t_pred\n\n ddim_t_pred (model, sz=(16, 1, 32, 32), device='cpu', n_steps=100,\n              eta=1.0, noisify=&lt;function ddim_noisify&gt;)\n\n\nx_0 = ddim(unet, sz=(256, 1, 32, 32))\nshow_images(x_0[:8, ...].squeeze(), imsize=1.2)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:03&lt;00:00, 31.74time step/s]\n\n\n\n\n\n\n\n\n\n\nx_0.min(), x_0.max()\n\n(tensor(-0.5532, device='cuda:0'), tensor(0.6188, device='cuda:0'))\n\n\n\neval.fid(x_0)\n\n147.68310546875\n\n\nCompare to 789.25 without predicting \\(t\\) (and 158.81 for a real batch). So, the metrics are much better but it doesn‚Äôt look better?",
    "crumbs": [
      "Predicting $t$ as a function of $x_t$"
    ]
  },
  {
    "objectID": "style_transfer.html",
    "href": "style_transfer.html",
    "title": "Neural Style Transfer",
    "section": "",
    "text": "Adapted from:\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/640px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\"\nimg = download_image(url)\nxo, yo = 100, 100\nimg = img[:, xo : 256 + xo, yo : 256 + yo]\nshow_image(img.permute(1, 2, 0));\nimg.min(), img.max()\n\n(tensor(0.), tensor(1.))\nWe can optimize the raw pixels, like any other parameter.\nclass DummyDataset:\n    \"\"\"Dataset that yields n iterations of dummy data\"\"\"\n\n    def __init__(self, n):\n        self.n = n\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        x, y = 0, 0\n        return x, y\n\n\ndef get_dls(n):\n    return DataLoaders(\n        splits={\"train\": DummyDataset(n), \"test\": DummyDataset(1)}, nworkers=0\n    )\nclass TensorModel(nn.Module):\n    def __init__(self, tensor_):\n        super().__init__()\n        self.tensor_ = nn.Parameter(tensor_.clone())\n\n    def forward(self, *args, **kwargs):\n        return self.tensor_\nm = TensorModel(tensor_=torch.rand_like(img))\nm().shape\n\ntorch.Size([3, 256, 256])\nclass ImgOptCb(TrainCB):\n    def __init__(self, target):\n        self.target = target\n        self.intermediates = []\n\n    def predict(self, learn):\n        learn.preds = learn.model()\n        self.intermediates.append(learn.preds.clone())\n\n    def get_loss(self, learn):\n        learn.loss = learn.loss_func(learn.preds, self.target)\ndef optimize_noise_to_target(noise_model, n, target, lr=0.001):\n    dls = get_dls(n)\n    img_opt_cb = ImgOptCb(target)\n    cbs = [img_opt_cb, ProgressCB(plot=True)]\n    Learner(\n        noise_model,\n        dls,\n        F.mse_loss,\n        lr=lr,\n        cbs=cbs,\n        opt_func=torch.optim.Adam,\n    ).fit(1)\n    return noise_model.tensor_.clip(0, 1), img_opt_cb.intermediates\ndenoise, intermediates = optimize_noise_to_target(\n    TensorModel(tensor_=torch.rand_like(img)),\n    250,\n    img,\n    lr=1e-1,\n)\nshow_images([denoise, img])\nshow_images([i.clip(0, 1) for i in intermediates], figsize=(6, 6))\nThis isn‚Äôt interesting on its own, but starts to become interesting when we incorporate the pretrained feature extractors such as VGG16. This gives us a richer representation than the raw pixels that we can manipulate.\nHere is a classic article on the discriminative features that these models learn.\nVGG16 is similar to the pre-resnet model we implemented for FashionMNIST.\nvgg16 = timm.create_model(\"vgg16\", pretrained=True).to(def_device)\nvgg16\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (pre_logits): ConvMlp(\n    (fc1): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n    (act1): ReLU(inplace=True)\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n    (act2): ReLU(inplace=True)\n  )\n  (head): ClassifierHead(\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc): Linear(in_features=4096, out_features=1000, bias=True)\n    (flatten): Identity()\n  )\n)\nTo use VGG16, we need to normalize for each color channel with the same constants that the model was trained with.\ndef normalize_imagenet(img):\n    i = img.clone()\n    i -= imagenet_mean[:, None, None]\n    i /= imagenet_std[:, None, None]\n    return i\nimg.shape\n\ntorch.Size([3, 256, 256])\nnimg = normalize_imagenet(img)\nshow_image(nimg.clip(0, 1))\nWe can also use the PyTorch version\nshow_image(pt_normalize_imagenet(img).clip(0, 1));\nSo, how do we use this normalized image to extract their features?\nsource",
    "crumbs": [
      "Neural Style Transfer"
    ]
  },
  {
    "objectID": "style_transfer.html#gram-loss",
    "href": "style_transfer.html#gram-loss",
    "title": "Neural Style Transfer",
    "section": "Gram Loss",
    "text": "Gram Loss\nWe don‚Äôt want to just transfer the spatial feature maps. We want something more abstract: style.\nThis is where the Gram matrix comes in. The gram matrix uses an intermediate represention, \\(X\\), where the magnitude of a feature basis is the row and the pixel ID in the column. By taking \\(X \\cdot X^T\\), we get the covariance matrix of the feature activations, which we call the Gram matrix.\n\nPaper here.\n\nsource\n\ncalculate_grams_vgg16\n\n calculate_grams_vgg16 (vgg16, imgs, target_layers=(18, 25))\n\n\nsource\n\n\nGramLoss\n\n GramLoss (target_img, target_layers=(18, 25), vgg=VGG(   (features):\n           Sequential(     (0): Conv2d(3, 64, kernel_size=(3, 3),\n           stride=(1, 1), padding=(1, 1))     (1): ReLU(inplace=True)\n           (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1),\n           padding=(1, 1))     (3): ReLU(inplace=True)     (4):\n           MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,\n           ceil_mode=False)     (5): Conv2d(64, 128, kernel_size=(3, 3),\n           stride=(1, 1), padding=(1, 1))     (6): ReLU(inplace=True)\n           (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1),\n           padding=(1, 1))     (8): ReLU(inplace=True)     (9):\n           MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,\n           ceil_mode=False)     (10): Conv2d(128, 256, kernel_size=(3, 3),\n           stride=(1, 1), padding=(1, 1))     (11): ReLU(inplace=True)\n           (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1),\n           padding=(1, 1))     (13): ReLU(inplace=True)     (14):\n           Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,\n           1))     (15): ReLU(inplace=True)     (16):\n           MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,\n           ceil_mode=False)     (17): Conv2d(256, 512, kernel_size=(3, 3),\n           stride=(1, 1), padding=(1, 1))     (18): ReLU(inplace=True)\n           (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1),\n           padding=(1, 1))     (20): ReLU(inplace=True)     (21):\n           Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\n           1))     (22): ReLU(inplace=True)     (23):\n           MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,\n           ceil_mode=False)     (24): Conv2d(512, 512, kernel_size=(3, 3),\n           stride=(1, 1), padding=(1, 1))     (25): ReLU(inplace=True)\n           (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1),\n           padding=(1, 1))     (27): ReLU(inplace=True)     (28):\n           Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,\n           1))     (29): ReLU(inplace=True)     (30):\n           MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1,\n           ceil_mode=False)   )   (pre_logits): ConvMlp(     (fc1):\n           Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n           (act1): ReLU(inplace=True)     (drop): Dropout(p=0.0,\n           inplace=False)     (fc2): Conv2d(4096, 4096, kernel_size=(1,\n           1), stride=(1, 1))     (act2): ReLU(inplace=True)   )   (head):\n           ClassifierHead(     (global_pool):\n           SelectAdaptivePool2d(pool_type=avg,\n           flatten=Flatten(start_dim=1, end_dim=-1))     (drop):\n           Dropout(p=0.0, inplace=False)     (fc):\n           Linear(in_features=4096, out_features=1000, bias=True)\n           (flatten): Identity()   ) ))\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_img\n\n\n\n\n\ntarget_layers\ntuple\n(18, 25)\nout of 30\n\n\nvgg\nVGG\nVGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace=True) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace=True) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace=True) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace=True) (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (18): ReLU(inplace=True) (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace=True) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace=True) (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (25): ReLU(inplace=True) (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (27): ReLU(inplace=True) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace=True) (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (pre_logits): ConvMlp( (fc1): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1)) (act1): ReLU(inplace=True) (drop): Dropout(p=0.0, inplace=False) (fc2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1)) (act2): ReLU(inplace=True) ) (head): ClassifierHead( (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1)) (drop): Dropout(p=0.0, inplace=False) (fc): Linear(in_features=4096, out_features=1000, bias=True) (flatten): Identity() ))\n\n\n\n\n\ndef optimize_gram_to_target(\n    noise_model,\n    n,\n    target,\n    lr=0.001,\n    epochs=5,\n    target_layers=(18, 25),\n):\n    intermediates = []\n    for _ in range(epochs):\n        TrainLearner(\n            noise_model,\n            get_dls(n // epochs),\n            GramLoss(target, target_layers=target_layers),\n            lr=lr,\n            opt_func=torch.optim.Adam,\n        ).fit(1)\n        i = noise_model.tensor_.clone().detach().clip(0, 1)\n        intermediates.append(i)\n    return intermediates\n\n\nspider_web = \"https://insideecology.com/wp-content/uploads/2018/06/spider-web-with-water-beads-921039_1280-810x540.jpg\"\nshow_image(download_image(spider_web))\n\n\n\n\n\n\n\n\n\ndef style_transfer(from_, to_):\n    if isinstance(from_, str):\n        from_ = pt_normalize_imagenet(download_image(from_))\n    if isinstance(to_, str):\n        to_ = pt_normalize_imagenet(download_image(to_))\n    nm = TensorModel(tensor_=from_)\n    return optimize_gram_to_target(nm, 2000, to_, 0.1, 9, target_layers=(2, 18, 25))\n\n\nshow_images(style_transfer(nimg, spider_web), figsize=(6, 6))\n\n\n\n\n\n\n\n\n\nstarry_night = \"https://sanctuarymentalhealth.org/wp-content/uploads/2021/03/The-Starry-Night-1200x630-1-979x514.jpg\"\nshow_image(download_image(starry_night))\nshow_images(style_transfer(nimg, starry_night), figsize=(6, 6))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the features from pretrained networks can be powerful.",
    "crumbs": [
      "Neural Style Transfer"
    ]
  },
  {
    "objectID": "noise_schedules.html",
    "href": "noise_schedules.html",
    "title": "Noise schedules",
    "section": "",
    "text": "Adapted from\n\nhttps://youtu.be/PXiD7ZjOKhA?si=iJMR2Tg050CemrxU&t=4314\n\n\nimport math\n\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom slowai.ddpm import DDPM, fashion_unet, get_dls\nfrom slowai.utils import show_images\n\n\nplt.style.use(\"ggplot\")\n\nNormally, we have a noise schedule like this:\n\ndef diff(x, dt):\n    return (x[1:] - x[:-1]) / dt\n\n\nnsteps = 1000\ndt = (0.02 - 0.0001) / nsteps\nbeta = torch.linspace(0.0001, 0.02, nsteps)\nalpha = 1 - beta\n·æ± = alpha.cumprod(dim=0)\nd·æ±_dt = diff(·æ±, dt)\nfig, (a0, a1) = plt.subplots(1, 2, figsize=(4.5, 2.5))\na0.set(title=r\"$\\bar{\\alpha}$\")\na0.plot(·æ±)\na1.set(title=r\"$\\frac{d\\bar{\\alpha}}{dt}$\")\na1.plot(d·æ±_dt)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThere are a number of steps in this process where the change in noise is almost nothing. Compare this to a cosine schedule.\n\ndef ·æ±_cos(t, T):\n    return (((t / T) * math.pi / 2).cos() ** 2).clamp(0.0, 0.999)\n\n\nfig, (a0, a1) = plt.subplots(1, 2, figsize=(6.5, 3.5))\na0.set(title=r\"$\\bar{\\alpha}$\")\na1.set(title=r\"$\\frac{d\\bar{\\alpha}}{dt}$\")\n\n# Linear\na0.plot(·æ±)\na1.plot(d·æ±_dt, label=\"linear\")\n\n# Cosine\nx = ·æ±_cos(torch.linspace(0, nsteps - 1, nsteps), nsteps)\na0.plot(x)\na1.plot(diff(x, dt), label=\"cos\")\n\nfig.legend(loc=\"center right\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThis is a more consistent noise scheduler, especially when considering the slope. Notice that tweaking beta_max actually makes the curvate more cosinusoidal.\n\nnsteps = 1000\ndt = (0.01 - 0.0001) / nsteps\nbeta = torch.linspace(\n    0.0001,\n    0.01,  # üëà\n    nsteps,\n)\nalpha = 1 - beta\n·æ± = alpha.cumprod(dim=0)\n\nfig, (a0, a1) = plt.subplots(1, 2, figsize=(6.5, 3.5))\na0.set(title=r\"$\\bar{\\alpha}$\")\na1.set(title=r\"$\\frac{d\\bar{\\alpha}}{dt}$\")\n\n# Linear\na0.plot(·æ±)\na1.plot(d·æ±_dt, label=\"linear\")\n\n# Cosine\nx = ·æ±_cos(torch.linspace(0, nsteps - 1, nsteps), nsteps)\na0.plot(x)\na1.plot(diff(x, dt), label=\"cos\")\n\nfig.legend(loc=\"center right\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nEven if we adopt a linear schedule, we‚Äôll clearly want to use a lower value of beta_max to spread out the noise addition over time. This was noticed by Robin Rombach, the author of Stable Diffusion, who noted that lower values of \\(\\beta_{max}\\) improved sampling.\nIf we were to train using this, we note that there are more examples that are clear.\n\ndef noisify(x0, ·æ±, n_steps=100):\n    device = x0.device\n    n = len(x0)\n    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n    Œµ = torch.randn(x0.shape, device=device)\n    ·æ±_t = ·æ±[t].reshape(-1, 1, 1, 1).to(device)\n    xt = ·æ±_t.sqrt() * x0 + (1 - ·æ±_t).sqrt() * Œµ\n    return (xt, t.to(device)), Œµ\n\n\ndls = get_dls()\nxb, _ = dls.peek()\n(out, _), _ = noisify(xb, x)\nout.shape\n\ntorch.Size([128, 1, 32, 32])\n\n\n\nshow_images(out[:16])",
    "crumbs": [
      "Noise schedules"
    ]
  },
  {
    "objectID": "augmentation_with_normalization.html",
    "href": "augmentation_with_normalization.html",
    "title": "Augmentation with fixed normality",
    "section": "",
    "text": "This dataloader code is copied from 13_ddpm.\n\npipe = [\n    transforms.Resize((32, 32)),\n    transforms.PILToTensor(),\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Lambda(lambda x: x - 0.5),\n]\n\n\ndef get_dls(bs=128):\n    return tensorize_images(\n        DataLoaders.from_hf(\"fashion_mnist\", bs=bs),\n        pipe=pipe,\n        normalize=False,\n    ).listify()\n\n\ndls = get_dls()\n\n\nxb, _ = dls.peek()\nxb.min(), xb.max()\n\n(tensor(-0.5000), tensor(0.5000))\n\n\n\nmz = ResNetWithGlobalPoolingInitialConv.kaiming(nfs=[32, 64, 128, 256, 512, 512])\nsummarize(mz, [*mz.layers, mz.lin, mz.norm])\ntrain(mz, dls=dls, n_epochs=10)\n\n\n\n\n\n\n\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nConv\n(8, 1, 28, 28)\n(8, 32, 28, 28)\n864\n0.6\n\n\nResidualConvBlock\n(8, 32, 28, 28)\n(8, 64, 14, 14)\n57,664\n11.2\n\n\nResidualConvBlock\n(8, 64, 14, 14)\n(8, 128, 7, 7)\n230,016\n11.2\n\n\nResidualConvBlock\n(8, 128, 7, 7)\n(8, 256, 4, 4)\n918,784\n14.7\n\n\nResidualConvBlock\n(8, 256, 4, 4)\n(8, 512, 2, 2)\n3,672,576\n14.7\n\n\nResidualConvBlock\n(8, 512, 2, 2)\n(8, 512, 1, 1)\n4,983,296\n5.0\n\n\nLinear\n(8, 512)\n(8, 10)\n5,130\n0.0\n\n\nBatchNorm1d\n(8, 10)\n(8, 10)\n20\n0.0\n\n\nTotal\n\n\n9,868,350\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.866\n0.557\n0\ntrain\n\n\n0.859\n0.429\n0\neval\n\n\n0.891\n0.352\n1\ntrain\n\n\n0.872\n0.352\n1\neval\n\n\n0.903\n0.282\n2\ntrain\n\n\n0.887\n0.313\n2\neval\n\n\n0.917\n0.238\n3\ntrain\n\n\n0.894\n0.303\n3\neval\n\n\n0.928\n0.204\n4\ntrain\n\n\n0.891\n0.321\n4\neval\n\n\n0.939\n0.173\n5\ntrain\n\n\n0.911\n0.263\n5\neval\n\n\n0.950\n0.138\n6\ntrain\n\n\n0.917\n0.244\n6\neval\n\n\n0.966\n0.095\n7\ntrain\n\n\n0.928\n0.239\n7\neval\n\n\n0.982\n0.054\n8\ntrain\n\n\n0.932\n0.228\n8\neval\n\n\n0.994\n0.026\n9\ntrain\n\n\n0.934\n0.231\n9\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Save this for later\ntorch.save(mz, \"../models/fashion_mnist_classifier.pt\")",
    "crumbs": [
      "Augmentation with fixed normality"
    ]
  },
  {
    "objectID": "convs.html",
    "href": "convs.html",
    "title": "Convolutions",
    "section": "",
    "text": "Adapted from:\ndsd = load_dataset(\"mnist\")\ntrn, tst = dsd[\"train\"], dsd[\"test\"]\nxb = T.to_tensor(trn[0][\"image\"])[None, ...]\nxb.shape\n\ntorch.Size([1, 1, 28, 28])\nThis is a nice resource.\nConvolutions encode position equivariance like the probability that there is a bird at any particular location in an image. It‚Äôs implemented as a sliding matrix multiplication, followed by a sum like so:\nkernel = tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n\n\ndef apply_kernel(row, col, img, kernel):\n    # Normally, you would need to check if the kernel has\n    # an odd or even size\n    width, height = kernel.shape\n    receptive_field = img[\n        row - (height // 2) : row + (height // 2) + 1,\n        col - (width // 2) : col + (width // 2) + 1,\n    ]\n    return (receptive_field * kernel).sum()\n\n\n*_, w, h = xb.shape\nprocessed = [\n    [apply_kernel(i, j, xb.squeeze(), kernel) for j in range(1, w - 1)]\n    for i in range(1, h - 1)\n]\nprocessed = tensor(processed)\n\nshow_images(\n    [kernel, xb.squeeze(), processed],\n    titles=[\"Kernel\", \"Original\", \"Original + Kernel\"],\n    figsize=(8, 8),\n);\nYou can see it is very easy to think of interesting features of images that are just convolutional filters.\nOne clever way of implementing this is the im2col algorithm that represents a convolution as a matrix multiplication and take advantage of highly efficient algorithms for matrix multiplications. It does so by unrolling the input matrix matrix such that every receptive field is a contiguous block of values, with a correspondingly unrolled contiguous block of convolutional filter values. The values of the convolutional matrix can change, but they are shared.\nThis has a corresponding function in PyTorch.\nF.unfold?\n\n\nSignature:\nF.unfold(\n    input: torch.Tensor,\n    kernel_size: None,\n    dilation: None = 1,\n    padding: None = 0,\n    stride: None = 1,\n) -&gt; torch.Tensor\nDocstring:\nExtracts sliding local blocks from a batched input tensor.\n.. warning::\n    Currently, only 4-D input tensors (batched image-like tensors) are\n    supported.\n.. warning::\n    More than one element of the unfolded tensor may refer to a single\n    memory location. As a result, in-place operations (especially ones that\n    are vectorized) may result in incorrect behavior. If you need to write\n    to the tensor, please clone it first.\nSee :class:`torch.nn.Unfold` for details\nFile:      ~/micromamba/envs/slowai/lib/python3.11/site-packages/torch/nn/functional.py\nType:      function\n# Need to add a channel dimension and a batch dimension\nxb_unfolded = F.unfold(xb, kernel_size=(3, 3)).float()\nkernel_unfolded = kernel.view(-1).float()\n\n# im2col\nxb_processed = kernel_unfolded @ xb_unfolded\n\n# Reshape\nxb_processed = xb_processed.view(26, 26)\nshow_image(xb_processed);\nThis unfold trick is about the same efficiency as the built-in convolution layer.\nFor better performance, we can apply a bunch of convolutions simultaneously.\ndiag1_edge = tensor([[0, -1, 1], [-1, 1, 0], [1, 0, 0]]).float()\ndiag2_edge = tensor([[1, -1, 0], [0, 1, -1], [0, 0, 1]]).float()\nleft_edge = tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]).float()\n\nedge_kernels = torch.stack([left_edge, diag1_edge, diag2_edge])\nedge_kernels.shape\n\ntorch.Size([3, 3, 3])\nbatch_features = F.conv2d(xb, edge_kernels[:, None, ...])\nshow_images([batch_features[0, i] for i in range(3)])\nThe parameters to a convolutional layer are the stride and padding, which allow us to manipulate the size of the feature map.",
    "crumbs": [
      "Convolutions"
    ]
  },
  {
    "objectID": "convs.html#creating-a-cnn",
    "href": "convs.html#creating-a-cnn",
    "title": "Convolutions",
    "section": "Creating a CNN",
    "text": "Creating a CNN\nFor classifying digits, we cannot solely use convolutional layers because it gives 10 outputs per pixel!\n\nnh, n_outputs = 30, 10\n\npartial_model = torch.nn.Sequential(\n    torch.nn.Conv2d(1, 30, kernel_size=3, padding=1),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(30, 10, kernel_size=3, padding=1),\n)\n\npartial_model(xb).shape\n\ntorch.Size([1, 10, 28, 28])\n\n\nInstead, we‚Äôll add a lot of layers and chew up the extra feature map dimensions until we have a 1x1x10 feature map.\nWe‚Äôll add a convenience function to create conv layers with optional activations.\n\nsource\n\nconv\n\n conv (ni, nf, ks=3, stride=2, act=True)\n\nWe also need some annoying device stuff.\n\nsource\n\n\nto_device\n\n to_device (x, device='cpu')\n\nNow, define the model‚Ä¶\nNote here that the default stride is 2, such that the feature map is downsampled by 2 each layer.\n\nsource\n\n\nget_model\n\n get_model ()\n\n\nget_model()\n\nAnd we can train! üèãÔ∏è\n\nsource\n\n\nget_dls_from_dataset_dict\n\n get_dls_from_dataset_dict (dsd, collate_fn=&lt;function default_collate&gt;,\n                            bs=32)\n\n\nsource\n\n\nfit\n\n fit (epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False)\n\n\nsource\n\n\naccuracy\n\n accuracy (y, y_pred)\n\n\nsource\n\n\nfashion_mnist\n\n fashion_mnist (bs=256)\n\n\nsource\n\n\nfashion_collate\n\n fashion_collate (examples)\n\n\nmodel = get_model()\nwith fashion_mnist() as dls:\n    fit(6, model, F.cross_entropy, optim.SGD(model.parameters(), lr=0.1), *dls)\n\nepoch=0, validation loss=1.041, validation accuracy=0.62\nepoch=1, validation loss=0.767, validation accuracy=0.73\nepoch=2, validation loss=0.623, validation accuracy=0.78\nepoch=3, validation loss=0.561, validation accuracy=0.80\nepoch=4, validation loss=0.515, validation accuracy=0.81\nepoch=5, validation loss=0.483, validation accuracy=0.82\n\n\nThis gives us comparable accuracy to the linear model, which had 39,760 parameters. In contrast‚Ä¶\n\nsum(p.numel() for p in model.parameters())\n\n5274\n\n\nMore tips:\n\nChewing up the feature map until we have one logit per dimension is not the only way to summarize the features before a classification/regression head. We can also use a dense layer or global average pooling\nThe receptive field of a convolution grows through the network, as the convolutions are functions of convolutions. Really nice illustration of that here",
    "crumbs": [
      "Convolutions"
    ]
  },
  {
    "objectID": "super_resolution.html",
    "href": "super_resolution.html",
    "title": "Super-resolution",
    "section": "",
    "text": "Adapted from\nThe challenge of this module is to: (a) upscale a 32x32 image to a 64x64 image while (b) un-erasing randomly cropped portions of the training data.\nsource",
    "crumbs": [
      "Super-resolution"
    ]
  },
  {
    "objectID": "super_resolution.html#approach-1-autoencoder",
    "href": "super_resolution.html#approach-1-autoencoder",
    "title": "Super-resolution",
    "section": "Approach #1: Autoencoder",
    "text": "Approach #1: Autoencoder\nRecall that we had poor results in FashionMNIST, so temper your expectations.\n\nsource\n\nAutoEncoder\n\n AutoEncoder (nfs:list[int]=(32, 64, 128, 256, 512, 1024))\n\nProject into a hidden space and reproject into the original space\n\nlr_find(AutoEncoder.kaiming(), dls, gamma=1.1, start_lr=1e-5, loss_fn=F.mse_loss)\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n    \n      \n      80.61% [79/98 00:51&lt;00:12 1.099]\n    \n    \n\n\n\n\n\n\n\n\n\n\nae = train(AutoEncoder.kaiming(), dls)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.845\n0\ntrain\n\n\n0.558\n0\neval\n\n\n0.487\n1\ntrain\n\n\n0.432\n1\neval\n\n\n0.387\n2\ntrain\n\n\n0.354\n2\neval\n\n\n0.325\n3\ntrain\n\n\n0.325\n3\neval\n\n\n0.286\n4\ntrain\n\n\n0.270\n4\neval\n\n\n0.258\n5\ntrain\n\n\n0.240\n5\neval\n\n\n0.241\n6\ntrain\n\n\n0.236\n6\neval\n\n\n0.226\n7\ntrain\n\n\n0.228\n7\neval\n\n\n0.216\n8\ntrain\n\n\n0.240\n8\neval\n\n\n0.206\n9\ntrain\n\n\n0.201\n9\neval\n\n\n0.197\n10\ntrain\n\n\n0.196\n10\neval\n\n\n0.190\n11\ntrain\n\n\n0.186\n11\neval\n\n\n0.183\n12\ntrain\n\n\n0.184\n12\neval\n\n\n0.177\n13\ntrain\n\n\n0.171\n13\neval\n\n\n0.172\n14\ntrain\n\n\n0.170\n14\neval\n\n\n0.168\n15\ntrain\n\n\n0.168\n15\neval\n\n\n0.164\n16\ntrain\n\n\n0.162\n16\neval\n\n\n0.161\n17\ntrain\n\n\n0.155\n17\neval\n\n\n0.160\n18\ntrain\n\n\n0.154\n18\neval\n\n\n0.156\n19\ntrain\n\n\n0.148\n19\neval\n\n\n0.154\n20\ntrain\n\n\n0.148\n20\neval\n\n\n0.153\n21\ntrain\n\n\n0.146\n21\neval\n\n\n0.152\n22\ntrain\n\n\n0.144\n22\neval\n\n\n0.152\n23\ntrain\n\n\n0.143\n23\neval\n\n\n0.151\n24\ntrain\n\n\n0.142\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    yp = ae(xb.to(def_device)).cpu()\n\n\nviz(xb, yb, yp)\n\n\n\n\n\n\n\n\nNot much better than the original.",
    "crumbs": [
      "Super-resolution"
    ]
  },
  {
    "objectID": "super_resolution.html#approach-2-u-net",
    "href": "super_resolution.html#approach-2-u-net",
    "title": "Super-resolution",
    "section": "Approach #2: U-net",
    "text": "Approach #2: U-net\nThis is nearly identical except that, for a particular up-sampling block, the logits from the same scale down-sampling are added or concatenated.\n\nsource\n\nTinyUnet\n\n TinyUnet (nfs:list[int]=(32, 64, 128, 256, 512, 1024), n_blocks=(3, 2, 2,\n           1, 1))\n\nU-net\n\nlr_find(TinyUnet.kaiming(), dls, gamma=1.1, start_lr=1e-5, loss_fn=F.mse_loss)\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n    \n      \n      58.16% [57/98 00:38&lt;00:27 1.355]\n    \n    \n\n\n\n\n\n\n\n\n\n\nun = train(TinyUnet.kaiming(), dls)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.265\n0\ntrain\n\n\n0.377\n0\neval\n\n\n0.249\n1\ntrain\n\n\n0.196\n1\neval\n\n\n0.173\n2\ntrain\n\n\n0.149\n2\neval\n\n\n0.142\n3\ntrain\n\n\n0.124\n3\neval\n\n\n0.128\n4\ntrain\n\n\n0.119\n4\neval\n\n\n0.120\n5\ntrain\n\n\n0.107\n5\neval\n\n\n0.115\n6\ntrain\n\n\n0.105\n6\neval\n\n\n0.110\n7\ntrain\n\n\n0.099\n7\neval\n\n\n0.107\n8\ntrain\n\n\n0.098\n8\neval\n\n\n0.105\n9\ntrain\n\n\n0.097\n9\neval\n\n\n0.103\n10\ntrain\n\n\n0.095\n10\neval\n\n\n0.102\n11\ntrain\n\n\n0.094\n11\neval\n\n\n0.101\n12\ntrain\n\n\n0.093\n12\neval\n\n\n0.099\n13\ntrain\n\n\n0.092\n13\neval\n\n\n0.099\n14\ntrain\n\n\n0.092\n14\neval\n\n\n0.098\n15\ntrain\n\n\n0.091\n15\neval\n\n\n0.097\n16\ntrain\n\n\n0.091\n16\neval\n\n\n0.097\n17\ntrain\n\n\n0.090\n17\neval\n\n\n0.096\n18\ntrain\n\n\n0.090\n18\neval\n\n\n0.096\n19\ntrain\n\n\n0.090\n19\neval\n\n\n0.096\n20\ntrain\n\n\n0.090\n20\neval\n\n\n0.096\n21\ntrain\n\n\n0.089\n21\neval\n\n\n0.096\n22\ntrain\n\n\n0.089\n22\neval\n\n\n0.096\n23\ntrain\n\n\n0.089\n23\neval\n\n\n0.096\n24\ntrain\n\n\n0.089\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    yp = un(xb.to(def_device)).cpu()\n\n\nviz(xb, yb, yp)\n\n\n\n\n\n\n\n\n\ndel un",
    "crumbs": [
      "Super-resolution"
    ]
  },
  {
    "objectID": "super_resolution.html#approach-3-u-net-with-perceptual-loss",
    "href": "super_resolution.html#approach-3-u-net-with-perceptual-loss",
    "title": "Super-resolution",
    "section": "Approach #3: U-net with perceptual loss",
    "text": "Approach #3: U-net with perceptual loss\nFirst, let‚Äôs train a model with an identical downsampling circuit and a classification head\n\nsource\n\nTinyImageResNet4\n\n TinyImageResNet4 (nfs:list[int]=(32, 64, 128, 256, 512, 1024))\n\nConvolutional classification model\n\ndls_clf = DataLoaders.from_dsd(dls.splits).listify([\"image_high_rez\", \"idx\"])\ndls_clf.bs = 1024\n\n\nm = MetricsCB(MulticlassAccuracy(num_classes=200))\nclf = train(\n    TinyImageResNet4.kaiming(),\n    dls_clf,\n    loss_fn=F.cross_entropy,\n    lr=1e-2,\n    n_epochs=25,\n    extra_cbs=[m],\n)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.064\n4.714\n0\ntrain\n\n\n0.133\n4.083\n0\neval\n\n\n0.138\n4.059\n1\ntrain\n\n\n0.200\n3.607\n1\neval\n\n\n0.197\n3.648\n2\ntrain\n\n\n0.223\n3.540\n2\neval\n\n\n0.243\n3.372\n3\ntrain\n\n\n0.289\n3.102\n3\neval\n\n\n0.284\n3.136\n4\ntrain\n\n\n0.292\n3.084\n4\neval\n\n\n0.316\n2.957\n5\ntrain\n\n\n0.353\n2.740\n5\neval\n\n\n0.349\n2.783\n6\ntrain\n\n\n0.360\n2.755\n6\neval\n\n\n0.379\n2.628\n7\ntrain\n\n\n0.391\n2.617\n7\neval\n\n\n0.404\n2.498\n8\ntrain\n\n\n0.376\n2.737\n8\neval\n\n\n0.434\n2.352\n9\ntrain\n\n\n0.407\n2.626\n9\neval\n\n\n0.457\n2.234\n10\ntrain\n\n\n0.419\n2.598\n10\neval\n\n\n0.483\n2.110\n11\ntrain\n\n\n0.445\n2.428\n11\neval\n\n\n0.509\n1.987\n12\ntrain\n\n\n0.455\n2.427\n12\neval\n\n\n0.535\n1.871\n13\ntrain\n\n\n0.473\n2.370\n13\neval\n\n\n0.564\n1.730\n14\ntrain\n\n\n0.476\n2.352\n14\neval\n\n\n0.592\n1.612\n15\ntrain\n\n\n0.505\n2.244\n15\neval\n\n\n0.624\n1.475\n16\ntrain\n\n\n0.496\n2.339\n16\neval\n\n\n0.656\n1.355\n17\ntrain\n\n\n0.513\n2.276\n17\neval\n\n\n0.685\n1.230\n18\ntrain\n\n\n0.535\n2.194\n18\neval\n\n\n0.712\n1.123\n19\ntrain\n\n\n0.537\n2.167\n19\neval\n\n\n0.737\n1.038\n20\ntrain\n\n\n0.551\n2.166\n20\neval\n\n\n0.756\n0.955\n21\ntrain\n\n\n0.555\n2.140\n21\neval\n\n\n0.773\n0.897\n22\ntrain\n\n\n0.560\n2.123\n22\neval\n\n\n0.783\n0.861\n23\ntrain\n\n\n0.563\n2.114\n23\neval\n\n\n0.788\n0.837\n24\ntrain\n\n\n0.564\n2.115\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.save(clf, \"../models/tiny-image-resnet-4.pt\")\nclf = torch.load(\"../models/tiny-image-resnet-4.pt\")\n\n\n@torch.no_grad()\ndef get_clf_features(x):\n    x = xb.cuda()\n    for l in clf.conv_layers[:4]:\n        x = l(x)\n    x = torch.flatten(x)\n    return x\n\nNow, we can train using these ‚Äúperceptual‚Äù features\n\ndef custom_loss(y, y_pred):\n    \"\"\"Compositive MSE and perceputal loss function\"\"\"\n    perceptual_loss = F.mse_loss(\n        get_clf_features(y),\n        get_clf_features(y_pred),\n    )\n    return F.mse_loss(y, y_pred) + (0.1 * perceptual_loss)\n\nNow, we can train with this loss function\n\nunp = train(TinyUnet.kaiming(), dls, loss_fn=custom_loss)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n2.163\n0\ntrain\n\n\n0.460\n0\neval\n\n\n0.315\n1\ntrain\n\n\n0.238\n1\neval\n\n\n0.204\n2\ntrain\n\n\n0.169\n2\neval\n\n\n0.162\n3\ntrain\n\n\n0.139\n3\neval\n\n\n0.141\n4\ntrain\n\n\n0.123\n4\neval\n\n\n0.128\n5\ntrain\n\n\n0.115\n5\neval\n\n\n0.119\n6\ntrain\n\n\n0.107\n6\neval\n\n\n0.115\n7\ntrain\n\n\n0.104\n7\neval\n\n\n0.111\n8\ntrain\n\n\n0.102\n8\neval\n\n\n0.108\n9\ntrain\n\n\n0.100\n9\neval\n\n\n0.106\n10\ntrain\n\n\n0.098\n10\neval\n\n\n0.105\n11\ntrain\n\n\n0.096\n11\neval\n\n\n0.103\n12\ntrain\n\n\n0.096\n12\neval\n\n\n0.102\n13\ntrain\n\n\n0.094\n13\neval\n\n\n0.101\n14\ntrain\n\n\n0.094\n14\neval\n\n\n0.101\n15\ntrain\n\n\n0.093\n15\neval\n\n\n0.100\n16\ntrain\n\n\n0.092\n16\neval\n\n\n0.099\n17\ntrain\n\n\n0.092\n17\neval\n\n\n0.099\n18\ntrain\n\n\n0.092\n18\neval\n\n\n0.098\n19\ntrain\n\n\n0.091\n19\neval\n\n\n0.098\n20\ntrain\n\n\n0.091\n20\neval\n\n\n0.098\n21\ntrain\n\n\n0.091\n21\neval\n\n\n0.098\n22\ntrain\n\n\n0.091\n22\neval\n\n\n0.098\n23\ntrain\n\n\n0.091\n23\neval\n\n\n0.098\n24\ntrain\n\n\n0.091\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    yp = unp(xb.to(def_device)).cpu()\n\n\nviz(xb, yb, yp)\n\n\n\n\n\n\n\n\n\ntorch.save(unp, \"../models/tiny-image-resnet-un-p.pt\")\nunp = torch.load(\"../models/tiny-image-resnet-un-p.pt\")\n\nAnd, finally, train with the same features from the classifier in the downsampler path\n\nsource\n\n\ninitialize_unet_weights_with_clf_weights\n\n initialize_unet_weights_with_clf_weights (c, unet)\n\n\nunpp = train(\n    initialize_unet_weights_with_clf_weights(clf, TinyUnet.kaiming()),\n    dls,\n    loss_fn=custom_loss,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.419\n0\ntrain\n\n\n0.420\n0\neval\n\n\n0.266\n1\ntrain\n\n\n0.183\n1\neval\n\n\n0.159\n2\ntrain\n\n\n0.130\n2\neval\n\n\n0.130\n3\ntrain\n\n\n0.110\n3\neval\n\n\n0.146\n4\ntrain\n\n\n0.117\n4\neval\n\n\n0.110\n5\ntrain\n\n\n0.097\n5\neval\n\n\n0.103\n6\ntrain\n\n\n0.093\n6\neval\n\n\n0.100\n7\ntrain\n\n\n0.091\n7\neval\n\n\n0.098\n8\ntrain\n\n\n0.090\n8\neval\n\n\n0.096\n9\ntrain\n\n\n0.089\n9\neval\n\n\n0.096\n10\ntrain\n\n\n0.088\n10\neval\n\n\n0.095\n11\ntrain\n\n\n0.088\n11\neval\n\n\n0.094\n12\ntrain\n\n\n0.087\n12\neval\n\n\n0.093\n13\ntrain\n\n\n0.087\n13\neval\n\n\n0.093\n14\ntrain\n\n\n0.086\n14\neval\n\n\n0.093\n15\ntrain\n\n\n0.086\n15\neval\n\n\n0.092\n16\ntrain\n\n\n0.086\n16\neval\n\n\n0.092\n17\ntrain\n\n\n0.086\n17\neval\n\n\n0.092\n18\ntrain\n\n\n0.086\n18\neval\n\n\n0.092\n19\ntrain\n\n\n0.085\n19\neval\n\n\n0.091\n20\ntrain\n\n\n0.085\n20\neval\n\n\n0.091\n21\ntrain\n\n\n0.085\n21\neval\n\n\n0.091\n22\ntrain\n\n\n0.085\n22\neval\n\n\n0.091\n23\ntrain\n\n\n0.085\n23\neval\n\n\n0.091\n24\ntrain\n\n\n0.085\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    yp = unpp(xb.to(def_device)).cpu()\n\n\nviz(xb, yb, yp)\n\n\n\n\n\n\n\n\n\ndel unpp",
    "crumbs": [
      "Super-resolution"
    ]
  },
  {
    "objectID": "super_resolution.html#approach-4-u-net-with-residual-link-processing-cross-convolutions",
    "href": "super_resolution.html#approach-4-u-net-with-residual-link-processing-cross-convolutions",
    "title": "Super-resolution",
    "section": "Approach #4: U-net with residual link processing (cross-convolutions)",
    "text": "Approach #4: U-net with residual link processing (cross-convolutions)\n\nsource\n\nTinyUnetWithCrossConvolutions\n\n TinyUnetWithCrossConvolutions (nfs:list[int]=(32, 64, 128, 256, 512,\n                                1024), n_blocks=(3, 2, 2, 1, 1))\n\nU-net\n\ndls2 = DataLoaders.from_dsd(dls.splits, bs=512).listify(\n    [\"image_low_rez\", \"image_high_rez\"]\n)\n\n\nunc = train(TinyUnetWithCrossConvolutions.kaiming(), dls2, loss_fn=F.mse_loss)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n2.813\n0\ntrain\n\n\n0.484\n0\neval\n\n\n0.325\n1\ntrain\n\n\n0.239\n1\neval\n\n\n0.201\n2\ntrain\n\n\n0.177\n2\neval\n\n\n0.156\n3\ntrain\n\n\n0.130\n3\neval\n\n\n0.134\n4\ntrain\n\n\n0.116\n4\neval\n\n\n0.122\n5\ntrain\n\n\n0.109\n5\neval\n\n\n0.115\n6\ntrain\n\n\n0.103\n6\neval\n\n\n0.109\n7\ntrain\n\n\n0.098\n7\neval\n\n\n0.105\n8\ntrain\n\n\n0.095\n8\neval\n\n\n0.102\n9\ntrain\n\n\n0.093\n9\neval\n\n\n0.100\n10\ntrain\n\n\n0.092\n10\neval\n\n\n0.098\n11\ntrain\n\n\n0.091\n11\neval\n\n\n0.097\n12\ntrain\n\n\n0.090\n12\neval\n\n\n0.096\n13\ntrain\n\n\n0.089\n13\neval\n\n\n0.095\n14\ntrain\n\n\n0.088\n14\neval\n\n\n0.094\n15\ntrain\n\n\n0.088\n15\neval\n\n\n0.093\n16\ntrain\n\n\n0.087\n16\neval\n\n\n0.093\n17\ntrain\n\n\n0.087\n17\neval\n\n\n0.093\n18\ntrain\n\n\n0.086\n18\neval\n\n\n0.093\n19\ntrain\n\n\n0.086\n19\neval\n\n\n0.092\n20\ntrain\n\n\n0.086\n20\neval\n\n\n0.092\n21\ntrain\n\n\n0.086\n21\neval\n\n\n0.092\n22\ntrain\n\n\n0.086\n22\neval\n\n\n0.092\n23\ntrain\n\n\n0.086\n23\neval\n\n\n0.092\n24\ntrain\n\n\n0.086\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCompare to 0.089 for the comparable U-net with MSE loss.\n\nwith torch.no_grad():\n    yp = unc(xb.to(def_device)).cpu()\n\n\nviz(xb, yb, yp)",
    "crumbs": [
      "Super-resolution"
    ]
  },
  {
    "objectID": "fid.html",
    "href": "fid.html",
    "title": "Fr√©chet inception distance",
    "section": "",
    "text": "Adapted from\n\nhttps://youtu.be/PXiD7ZjOKhA?si=-JY6IiV4tvGWZvr9&t=1379\n\nNotes debugging things here\n\nplt.style.use(\"ggplot\")\n\nWe want to compute how closely our generated images match the training distribution.\n\nmodel_fp = Path(\"../models/fashion_unet.pt\")\nunet = torch.load(model_fp)\nddpm = DDPM()\n\n\nddpm.sample?\n\n\nSignature: ddpm.sample(model, sz=(16, 1, 32, 32), device='cuda', return_all=False)\nDocstring: &lt;no docstring&gt;\nFile:      ~/Desktop/SlowAI/nbs/slowai/ddpm.py\nType:      method\n\n\n\n\n_28x28 = T.Resize((28, 28), antialias=True)\n\n\n*earlier, out = ddpm.sample(unet, sz=(BS, 1, 32, 32), return_all=True)\nout = _28x28(out)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:31&lt;00:00, 32.22time step/s]\n\n\n\nout.shape\n\ntorch.Size([256, 1, 28, 28])\n\n\n\nout.min(), out.max()\n\n(tensor(-0.5499, device='cuda:0'), tensor(0.5491, device='cuda:0'))\n\n\n\nclf_resnet: ResNetWithGlobalPoolingInitialConv = torch.load(\n    \"../models/fashion_mnist_classifier.pt\"\n)\n\n\nfashion_categories = {\n    0: \"T-shirt/top\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle boot\",\n}\n\n\npreds = clf_resnet(out).argmax(axis=1)\npreds = [fashion_categories[pred.cpu().item()] for pred in preds]\n\n\nshow_images(out[:8, ...], titles=preds[:8])\n\n\n\n\n\n\n\n\nTo do so, we‚Äôll use a metric called ‚ÄúFr√©chet inception distance‚Äù that considers the statistics of the activations. This shall be the global average pool layer of a convolutional classifier. This does not give us the similarity between individual samples; rather, it considers the summary statistics of a batch of samples.\nJeremy really strains to use the Learner to capture the activations, but its much easier to just implement a hook.\n\nsource\n\nget_fid_logits\n\n get_fid_logits (model, xb, layer='pool')\n\n\nsource\n\n\nHook\n\n Hook (h)\n\n\nfake_feats = get_fid_logits(clf_resnet, out)\nfake_feats.shape\n\ntorch.Size([256, 512])\n\n\n\nplt.hist(fake_feats.reshape(-1).cpu(), bins=100);\n\n\n\n\n\n\n\n\nWe can visualize the feature density in a few dimensions.\n\nX = PCA(n_components=4).fit_transform(fake_feats.cpu())\nsns.kdeplot(X, legend=False);\n\n\n\n\n\n\n\n\nHowever, what we really want to capture is the co-variance of one feature with another to be able to ‚Äúsummarize‚Äù a batch.\n\nsns.pairplot(pd.DataFrame(X), kind=\"kde\", corner=True)\n\n\n\n\n\n\n\n\nThis figure starts to show how the model makes it simple to draw hyperplanes to make classification decisions. We can also use this where the ‚Äúreal‚Äù images have a certain covariance and the ‚Äúfake‚Äù images have a different covariance.\n\ndls = get_dls(BS)\n\n\nxb, _ = dls.peek()\nxb = xb.to(def_device)\nreal_feats = get_fid_logits(clf_resnet, xb)\nreal_feats.shape\n\ntorch.Size([256, 512])\n\n\n\nsource\n\n\nsummarize\n\n summarize (X)\n\n\n(m0, c0), (m1, c1) = summarize(fake_feats), summarize(real_feats)\nm0.shape, m1.shape, c0.shape, c1.shape\n\n(torch.Size([512]),\n torch.Size([512]),\n torch.Size([512, 512]),\n torch.Size([512, 512]))\n\n\n\nsource\n\n\nfid\n\n fid (real, fake, bs=256)\n\n\nfid(summarize(real_feats), summarize(fake_feats))\n\n346.86865234375\n\n\nLet‚Äôs double check using Jeremy Howard‚Äôs implmementation\n\ndef _calc_stats(feats):\n    feats = feats.squeeze()\n    return feats.mean(0), feats.T.cov()\n\n\ndef _calc_fid(m1, c1, m2, c2, bs=BS):\n    m1, c1, m2, c2 = map(to_cpu, (m1, c1, m2, c2))\n    csr = tensor(linalg.sqrtm(c1 @ c2, bs).real)\n    return (((m1 - m2) ** 2).sum() + c1.trace() + c2.trace() - 2 * csr.trace()).item()\n\n\ndef fid_howard(feats1, feats2):\n    s1, s2 = _calc_stats(feats1), _calc_stats(feats2)\n    return _calc_fid(*s1, *s2)\n\n\nfid_howard(real_feats, fake_feats)\n\n346.86865234375\n\n\nGood. Note that we take the product of two matrices, so we take the matrix square root to keep the variance within a reasonable boundary. This is the Newton-Schultz method where you compute: \\[\n\\begin{align*}\na &= \\sqrt{x} \\\\\na^2 &= x \\\\\na^2 - x &= 0\n\\end{align*}\n\\] By taking the derivative, subtracting a certain amount from the original matrix and determining if the difference between the new matrix squared and the original matrix squared is less than a tolerance.\n\nlinalg.sqrtm?\n\n\nSignature: linalg.sqrtm(A, disp=True, blocksize=64)\nDocstring:\nMatrix square root.\nParameters\n----------\nA : (N, N) array_like\n    Matrix whose square root to evaluate\ndisp : bool, optional\n    Print warning if error in the result is estimated large\n    instead of returning estimated error. (Default: True)\nblocksize : integer, optional\n    If the blocksize is not degenerate with respect to the\n    size of the input array, then use a blocked algorithm. (Default: 64)\nReturns\n-------\nsqrtm : (N, N) ndarray\n    Value of the sqrt function at `A`. The dtype is float or complex.\n    The precision (data size) is determined based on the precision of\n    input `A`. When the dtype is float, the precision is the same as `A`.\n    When the dtype is complex, the precision is double that of `A`. The\n    precision might be clipped by each dtype precision range.\nerrest : float\n    (if disp == False)\n    Frobenius norm of the estimated error, ||err||_F / ||A||_F\nReferences\n----------\n.. [1] Edvin Deadman, Nicholas J. Higham, Rui Ralha (2013)\n       \"Blocked Schur Algorithms for Computing the Matrix Square Root,\n       Lecture Notes in Computer Science, 7782. pp. 171-182.\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.linalg import sqrtm\n&gt;&gt;&gt; a = np.array([[1.0, 3.0], [1.0, 4.0]])\n&gt;&gt;&gt; r = sqrtm(a)\n&gt;&gt;&gt; r\narray([[ 0.75592895,  1.13389342],\n       [ 0.37796447,  1.88982237]])\n&gt;&gt;&gt; r.dot(r)\narray([[ 1.,  3.],\n       [ 1.,  4.]])\nFile:      ~/micromamba/envs/slowai/lib/python3.11/site-packages/scipy/linalg/_matfuncs_sqrtm.py\nType:      function\n\n\n\nThis is called the ‚ÄúInception‚Äù distance because of the use of the Inception model. This allows us to compare metrics with other papers, but using a special-use classifier can be beneficial.\nFID can be biased:\n\nFID depends on the batch size, where smaller batches have systematically larger distances\nThe Inception model uses an image size of 299x299, which can cause artifacts if resizing images\n\nTo compare with other results from literature, make sure to keep the batch size consistent and make sure your images make a similar size.\nThe KID (Kernel Inception distance) is designed to mitigate these biases.\n\n\n\n\n\n\nWhat is the KID\n\n\n\nJeremy glosses over the mathematics here, so the following is just copied from the notebook. KID is not common used due to the high variance: it is quite dependant on the random seed. This underscores that there is no unbiased metric of image distribution similarity. Human evaluation is the gold standard.\n\n\n\nsource\n\n\nkid\n\n kid (x, y, maxs=50)\n\n\nkid(real_feats, fake_feats)\n\n1.2456791400909424\n\n\nLet‚Äôs make this a class.\n\nsource\n\n\nImageEval\n\n ImageEval (inception, x_example, layer='pool', validate=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nget_dls(32).peek()[0].min()\n\ntensor(-0.5000)\n\n\n\nimg_eval = ImageEval(clf_resnet, xb)\nimg_eval.kid(out), img_eval.fid(out)\n\n(1.2456791400909424, 346.9293212890625)\n\n\nFor comparison, we need to look at another real batch of data to see the ideal FID.\n\ndl = iter(dls[\"test\"])\n_ = next(dl)\nxb2, _ = next(dl)\nimg_eval.fid(xb2.cuda())\n\n95.071044921875\n\n\nNow, we can show that denoising improves the FID over the sampling trajectory.\n\nfids = []\nfor i, x in enumerate(tqdm([*earlier, out][::10])):\n    fids.append((i, img_eval.fid(x)))\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:13&lt;00:00,  7.66it/s]\n\n\n\nfig, ax = plt.subplots(1, 1)\nx, y = zip(*fids)\nax.plot(x, y)\nax.set_ylim(0, max(y) + 10)\n\n\n\n\n\n\n\n\n\nfids[-1]\n\n(99, 221.7041015625)\n\n\n\n\n\n\n\n\nWhat should the minimum and maximum values be for images?\n\n\n\nJeremy mentions here that he noticed a ‚Äúbug‚Äù where our dataset is normalized between 0 and 1, whereas everyone else normalizes between -1 and 1. However, when he implemented this ‚Äúfix‚Äù, performance was much worse. It was only when he normalized between -0.5 and 0.5 that the model performance improved compared to 0 and 1 normalization.\n\n\nLet‚Äôs try Jeremy‚Äôs experiments here.\nNote that the original validation accuracy was 91.7%.\n\ndel unet\nclean_mem()\n\n\ndef train_clf(model, dls, lr=1e-2, n_epochs=2):\n    T_max = len(dls[\"train\"]) * n_epochs\n    scheduler = BatchSchedulerCB(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=T_max)\n    acc = MetricsCB(MulticlassAccuracy(num_classes=10))\n    cbs = [\n        acc,\n        DeviceCB(),\n        ProgressCB(plot=False),\n        scheduler,\n    ]\n    learner = TrainLearner(\n        model,\n        dls,\n        F.cross_entropy,\n        lr=lr,\n        cbs=cbs,\n        opt_func=partial(torch.optim.AdamW, eps=1e-5),\n    )\n    learner.fit(n_epochs)\n    return acc.metrics[\"MulticlassAccuracy\"].compute()\n\n\npipe = [T.PILToTensor(), T.ConvertImageDtype(torch.float), T.Pad((2, 2))]\nto_tensor = T.Compose(pipe)\n\n\ndef get_norm_dls(a, b):\n    def norm(x):\n        return (to_tensor(x) - a) / b\n\n    return (\n        fashion_mnist(512)\n        .with_transforms({\"image\": batchify(norm)}, lazy=True)\n        .listify()\n    )\n\n\nxb, _ = get_norm_dls(0, 1).peek()\nxb.shape\n\ntorch.Size([512, 1, 32, 32])\n\n\n\ndef classification_accuracy_for_normalization(a, b, n_epochs=2):\n    dls = get_norm_dls(a, b)\n    batch, _ = dls.peek()\n    nfs = [\n        16,\n        32,\n        64,\n        128,\n        256,\n    ]\n    model = ResNetWithGlobalPooling.kaiming(nfs)\n    dls = get_norm_dls(a, b)\n    xb, _ = dls.peek()\n    range_ = (batch.min().item(), batch.max().item())\n    return range_, train_clf(model, dls=dls, n_epochs=n_epochs)\n\n\nres = []\nfor a, b in tqdm([(0, 1), (0.5, 0.5), (0.5, 1)]):\n    with io.capture_output():\n        (min_, max_), acc = classification_accuracy_for_normalization(a, b)\n    res.append((min_, max_, acc))\nres\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:32&lt;00:00, 10.77s/it]\n\n\n[(0.0, 1.0, tensor(0.9148)),\n (-1.0, 1.0, tensor(0.9136)),\n (-0.5, 0.5, tensor(0.9142))]\n\n\nThese are all within 0.1% of one another and, moreover, the range that he said was dramatically better was slight worse than the one he said was the worst (-1 to 1). I‚Äôm not sure if Jeremy is right here.\n\n\nComparison to Howard‚Äôs classifier\nMy FID‚Äôs are much higher than Jeremy‚Äôs, even when comparing one batch of real data to another. Why is this?\nPerhaps this is because Howard‚Äôs normalization was between -1 and 1, whereas I normalized between -0.5 and 0.5\n\ndls = get_dls(BS)\nxb_real_fisher_trn, _ = dls.peek(\"train\")\n\n\nxb_real_howard_trn = xb_real_fisher_trn * 2\nfig, ax = plt.subplots(figsize=(3, 3))\nax.hist(xb_real_fisher_trn.reshape(-1), label=\"Fisher\", alpha=0.5)\nax.hist(xb_real_howard_trn.reshape(-1), label=\"Howard\", alpha=0.5)\nfig.legend()\n\n\n\n\n\n\n\n\nLet‚Äôs compare logits\n\nclf_fisher = torch.load(\"../models/fashion_mnist_classifier.pt\")\nclf_howard = torch.load(\"../course22p2/nbs/models/data_aug2.pkl\")\n\n\nres = {}\nfor name, layer, clf, xb in [\n    (\"f\", \"pool\", clf_fisher, xb_real_fisher_trn),\n    (\"h\", GT[6], clf_howard, xb_real_howard_trn),\n]:\n    assert xb.shape[0] == BS\n    print(xb.min(), xb.max())\n    res[name] = get_fid_logits(clf.to(xb.device), xb, layer)\nf, h = res[\"f\"], res[\"h\"]\n\ntensor(-0.5000) tensor(0.5000)\ntensor(-1.) tensor(1.)\n\n\n\nf.mean(), f.T.cov().sum(), h.mean(), h.T.cov().sum()\n\n(tensor(0.2282), tensor(2885.0217), tensor(0.1809), tensor(1464.5834))\n\n\n\nf.sum() / 256, h.sum() / 256\n\n(tensor(116.8285), tensor(92.6292))\n\n\n\nfig, ax = plt.subplots(figsize=(3, 3))\nax.hist(f.reshape(-1), label=\"Fisher\", alpha=0.5, bins=100)\nax.hist(h.reshape(-1), label=\"Howard\", alpha=0.5, bins=100)\nfig.legend()\n\n\n\n\n\n\n\n\n\nxb_real_fisher_tst, _ = dls.peek(\"test\")\nxb_real_howard_tst = xb_real_fisher_tst * 2\n\n\nimg_eval_howard = ImageEval(clf_howard, xb_real_howard_trn, GT[6])\nimg_eval_fisher = ImageEval(clf_fisher, xb_real_fisher_trn, \"pool\")\n\n\nimg_eval_howard.fid(xb_real_howard_tst)\n\n18.5101318359375\n\n\n\nimg_eval_fisher.fid(xb_real_fisher_tst)\n\n97.3258056640625\n\n\n\nfids = []\nfor xb_fisher, _ in dls[\"test\"]:\n    if xb_fisher.shape[0] != BS:\n        continue\n    ff = img_eval_fisher.fid(xb_fisher)\n    fh = img_eval_howard.fid(xb_fisher * 2)\n    fids.append((ff, fh))\nffs, fhs = zip(*fids)\n\n\nnp.mean(ffs), np.std(ffs), np.mean(fhs), np.std(fhs)\n\n(93.02215732672276, 7.078136281810362, 17.31780536358173, 3.2822787956176085)\n\n\nIt seems the difference comes from a property of the model itself, but which property?\n\nŒºf, Œ£f = summarize(img_eval_fisher.featurize(xb_real_fisher_trn))\nŒºh, Œ£h = summarize(img_eval_howard.featurize(xb_real_howard_trn))\nŒºf.mean(), Œºh.mean(), Œ£f.sum(), Œ£h.sum()\n\n(tensor(0.2282, device='cuda:0'),\n tensor(0.1809, device='cuda:0'),\n tensor(2884.9214, device='cuda:0'),\n tensor(1464.5806, device='cuda:0'))\n\n\n\nx = clf_fisher.layers[-1](torch.randn(1, 512, 1, 1).to(def_device))\nx.shape\n\ntorch.Size([1, 512, 1, 1])\n\n\n\n(clf_fisher.pool(x) == x.squeeze()).all()\n\ntensor(True, device='cuda:0')\n\n\n\nclf_howard[5]\n\nResBlock(\n  (convs): Sequential(\n    (0): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): GeneralRelu()\n    )\n    (1): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (idconv): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (act): GeneralRelu()\n)",
    "crumbs": [
      "Fr√©chet inception distance"
    ]
  },
  {
    "objectID": "imagenet_diffusion_unet.html",
    "href": "imagenet_diffusion_unet.html",
    "title": "Scaling up U-net diffusion",
    "section": "",
    "text": "Adapted from: - https://www.youtube.com/watch?v=8AgZ9jcQ9v8&list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP&index=17\n\ndls = get_imagenet_dls(bs=64)\n\nHow many classes do we have?\n\ncs = set()\nfor xb, c in dls[\"test\"]:\n    cs.update(c.tolist())\nmax(cs)\n\n199\n\n\n\nshow_images(denorm(xb)[:8, ...], imsize=0.8)\n\n\n\n\n\n\n\n\n\nun = ConditionalTAUnet(\n    n_classes=200,\n    color_channels=3,\n    nfs=(32, 64, 128, 256, 384, 512),\n    n_blocks=(3, 2, 1, 1, 1, 1, 1),\n    attention_heads=(0, 8, 8, 8, 8, 8, 8),\n)\nf\"{sum(p.numel() for p in un.parameters()):,}\"\n\n'34,446,784'\n\n\n\nun = conditional_train(un, dls, lr=4e-3, n_epochs=1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.148\n0\ntrain\n\n\n0.107\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 6min 15s, sys: 1min 12s, total: 7min 28s\nWall time: 7min 31s\n\n\n\nN = 8\nx_0, _ = conditional_ddpm(un, torch.arange(N), (N, 3, 64, 64))\nshow_images(denorm(x_0.cpu()), imsize=0.8);\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:01&lt;00:00, 73.02time step/s]\n\n\n\n\n\n\n\n\n\nNot great üòÇ",
    "crumbs": [
      "Scaling up U-net diffusion"
    ]
  },
  {
    "objectID": "diffusion_unet.html",
    "href": "diffusion_unet.html",
    "title": "Diffusion U-Net",
    "section": "",
    "text": "Adapted from\ndls = get_fashion_dls(bs=512)",
    "crumbs": [
      "Diffusion U-Net"
    ]
  },
  {
    "objectID": "diffusion_unet.html#time-embedding",
    "href": "diffusion_unet.html#time-embedding",
    "title": "Diffusion U-Net",
    "section": "Time embedding",
    "text": "Time embedding\nSupposedly, time embeddings are neccesary to achieve modeling high performance\n\nemb_dim = 16\n\n# This was thought to be the longest sequence that a transformer\n# should be able to handle, even though nowadays sequences can\n# be much longer\nmax_period = 10_000\n\nprint(f\"{-math.log(max_period)=}\")\nexponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim // 2)\nplt.plot(exponent);\n\n-math.log(max_period)=-9.210340371976184\n\n\n\n\n\n\n\n\n\nThey are computed by computing the outer product of the time vector, ts with the exponent function‚Ä¶\n\nbs = 100\nts = torch.linspace(-10, 10, bs)\n\nembedding = ts[:, None].float() * exponent.exp()[None, :]\nembedding.shape\n\ntorch.Size([100, 8])\n\n\n\nnote here that, so far, the embeddings aren‚Äôt very different\n\n\nfig, ax = plt.subplots(figsize=(4, 4))\nfor i in range(0, bs, 25):\n    ax.plot(embedding[i], label=f\"Timestep #{i+1} Embedding\")\nax.set(ylabel=\"Logit\", xlabel=\"Dimension\")\nfig.legend();\n\n\n\n\n\n\n\n\n‚Ä¶and projected into cosine and sine space, and then concatenated.\n\nembedding = torch.cat([embedding.sin(), embedding.cos()], dim=-1)\nembedding.shape\n\ntorch.Size([100, 16])\n\n\n\nfig, ax = plt.subplots(figsize=(4, 4))\nfor i in range(2):\n    ax.plot(embedding[:, i], label=f\"Sine Dimension #{i+1}\")\nfor i in range(8, 10):\n    ax.plot(embedding[:, i], label=f\"Cosine Dimension #{i+1}\")\nax.set(ylabel=\"Logit\", xlabel=\"Timestep\")\nfig.legend();\n\n\n\n\n\n\n\n\nOverall, this results in a time embedding that is similar to its neighbors, but is overall very diverse. This figure demonstrates this, where the first column is similar to the second column, but dissimilar to the 100th column.\n\nshow_image(embedding.T);\n\n\n\n\n\n\n\n\nWe can consolidate this.\n\nsource\n\ntimestep_embedding\n\n timestep_embedding (ts, emb_dim, max_period=10000)\n\nEditting the number of dimensions increases the column size\n\nshow_image(timestep_embedding(ts, 50).T);\n\n\n\n\n\n\n\n\nDecreasing the max_period increases inter-column heterogeneity.\n\nshow_image(timestep_embedding(ts, 16, 100).T);\n\n\n\n\n\n\n\n\nLet‚Äôs reimplement the U-net with time\n\nsource\n\n\nConv\n\n Conv (c_in, c_out, ks=3, stride=1)\n\nWrapper for a Conv block with normalization and activation\n\nsource\n\n\nEmbeddingPreactResBlock\n\n EmbeddingPreactResBlock (t_embed, c_in, c_out, ks=3, stride=2)\n\nConv res block with the preactivation configuration\n\nsource\n\n\nSaveTimeActivationMixin\n\n SaveTimeActivationMixin ()\n\nHelper to save the output of the downblocks to consume in the upblocks\n\nsource\n\n\nTResBlock\n\n TResBlock (t_embed, c_in, c_out, ks=3, stride=2)\n\nRes block with saved outputs\n\nsource\n\n\nTDownblock\n\n TDownblock (t_embed, c_in, c_out, downsample=True, n_layers=1)\n\nA superblock consisting of many downblocks of similar resolutions\n\nsource\n\n\nTUpblock\n\n TUpblock (t_embed, c_in, c_out, upsample=True, n_layers=1)\n\nA superblock consisting of many upblocks of similar resolutions and logic to use the activations of the counterpart downblock.\n\nsource\n\n\nTimeEmbeddingMLP\n\n TimeEmbeddingMLP (c_in, c_out)\n\nSmall neural network to alter the ‚Äúraw‚Äù time embeddings\n\nsource\n\n\nTUnet\n\n TUnet (nfs=(224, 448, 672, 896), n_blocks=(3, 2, 2, 1, 1),\n        color_channels=3)\n\nDiffusion U-net with a diffusion time dimension\n\nsource\n\n\nFashionDDPM\n\n FashionDDPM ()\n\nTraining specific behaviors for the Learner\n\nsource\n\n\ntrain\n\n train (model, dls, lr=0.004, n_epochs=25, extra_cbs=[], loss_fn=&lt;function\n        mse_loss&gt;)\n\n\nun = train(\n    TUnet(\n        color_channels=1,\n        nfs=(32, 64, 128, 256, 384),\n        n_blocks=(3, 2, 1, 1, 1, 1),\n    ),\n    dls,\n    lr=4e-3,\n    n_epochs=25,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.150\n0\ntrain\n\n\n0.087\n0\neval\n\n\n0.056\n1\ntrain\n\n\n0.054\n1\neval\n\n\n0.046\n2\ntrain\n\n\n0.051\n2\neval\n\n\n0.040\n3\ntrain\n\n\n0.044\n3\neval\n\n\n0.037\n4\ntrain\n\n\n0.044\n4\neval\n\n\n0.035\n5\ntrain\n\n\n0.041\n5\neval\n\n\n0.034\n6\ntrain\n\n\n0.044\n6\neval\n\n\n0.032\n7\ntrain\n\n\n0.037\n7\neval\n\n\n0.031\n8\ntrain\n\n\n0.031\n8\neval\n\n\n0.030\n9\ntrain\n\n\n0.033\n9\neval\n\n\n0.029\n10\ntrain\n\n\n0.035\n10\neval\n\n\n0.029\n11\ntrain\n\n\n0.032\n11\neval\n\n\n0.029\n12\ntrain\n\n\n0.035\n12\neval\n\n\n0.028\n13\ntrain\n\n\n0.030\n13\neval\n\n\n0.028\n14\ntrain\n\n\n0.028\n14\neval\n\n\n0.027\n15\ntrain\n\n\n0.028\n15\neval\n\n\n0.027\n16\ntrain\n\n\n0.029\n16\neval\n\n\n0.027\n17\ntrain\n\n\n0.027\n17\neval\n\n\n0.026\n18\ntrain\n\n\n0.027\n18\neval\n\n\n0.026\n19\ntrain\n\n\n0.027\n19\neval\n\n\n0.026\n20\ntrain\n\n\n0.027\n20\neval\n\n\n0.026\n21\ntrain\n\n\n0.026\n21\neval\n\n\n0.026\n22\ntrain\n\n\n0.025\n22\neval\n\n\n0.026\n23\ntrain\n\n\n0.026\n23\neval\n\n\n0.025\n24\ntrain\n\n\n0.026\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 13min 10s, sys: 24.9 s, total: 13min 34s\nWall time: 13min 42s\n\n\n\nxb, _ = dls.peek()\nxb.shape\n\ntorch.Size([512, 1, 32, 32])\n\n\n\nn_steps = 100\nts = torch.linspace(1 - (1 / n_steps), 0, n_steps).to(xb.device)\nx_t = torch.randn(16, 3, 64, 64)\n\n\nsource\n\n\nddpm\n\n ddpm (model, sz=(16, 1, 32, 32), device='cpu', n_steps=100)\n\n\nx_0, _ = ddpm(un, (8, 1, 32, 32))\nshow_images(x_0, imsize=0.8)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00&lt;00:00, 145.43time step/s]\n\n\n\n\n\n\n\n\n\nThis is not bad! This achieves a similar performance to the Huggingface implementation with a simliar number of parameters.",
    "crumbs": [
      "Diffusion U-Net"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SlowAI",
    "section": "",
    "text": "pip install -e .\nThis repo represents my plodding, meticulous notetaking from FastAI‚Äôs Practical Deep Learning Part II. Hence, SlowAI üò¥\nThese notebook contains copied code from the course repository and, therefore, no part of this work should be considered original. Most of it is rewritten to reflect my code aesthetics and to clarify mathematical points, which is the reason I am sharing it. However, all credit goes to Jeremy Howard except where noted otherwise.",
    "crumbs": [
      "SlowAI"
    ]
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Optimizers and Schedulers: Homework",
    "section": "",
    "text": "from functools import partial\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nplt.style.use(\"ggplot\")\nRecall, we want something to situate within this interface\nThat is:\nWe also need to implement the LRScheduler interface:\ntorch.optim.lr_scheduler.LRScheduler??\n\n\nInit signature:\ntorch.optim.lr_scheduler.LRScheduler(\n    optimizer,\n    last_epoch=-1,\n    verbose=False,\n)\nDocstring:      &lt;no docstring&gt;\nSource:        \nclass LRScheduler:\n    def __init__(self, optimizer, last_epoch=-1, verbose=False):\n        # Attach optimizer\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(f'{type(optimizer).__name__} is not an Optimizer')\n        self.optimizer = optimizer\n        # Initialize epoch and base learning rates\n        if last_epoch == -1:\n            for group in optimizer.param_groups:\n                group.setdefault('initial_lr', group['lr'])\n        else:\n            for i, group in enumerate(optimizer.param_groups):\n                if 'initial_lr' not in group:\n                    raise KeyError(\"param 'initial_lr' is not specified \"\n                                   f\"in param_groups[{i}] when resuming an optimizer\")\n        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]\n        self.last_epoch = last_epoch\n        # Following https://github.com/pytorch/pytorch/issues/20124\n        # We would like to ensure that `lr_scheduler.step()` is called after\n        # `optimizer.step()`\n        def with_counter(method):\n            if getattr(method, '_with_counter', False):\n                # `optimizer.step()` has already been replaced, return.\n                return method\n            # Keep a weak reference to the optimizer instance to prevent\n            # cyclic references.\n            instance_ref = weakref.ref(method.__self__)\n            # Get the unbound method for the same purpose.\n            func = method.__func__\n            cls = instance_ref().__class__\n            del method\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                instance = instance_ref()\n                instance._step_count += 1\n                wrapped = func.__get__(instance, cls)\n                return wrapped(*args, **kwargs)\n            # Note that the returned function here is no longer a bound method,\n            # so attributes like `__func__` and `__self__` no longer exist.\n            wrapper._with_counter = True\n            return wrapper\n        self.optimizer.step = with_counter(self.optimizer.step)\n        self.verbose = verbose\n        self._initial_step()\n    def _initial_step(self):\n        \"\"\"Initialize step counts and performs a step\"\"\"\n        self.optimizer._step_count = 0\n        self._step_count = 0\n        self.step()\n    def state_dict(self):\n        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        \"\"\"\n        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the schedulers state.\n        Args:\n            state_dict (dict): scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        self.__dict__.update(state_dict)\n    def get_last_lr(self):\n        \"\"\" Return last computed learning rate by current scheduler.\n        \"\"\"\n        return self._last_lr\n    def get_lr(self):\n        # Compute learning rate using chainable form of the scheduler\n        raise NotImplementedError\n    def print_lr(self, is_verbose, group, lr, epoch=None):\n        \"\"\"Display the current learning rate.\n        \"\"\"\n        if is_verbose:\n            if epoch is None:\n                print(f'Adjusting learning rate of group {group} to {lr:.4e}.')\n            else:\n                epoch_str = (\"%.2f\" if isinstance(epoch, float) else\n                             \"%.5d\") % epoch\n                print(f'Epoch {epoch_str}: adjusting learning rate of group {group} to {lr:.4e}.')\n    def step(self, epoch=None):\n        # Raise a warning if old pattern is detected\n        # https://github.com/pytorch/pytorch/issues/20124\n        if self._step_count == 1:\n            if not hasattr(self.optimizer.step, \"_with_counter\"):\n                warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n                              \"initialization. Please, make sure to call `optimizer.step()` before \"\n                              \"`lr_scheduler.step()`. See more details at \"\n                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n            elif self.optimizer._step_count &lt; 1:\n                warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n                              \"In PyTorch 1.1.0 and later, you should call them in the opposite order: \"\n                              \"`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this \"\n                              \"will result in PyTorch skipping the first value of the learning rate schedule. \"\n                              \"See more details at \"\n                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n        self._step_count += 1\n        with _enable_get_lr_call(self):\n            if epoch is None:\n                self.last_epoch += 1\n                values = self.get_lr()\n            else:\n                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n                self.last_epoch = epoch\n                if hasattr(self, \"_get_closed_form_lr\"):\n                    values = self._get_closed_form_lr()\n                else:\n                    values = self.get_lr()\n        for i, data in enumerate(zip(self.optimizer.param_groups, values)):\n            param_group, lr = data\n            param_group['lr'] = lr\n            self.print_lr(self.verbose, i, lr, epoch)\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\nFile:           ~/miniforge3/envs/slowai/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\nType:           type\nSubclasses:     _LRScheduler, LambdaLR, MultiplicativeLR, StepLR, MultiStepLR, ConstantLR, LinearLR, ExponentialLR, SequentialLR, PolynomialLR, ...\nFirst, let‚Äôs write helpers.\ndef plot_scheduler(sched, n_batches):\n    fig, ax = plt.subplots(figsize=(4, 4))\n    lrs = []\n    lrs.append(sched.get_last_lr())\n    for _ in range(n_batches):\n        sched.optimizer.step()\n        sched.step()\n        lrs.append(sched.get_last_lr())\n    ax.plot(lrs)\n    ax.set(xlabel=\"Time\", ylabel=\"LR\")\nNow, we can do some dummy training to help with plotting and ensure it actually works in a training loop\nX = torch.randn(100, 1)  # 100 samples with 1 feature\ny = 2 * X + 1 + torch.randn(100, 1)  # Add some noise\n\n\nclass LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\ndef train(scheduler_f, nbatches=100, lr=0.01):\n    model = LinearRegressionModel(1, 1)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    scheduler = scheduler_f(optimizer)\n    lrs = []\n    for epoch in range(nbatches):\n        lrs.append(scheduler)\n        outputs = model(X)\n        loss = criterion(outputs, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    plot_scheduler(scheduler, n_batches)\nn_batches = 100\nscheduler_f = partial(torch.optim.lr_scheduler.CosineAnnealingLR, T_max=n_batches)\ntrain(scheduler_f, n_batches)",
    "crumbs": [
      "Optimizers and Schedulers: Homework"
    ]
  },
  {
    "objectID": "homework.html#part-i-cosine-annealing",
    "href": "homework.html#part-i-cosine-annealing",
    "title": "Optimizers and Schedulers: Homework",
    "section": "Part I: Cosine Annealing",
    "text": "Part I: Cosine Annealing\n\nt_max = 100\nlr_max = 2.5\nlr_min = 0.5\nx = torch.arange(t_max)\ny = (1 + torch.cos(x * 3.141 / t_max)) / 2 * (lr_max - lr_min) + lr_min\nfig, ax = plt.subplots(figsize=(4, 4))\nax.plot(x, y)\nax.set_ylim(0, 5);\n\n\n\n\n\n\n\n\n\nclass CosineAnnealingLRScheduler(torch.optim.lr_scheduler.LRScheduler):\n    def __init__(\n        self,\n        optimizer,\n        lr_max: float,\n        lr_min: float,\n        t_max: int,\n        last_epoch=-1,\n        verbose=False,\n    ):\n        # That that the superclass constructor calls .step() on the instance,\n        # such that we need one additional learning rate beyond the number of\n        # steps associated with each batch\n        xs = torch.arange(t_max + 1).float()\n        self.lrs = (1 + torch.cos(xs * 3.141 / t_max)) / 2 * (lr_max - lr_min) + lr_min\n        super().__init__(optimizer, last_epoch, verbose)\n\n    def get_lr(self):\n        return [self.lrs[self._step_count - 1] for _ in self.optimizer.param_groups]\n\n\nscheduler_f = partial(\n    CosineAnnealingLRScheduler,\n    lr_max=1.0,\n    lr_min=0.5,\n    t_max=n_batches,\n)\ntrain(scheduler_f, n_batches)",
    "crumbs": [
      "Optimizers and Schedulers: Homework"
    ]
  },
  {
    "objectID": "homework.html#part-ii-1-cycle",
    "href": "homework.html#part-ii-1-cycle",
    "title": "Optimizers and Schedulers: Homework",
    "section": "Part II: 1 cycle",
    "text": "Part II: 1 cycle\n\nt_max = 100\nlr_start, lr_max, lr_end = 0.1, 0.8, 0.01\n\n\nlra = torch.linspace(lr_start, lr_max, t_max // 2)\nlrb = torch.linspace(lr_max, lr_end, t_max // 2 + t_max % 2)\nlrs = torch.cat((ya, yb))\n\n\nfig, ax = plt.subplots(figsize=(4, 4))\nax.plot(x, lrs)\nax.set_ylim(0, 1);\n\n\n\n\n\n\n\n\n\nclass OneCycleLRScheduler(torch.optim.lr_scheduler.LRScheduler):\n    def __init__(\n        self,\n        optimizer,\n        lrs,\n        t_max: int,\n        last_epoch=-1,\n        verbose=False,\n    ):\n        # That that the superclass constructor calls .step() on the instance,\n        # such that we need one additional learning rate beyond the number of\n        # steps associated with each batch\n        lr_start, lr_max, lr_end = lrs\n        lra = torch.linspace(lr_start, lr_max, t_max // 2)\n        lrb = torch.linspace(lr_max, lr_end, t_max // 2 + (t_max % 2) + 1)\n        self.lrs = torch.cat((lra, lrb))\n        print(len(self.lrs))\n        super().__init__(optimizer, last_epoch, verbose)\n\n    def get_lr(self):\n        return [self.lrs[self._step_count - 1] for _ in self.optimizer.param_groups]\n\n\nscheduler_f = partial(\n    OneCycleLRScheduler,\n    lrs=(0.2, 0.8, 0.02),\n    t_max=n_batches,\n)\ntrain(scheduler_f, n_batches)\n\n101",
    "crumbs": [
      "Optimizers and Schedulers: Homework"
    ]
  },
  {
    "objectID": "tiny_imagenet_b.html",
    "href": "tiny_imagenet_b.html",
    "title": "Tiny Imagenet (Part II)",
    "section": "",
    "text": "Adapted from\n\nhttps://youtu.be/z1In7QaG0fg?si=U_umYKYz2E_ODGzC&t=2714\n\n\ndls = get_imagenet_dls(bs=512, training_preprocessor=preprocess_and_trivial_augment)\n\n\nImprovements to the ResBlock\nConsider the implementation of the ResBlock.\n\nResidualConvBlock??\n\n\nInit signature: ResidualConvBlock(c_in, c_out, stride=2, ks=3, act=True, norm=True)\nSource:        \nclass ResidualConvBlock(nn.Module):\n    \"\"\"Convolutional block with residual links\"\"\"\n    def __init__(self, c_in, c_out, stride=2, ks=3, act=True, norm=True):\n        super().__init__()\n        self.conv_a = Conv(c_in, c_out, stride=1, ks=ks, act=act, norm=norm)\n        self.conv_b = Conv(c_out, c_out, stride=stride, ks=ks, act=False, norm=norm)\n        self.id_conv = nn.Conv2d(c_in, c_out, stride=1, kernel_size=1)\n        self.act = GeneralReLU() if act else None\n    def forward(self, x):\n        x_orig = x.clone()\n        # Downsample the original, if neccesary\n        if self.conv_b.stride == (2, 2):\n            x_orig = F.avg_pool2d(x_orig, kernel_size=2, ceil_mode=True)\n        elif self.conv_b.stride[0] &gt; 2 or self.conv_b.stride[1] &gt; 2:\n            raise ValueError\n        else:\n            assert self.conv_b.stride == (1, 1)\n        # Add extra channels, if neccesary\n        if self.conv_a.in_channels != self.conv_b.out_channels:\n            x_orig = self.id_conv(x_orig)\n        x = self.conv_a(x)\n        x = self.conv_b(x)\n        x += x_orig\n        if self.act:\n            x = self.act(x)\n        return x\nFile:           ~/Desktop/SlowAI/nbs/slowai/resnets.py\nType:           type\nSubclasses:     \n\n\n\nNotice that the original logits are passed through the activation function. There is no ‚Äúpure‚Äù identity path.\nx += x_orig\nif self.act:\n    x = self.act(x)\nreturn x\nThis was pointed out in the ‚ÄúIdentity Mappings in Deep Residual Networks‚Äù paper by the original author of the ResNet paper. This is called a pre-activation resblock.\n\n\n\npreactivation conv diagram\n\n\n\nclass PreactivationConv(nn.Module):\n    def __init__(self, c_in, c_out, stride=2, ks=3):\n        super().__init__()\n        self.norm = nn.BatchNorm2d(c_in)\n        self.act = GeneralReLU()\n        self.conv = nn.Conv2d(\n            c_in,\n            c_out,\n            stride=stride,\n            kernel_size=ks,\n            padding=ks // 2,\n        )\n\n    def forward(self, x):\n        x = self.norm(x)\n        x = self.act(x)\n        x = self.conv(x)\n        return x\n\n\nclass PreactivationResidualConv(nn.Module):\n    \"\"\"Convolutional block with residual links\"\"\"\n\n    def __init__(self, c_in, c_out, stride=2, ks=3):\n        super().__init__()\n        self.c_in = c_in\n        self.c_out = c_out\n        self.stride = stride\n        self.conv_a = PreactivationConv(c_in, c_out, stride=1, ks=ks)\n        self.conv_b = PreactivationConv(c_out, c_out, stride=stride, ks=ks)\n        self.id_conv = nn.Conv2d(c_in, c_out, stride=1, kernel_size=1)\n        self.pool = nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x):\n        # residual path\n        xr = x.clone()\n        if self.stride == 1:\n            pass\n        elif self.stride == 2:\n            xr = self.pool(xr)\n        else:\n            raise ValueError(\"Only 2x downsampling supported\")\n        if self.c_in != self.c_out:\n            xr = self.id_conv(xr)\n\n        # non-residual path\n        x = self.conv_a(x)\n        x = self.conv_b(x)\n\n        return xr + x\n\n\nclass TinyImagePreactivationResNet(nn.Module):\n    def __init__(\n        self,\n        nfs,\n        n_blocks,\n        n_outputs=10,\n        p_drop=0.1,\n    ):\n        super().__init__()\n\n        # Note that we __cannot__ start with a pre-activation\n        # resblock because it starts with a non-linearity!\n        # Doing so would throw away half the data\n        layers = [Conv(3, nfs[0], ks=5, stride=1)]\n        for nb, c_in, c_out in zip(n_blocks, nfs, nfs[1:]):\n            block = StackableResidualConvBlock(\n                nb,\n                c_in,\n                c_out,\n                conv_cls=PreactivationResidualConv,\n            )\n            layers.append(block)\n        self.layers = nn.Sequential(*layers)\n\n        self.conv_act = GeneralReLU()\n        self.conv_norm = nn.BatchNorm2d(nfs[-1])\n        self.pool = nn.AdaptiveAvgPool2d(output_size=1)\n        self.flatten = nn.Flatten()\n        self.drop = nn.Dropout(p_drop)\n        self.lin = nn.Linear(nfs[-1], n_outputs, bias=False)\n        self.lin_norm = nn.BatchNorm1d(n_outputs)\n\n    def forward(self, x):\n        x = self.layers(x)\n        # Since the identity path has no ReLu, we need to\n        # perform a non-linearity and normalization at the\n        # end of the conv block stack\n        x = self.conv_act(x)\n        x = self.conv_norm(x)\n        x = self.pool(x)\n        x = self.flatten(x)\n        x = self.drop(x)\n        x = self.lin(x)\n        x = self.lin_norm(x)\n        return x\n\n    @classmethod\n    def kaiming(cls, *args, **kwargs):\n        model = cls(*args, **kwargs)\n        model.apply(init_leaky_weights)\n        return model\n\n\ndef get_model():\n    n_blocks = (3, 2, 2, 1, 1)\n    return TinyImagePreactivationResNet.kaiming(\n        n_outputs=200,\n        nfs=[32, 64, 128, 256, 512, 1024],\n        n_blocks=(3, 2, 2, 1, 1),\n    )\n\n\nmodel = get_model()\nmods = [*model.layers, model.conv_norm, model.lin, model.lin_norm]\nsummarize(model, mods, dls)\n\n\n\n\n\n\n\n\n\n\n\nType\nInput\nOutput\nN. params\nMFlops\n\n\n\n\nConv\n(512, 3, 64, 64)\n(512, 32, 64, 64)\n2,464\n9.8\n\n\nStackableResidualConvBlock\n(512, 32, 64, 64)\n(512, 64, 32, 32)\n214,272\n218.1\n\n\nStackableResidualConvBlock\n(512, 64, 32, 32)\n(512, 128, 16, 16)\n542,336\n138.4\n\n\nStackableResidualConvBlock\n(512, 128, 16, 16)\n(512, 256, 8, 8)\n2,166,016\n138.4\n\n\nStackableResidualConvBlock\n(512, 256, 8, 8)\n(512, 512, 4, 4)\n3,673,088\n58.7\n\n\nStackableResidualConvBlock\n(512, 512, 4, 4)\n(512, 1024, 2, 2)\n14,686,208\n58.7\n\n\nBatchNorm2d\n(512, 1024, 2, 2)\n(512, 1024, 2, 2)\n2,048\n0.0\n\n\nLinear\n(512, 1024)\n(512, 200)\n204,800\n0.2\n\n\nBatchNorm1d\n(512, 200)\n(512, 200)\n400\n0.0\n\n\nTotal\n\n\n21,491,632\n622.4215839999999\n\n\n\n\n\n\nlr_find(model, dls, start_lr=1e-5, extra_cbs=[])\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n    \n      \n      28.57% [56/196 00:22&lt;00:56 10.839]\n    \n    \n\n\n\n\n\n\n\n\n\n\nmodel = get_model()\ntrain(model, dls, lr=0.1, n_epochs=25, extra_cbs=[])\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.041\n4.908\n0\ntrain\n\n\n0.065\n4.602\n0\neval\n\n\n0.067\n4.606\n1\ntrain\n\n\n0.073\n4.447\n1\neval\n\n\n0.084\n4.434\n2\ntrain\n\n\n0.044\n5.870\n2\neval\n\n\n0.096\n4.323\n3\ntrain\n\n\n0.087\n4.546\n3\neval\n\n\n0.106\n4.231\n4\ntrain\n\n\n0.065\n4.708\n4\neval\n\n\n0.118\n4.127\n5\ntrain\n\n\n0.060\n5.467\n5\neval\n\n\n0.132\n4.029\n6\ntrain\n\n\n0.035\n6.517\n6\neval\n\n\n0.144\n3.939\n7\ntrain\n\n\n0.061\n4.914\n7\neval\n\n\n0.157\n3.848\n8\ntrain\n\n\n0.093\n4.422\n8\neval\n\n\n0.170\n3.756\n9\ntrain\n\n\n0.093\n4.788\n9\neval\n\n\n0.181\n3.696\n10\ntrain\n\n\n0.060\n5.643\n10\neval\n\n\n0.193\n3.611\n11\ntrain\n\n\n0.036\n5.431\n11\neval\n\n\n0.210\n3.528\n12\ntrain\n\n\n0.060\n5.781\n12\neval\n\n\n0.221\n3.451\n13\ntrain\n\n\n0.105\n5.013\n13\neval\n\n\n0.237\n3.364\n14\ntrain\n\n\n0.074\n5.118\n14\neval\n\n\n0.255\n3.260\n15\ntrain\n\n\n0.109\n4.867\n15\neval\n\n\n0.272\n3.162\n16\ntrain\n\n\n0.131\n4.765\n16\neval\n\n\n0.290\n3.056\n17\ntrain\n\n\n0.018\n7.841\n17\neval\n\n\n0.311\n2.954\n18\ntrain\n\n\n0.145\n4.621\n18\neval\n\n\n0.332\n2.840\n19\ntrain\n\n\n0.215\n3.830\n19\neval\n\n\n0.355\n2.729\n20\ntrain\n\n\n0.326\n2.938\n20\neval\n\n\n0.377\n2.618\n21\ntrain\n\n\n0.364\n2.700\n21\neval\n\n\n0.396\n2.523\n22\ntrain\n\n\n0.417\n2.401\n22\neval\n\n\n0.414\n2.443\n23\ntrain\n\n\n0.453\n2.234\n23\neval\n\n\n0.420\n2.402\n24\ntrain\n\n\n0.457\n2.214\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nConsiderably worse. I wonder why.",
    "crumbs": [
      "Tiny Imagenet (Part II)"
    ]
  },
  {
    "objectID": "neural_cellular_automata.html",
    "href": "neural_cellular_automata.html",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "import random\nfrom functools import partial\nfrom pdb import set_trace as bp\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport timm\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom einops import rearrange\nfrom IPython.display import display\nfrom torch import nn, tensor\nfrom tqdm import trange\n\nfrom slowai.learner import (\n    DataLoaders,\n    Learner,\n    MetricsCB,\n    ProgressCB,\n    TrainCB,\n    TrainLearner,\n    def_device,\n)\nfrom slowai.style_transfer import GramLoss, pt_normalize_imagenet\nfrom slowai.utils import download_image, show_image, show_images\nComplex behavior can emerge from simple rules. Conway‚Äôs game of life is a famous example.\nThis lesson was inspired by this distil.pub article that demonstrates a self-organizing, self-repairing system. How do we train something like this?\nThis starts with a neural network that takes the cell state and that that of its neighbors and predicts a evolution that leads to a particular image. Unfortunately, this doesn‚Äôt produce a stable output. We also need to train with random initialization that gives it the ability to correct and maintain its shape.\nWe‚Äôll train a simple texture restoration model, which allows us to leverage the Gram loss from the previous module.",
    "crumbs": [
      "Neural Cellular Automata"
    ]
  },
  {
    "objectID": "neural_cellular_automata.html#defining-the-model",
    "href": "neural_cellular_automata.html#defining-the-model",
    "title": "Neural Cellular Automata",
    "section": "Defining the model",
    "text": "Defining the model\n\nI = tensor([[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]])\nG = tensor([[-1.0, 0.0, 1.0], [-2.0, 1.0, 2.0], [-1.0, 0.0, 0.1]])\nS = tensor([[1.0, 2.0, 1.0], [2.0, -12.0, 2.0], [1.0, 2.0, 1.0]])\nfilters = torch.stack([I, G, G.T, S]).to(def_device)\nchannels, _, _ = filters.shape\n\n\nshow_images([I, G, G.T, S])\n\n\n\n\n\n\n\n\nThe last filter is called a Sobel operator.\n\n\ndef make_grid(n, sz=128):\n    return torch.zeros(n, channels, sz, sz).to(def_device)\n\n\ndef apply_filters(x):\n    b, c, w, h = x.shape\n    y = rearrange(x, \"b c h w -&gt; (b c) h w\").unsqueeze(1)\n    y = F.pad(y, (1, 1, 1, 1), \"circular\")\n    y = F.conv2d(y, filters.unsqueeze(1))\n    return y.reshape(b, -1, w, h)\n\n\ngrid = make_grid(1)\ngrid.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nOur ‚Äúworld‚Äù is \\(128 \\times 128\\) and each position carries four data points or channels.\n\nx = apply_filters(grid)\nx.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\nThis gives us 16 model inputs per pixel, which comes from the four filters applied to each of the four channels .\n\nclass LinearBrain(nn.Module):\n    def __init__(self, grid, nh=8, nc=4, nf=4):\n        \"\"\"NCA update model\n\n        Args:\n            grid: grid, needed for shape\n            nh: number of hidden dimensions\n            nc: number of input channels\n            nf: number of filters\n        \"\"\"\n        super().__init__()\n        layers = [\n            # Bias must be true here to break the symmetry of a newly\n            # initialized zero-filled grid\n            nn.Linear(nc * nf, nh, bias=True),\n            nn.ReLU(),\n            # The bias is false here because updates should be centered\n            # around 0; and, we also want to keep the number of parameters\n            # to a minimum\n            nn.Linear(nh, nc, bias=False),\n        ]\n        self.layers = nn.ModuleList(layers)\n        self.grid = grid\n\n    def forward(self, x):\n        x = rearrange(x, \"b c h w -&gt; (b h w) c\")\n        for layer in self.layers:\n            x = layer(x)\n        return x.reshape(self.grid.shape)\n\n\nm = LinearBrain(grid)\nm.to(def_device)\nm.forward(x).shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nAn alernate approach to the reshaping to use a convolution with a kernel size of 1.\n\nclass Brain(nn.Module):\n    def __init__(self, nh=8, nc=4, nf=4):\n        \"\"\"NCA update model\n\n        Args:\n            nh: number of hidden dimensions\n            nc: number of input channels\n            nf: number of filters\n        \"\"\"\n        super().__init__()\n        layers = [\n            nn.Conv2d(nc * nf, nh, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(nh, nc, kernel_size=1, bias=False),\n        ]\n        self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nm = Brain()\nm.to(def_device)\nm.forward(x).shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nThis is quite elegant! It‚Äôs also highly performant on GPUs, since they were designed to run matrix operations on each pixel.",
    "crumbs": [
      "Neural Cellular Automata"
    ]
  },
  {
    "objectID": "neural_cellular_automata.html#consolidating-the-model",
    "href": "neural_cellular_automata.html#consolidating-the-model",
    "title": "Neural Cellular Automata",
    "section": "Consolidating the model",
    "text": "Consolidating the model\nLet‚Äôs put this all into a class\n\nclass NCA(Brain):\n    @torch.no_grad()\n    def init_(self):\n        w2 = self.layers[-1]\n        w2.weight.data.zero_()\n\n    def forward(self, grid, update_rate=0.5):\n        y = apply_filters(grid)\n        for layer in self.layers:\n            y = layer(y)\n        b, c, h, w = y.shape\n        # Randomly dropout some updates to reflect the non-global\n        # update behavior of biological systems\n        y = F.dropout(y, update_rate)\n        return grid + y\n\n\nm = NCA()\nm.to(def_device)\nx = m.forward(grid)\nx.shape, grid.shape\n\n(torch.Size([1, 4, 128, 128]), torch.Size([1, 4, 128, 128]))",
    "crumbs": [
      "Neural Cellular Automata"
    ]
  },
  {
    "objectID": "neural_cellular_automata.html#training",
    "href": "neural_cellular_automata.html#training",
    "title": "Neural Cellular Automata",
    "section": "Training",
    "text": "Training\n\nstarry_night = \"https://sanctuarymentalhealth.org/wp-content/uploads/2021/03/The-Starry-Night-1200x630-1-979x514.jpg\"\ntarget = download_image(starry_night)\ntarget = pt_normalize_imagenet(target)\nshow_image(target);\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\ndef to_rgb(x):\n    return x[:, :3, :, :] + 0.5\n\n\nclass StyleLoss:\n    def __init__(\n        self,\n        target_img,\n        target_layers=(1, 6, 11, 18, 25),\n        vgg=None,\n    ):\n        if vgg is None:\n            self.vgg = timm.create_model(\"vgg16\", pretrained=True).to(def_device)\n        else:\n            self.vgg = vgg\n        for p in self.vgg.parameters():\n            # No need to train VGG\n            p.requires_grad = False\n\n        self.target_layers = target_layers\n\n        with torch.no_grad():\n            self.tgt = self.grams(target_img.to(def_device))\n\n    def grams(self, x):\n        x = pt_normalize_imagenet(x)\n        if len(x) &lt; 4:\n            x = x.unsqueeze(0)\n        grams_ = []\n        for i, layer in enumerate(self.vgg.features[: max(self.target_layers) + 1]):\n            b, c, h, w = x.shape\n            x = layer(x)\n            if i in self.target_layers:\n                f = x.clone()  # Not sure if I need this\n                g = torch.einsum(\"bchw, bdhw -&gt; bcd\", f, f) / (h * w)\n                grams_.append(g)\n        return grams_\n\n    def __call__(self, img):\n        src = self.grams(img)\n        # Writing MSE out manually here helps by broadcasting the style gram\n        # matrices to each of the sample image gram matrices\n        return sum((f1 - f2).pow(2).mean() for f1, f2 in zip(src, self.tgt))\n\n\nloss_f = StyleLoss(torch.randn((3, 64, 64)).to(def_device))\n\n\ndef train(\n    stlye_img,\n    style_loss_scale=0.1,\n    n=128,\n    sz=256,\n    bs=4,\n    step_n_min=32,\n    step_n_max=96,\n    lr=1e-3,\n    train_iterations=1200,\n    model_application_iterations=(32, 96),\n):\n    nca = NCA()\n    nca.to(def_device)\n    nca.init_()\n    pool = make_grid(n, sz=sz).to(def_device)\n    loss_f = StyleLoss(stlye_img)\n    opt = torch.optim.Adam(nca.parameters(), lr)\n\n    ipy_output = None\n    K = 3.5\n    fig, (a0, a1, a2) = plt.subplots(1, 3, figsize=(K * 3, K))\n    losses = []\n\n    pbar = trange(train_iterations)\n    for i in pbar:\n        # Subsample with replacement\n        subpool_idxs = torch.randint(0, n, (bs,))\n        subpool = pool[subpool_idxs]\n\n        # Randomly zero out samples\n        if random.random() &gt; 0.8:\n            subpool[:1] = make_grid(1, sz=sz).to(def_device)\n\n        # Apply the model\n        min_, max_ = model_application_iterations\n        n_iterations = random.randrange(min_, max_ + 1)\n        for _ in range(n_iterations):\n            subpool = nca(subpool)\n\n        if i &gt; 0:\n            assert not (subpool == 0).all()\n\n        # Update the pool\n        with torch.no_grad():\n            pool[subpool_idxs] = subpool\n\n        # Compute loss\n        style_loss = loss_f(to_rgb(subpool)) * style_loss_scale\n        overflow_loss = (subpool - subpool.clamp(-1.0, 1.0)).abs().sum()\n        loss = style_loss + overflow_loss\n        losses.append((loss.item(), style_loss.item(), overflow_loss.item()))\n\n        pbar.set_description(f\"{style_loss.item():.2f} {overflow_loss.item():.2f}\")\n        if i % 100 == 0 and i &gt; 0:\n            x = range(0, i + 1)\n            combined, style_losses, overflow_losses = zip(*losses)\n            for ax, y, label in [\n                (a0, style_losses, \"style\"),\n                (a1, overflow_losses, \"overflow\"),\n                (a2, combined, \"overall\"),\n            ]:\n                ax.clear()\n                ax.scatter(x, y, label=label)\n                ax.set_yscale(\"log\")\n                ax.legend()\n            fig.tight_layout()\n\n            if ipy_output is None:\n                ipy_output = display(fig, display_id=True)\n            else:\n                ipy_output.update(fig)\n\n        # Backprop with gradient normalization\n        loss.backward()\n        for p in nca.parameters():\n            p.grad /= p.grad.norm() + 1e-8\n        opt.step()\n        opt.zero_grad()\n\n    return nca, pool.detach()\n\n\nmodel, pool = train(target)\n\n1258.36 34410.16:   9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                  | 108/1200 [00:10&lt;01:28, 12.38it/s]\n\n\n\n\n\n\n\n\n\n750.20 28.38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [01:56&lt;00:00, 10.28it/s]\n\n\n\n\n\n\n\n\n\n\nshow_images(to_rgb(pool).clip(0, 1)[:8, ...])\n\n\n\n\n\n\n\n\nStarting from an empty grid\n\nimages = []\nx = make_grid(n=1)\nfor i in range(90):\n    x = model(x)\n    if i % 10 == 0:\n        imgs = to_rgb(x).clip(0, 1).squeeze()\n        images.append(imgs)\nshow_images(images)",
    "crumbs": [
      "Neural Cellular Automata"
    ]
  },
  {
    "objectID": "cifar10.html",
    "href": "cifar10.html",
    "title": "CIFAR-10",
    "section": "",
    "text": "Adapted from\nimport math\n\nimport fastcore.all as fc\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom functools import partial\nfrom diffusers import UNet2DModel\nfrom torch import tensor\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\n\nfrom slowai.cos_revisited import DDPM\nfrom slowai.learner import (\n    Callback,\n    DataLoaders,\n    DeviceCB,\n    Learner,\n    MetricsCB,\n    ProgressCB,\n    TrainCB,\n    after,\n    def_device,\n    only,\n    tensorize_images,\n)\nfrom slowai.sgd import BatchSchedulerCB, RecorderCB\nfrom slowai.utils import clean_mem, get_grid, glomf, show_image, show_images\nRecorderCB??\n\n\nInit signature: RecorderCB(**d)\nSource:        \nclass RecorderCB(Callback):\n    \"\"\"Record internal state values at each batch.\"\"\"\n    def __init__(self, **d):\n        self.d = d\n        self.learn = None\n    def before_fit(self, learn):\n        self.learn = learn\n        self.recs = {k: [] for k in self.d}\n        self.pg = learn.opt.param_groups[0]\n    def after_batch(self, learn):\n        if not learn.training:\n            return\n        for k, v in self.d.items():\n            self.recs[k].append(v(self))\n    def plot(self, **kwargs):\n        n = len(self.recs)\n        if \"figsize\" not in kwargs:\n            K = 3\n            kwargs[\"figsize\"] = (K * n, K)\n        fig, axes = plt.subplots(1, n, **kwargs)\n        if n &gt; 1:\n            axes = axes.flatten()\n        else:\n            axes = [axes]\n        for ax, (k, v) in zip(axes, self.recs.items()):\n            ax.plot(v, label=k)\n            ax.legend()\n        fig.tight_layout()\nFile:           ~/Desktop/SlowAI/nbs/slowai/sgd.py\nType:           type\nSubclasses:\nSome notes about CIFAR-10:\ndef get_dls(bs=32):\n    return tensorize_images(\n        DataLoaders.from_hf(\"cifar10\", bs=bs), feature=\"img\", normalize=False\n    ).listify()\ndls = get_dls()\nimgs, _ = dls.peek()\nshow_images(imgs[:8, ...])",
    "crumbs": [
      "CIFAR-10"
    ]
  },
  {
    "objectID": "cifar10.html#experiment-tracking-with-weights-and-biases",
    "href": "cifar10.html#experiment-tracking-with-weights-and-biases",
    "title": "CIFAR-10",
    "section": "Experiment Tracking with Weights and Biases",
    "text": "Experiment Tracking with Weights and Biases\nWe want to practice keeping track of the experimental results. Weights and Biases is a great to do so.\n\nMetricsCB??\n\n\nInit signature: MetricsCB(*ms, **metrics)\nSource:        \nclass MetricsCB(Callback):\n    \"\"\"Update and print metrics\"\"\"\n    def __init__(self, *ms, **metrics):\n        for o in ms:\n            metrics[type(o).__name__] = o\n        self.metrics = metrics\n        self.all_metrics = copy(metrics)\n        self.all_metrics[\"loss\"] = self.loss = torchmetrics.aggregation.MeanMetric()\n    def _log(self, d, learn):\n        print(d)\n    def before_fit(self, learn):\n        learn.metrics = self\n    def before_epoch(self, learn):\n        [o.reset() for o in self.all_metrics.values()]\n    def after_epoch(self, learn):\n        log = {k: f\"{v.compute():.3f}\" for k, v in self.all_metrics.items()}\n        log[\"epoch\"] = learn.epoch\n        log[\"train\"] = \"train\" if learn.model.training else \"eval\"\n        self._log(log, learn)\n    def after_batch(self, learn):\n        x, y = to_cpu(learn.batch)\n        for m in self.metrics.values():\n            m.update(learn.preds.cpu(), y)\n        self.loss.update(learn.loss.cpu(), weight=len(x))\nFile:           ~/Desktop/SlowAI/nbs/slowai/learner.py\nType:           type\nSubclasses:     \n\n\n\n\nsource\n\nWandBDDPM\n\n WandBDDPM (config, *ms, project=None, sample_f, **metrics)\n\nUpdate and print metrics\nAs a motivating example, we‚Äôll train a CIFAR-10 U-Net with more or fewer parameters.\n\nsource\n\n\ncifar10_unet\n\n cifar10_unet (small:bool)\n\n\ndef get_model_sizes():\n    for small in [True, False]:\n        model = cifar10_unet(small=small)\n        print(f\"{sum(p.numel() for p in model.parameters()):,}\")\n\n\nget_model_sizes()\n\n15,891,907\n274,056,163\n\n\n\nsource\n\n\ntrain\n\n train (model, lr=0.001, n_epochs=2, bs=256,\n        opt_func=functools.partial(&lt;class 'torch.optim.adam.Adam'&gt;,\n        eps=1e-05), extra_cbs=[], ddpm=&lt;slowai.ddpm.DDPM object at\n        0x7faa60e4afe0&gt;)\n\n\ntrain(cifar10_unet(small=True))\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: jfisher40. Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.16.2\n\n\nRun data is saved locally in /home/jeremy/Desktop/SlowAI/nbs/wandb/run-20240204_173430-vtygmh7z\n\n\nSyncing run avid-resonance-26 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/jfisher40/cifar10-ddpm\n\n\n View run at https://wandb.ai/jfisher40/cifar10-ddpm/runs/vtygmh7z\n\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.260\n0\ntrain\n\n\n0.071\n0\neval\n\n\n0.063\n1\ntrain\n\n\n0.059\n1\neval\n\n\n\n\n\n{'loss': '0.260', 'epoch': 0, 'train': 'train'}\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:01&lt;00:00, 64.64time step/s]\n\n\n{'loss': '0.071', 'epoch': 0, 'train': 'eval'}\n{'loss': '0.063', 'epoch': 1, 'train': 'train'}\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:01&lt;00:00, 66.82time step/s]\n\n\n{'loss': '0.059', 'epoch': 1, 'train': 'eval'}\n\n\nwandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n\n\n\n\n\n\nRun history:\n\n\n\nloss\n‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñÅ\n\n\nval_loss\n‚ñà‚ñÅ\n\n\n\nRun summary:\n\n\n\nloss\n0.10664\n\n\ntrain_loss\n0.063\n\n\nval_loss\n0.059\n\n\n\n\n\n\n View run avid-resonance-26 at: https://wandb.ai/jfisher40/cifar10-ddpm/runs/vtygmh7zSynced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20240204_173430-vtygmh7z/logs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain(cifar10_unet(small=False), bs=32)\n\nTracking run with wandb version 0.16.2\n\n\nRun data is saved locally in /home/jeremy/Desktop/SlowAI/nbs/wandb/run-20240204_173618-ztd8ioj7\n\n\nSyncing run copper-elevator-27 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/jfisher40/cifar10-ddpm\n\n\n View run at https://wandb.ai/jfisher40/cifar10-ddpm/runs/ztd8ioj7\n\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.570\n0\ntrain\n\n\n1.000\n0\neval\n\n\n1.000\n1\ntrain\n\n\n1.000\n1\neval\n\n\n\n\n\n{'loss': '0.570', 'epoch': 0, 'train': 'train'}\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:03&lt;00:00, 29.56time step/s]\n\n\n{'loss': '1.000', 'epoch': 0, 'train': 'eval'}\n{'loss': '1.000', 'epoch': 1, 'train': 'train'}\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:03&lt;00:00, 30.27time step/s]\n\n\n{'loss': '1.000', 'epoch': 1, 'train': 'eval'}\n\n\nwandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n\n\n\n\n\n\nRun history:\n\n\n\nloss\n‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\ntrain_loss\n‚ñÅ‚ñà\n\n\nval_loss\n‚ñÅ‚ñÅ\n\n\n\nRun summary:\n\n\n\nloss\n0.99901\n\n\ntrain_loss\n1.0\n\n\nval_loss\n1.0\n\n\n\n\n\n\n View run copper-elevator-27 at: https://wandb.ai/jfisher40/cifar10-ddpm/runs/ztd8ioj7Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20240204_173618-ztd8ioj7/logs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe seem to have convergence issues.\nSome things to try:\n\nReduce LR\nExperiment with initialization",
    "crumbs": [
      "CIFAR-10"
    ]
  },
  {
    "objectID": "diving_deeper.html",
    "href": "diving_deeper.html",
    "title": "Diving Deeper",
    "section": "",
    "text": "Adapted from:",
    "crumbs": [
      "Diving Deeper"
    ]
  },
  {
    "objectID": "diving_deeper.html#matrix-notation-and-playing-with-data",
    "href": "diving_deeper.html#matrix-notation-and-playing-with-data",
    "title": "Diving Deeper",
    "section": "Matrix notation and playing with data",
    "text": "Matrix notation and playing with data\n\nGet the data\n\nsource\n\n\ndownload_mnist\n\n download_mnist (path_gz=Path('data/mnist.pkl.gz'))\n\n\nsource\n\n\ndownload_file\n\n download_file (url, destination)\n\n\ndata_fp = download_mnist()\ndata_fp\n\nPosixPath('data/mnist.pkl.gz')\n\n\n\nwith gzip.open(data_fp, \"rb\") as f:\n    data = pickle.load(f, encoding=\"latin-1\")\n    ((X_trn, y_trn), (X_vld, y_vld), _) = data\nX_trn.shape\n\nNote that \\(784 = 28^2\\)\n\nfig, ax = plt.subplots(1, 1)\nax.imshow(X_trn[0].reshape(28, 28))\n\n\n\n\n\n\n\n\n\n\nWriting a matrix class\n\n@dataclass\nclass Matrix:\n    xs: List[List[float]]\n\n    def __getitem__(self, idxs):\n        x, y = idxs\n        return self.xs[x][y]\n\nThis is implemented by PyTorch (along with all the auto-differentiation stuff)\n\ntorch.Tensor?\n\n\nInit signature: torch.Tensor(self, /, *args, **kwargs)\nDocstring:      &lt;no docstring&gt;\nFile:           ~/miniforge3/envs/slowai/lib/python3.9/site-packages/torch/__init__.py\nType:           _TensorMeta\nSubclasses:     Parameter, UninitializedBuffer, FakeTensor, MaskedTensor",
    "crumbs": [
      "Diving Deeper"
    ]
  },
  {
    "objectID": "diving_deeper.html#apl-functionality",
    "href": "diving_deeper.html#apl-functionality",
    "title": "Diving Deeper",
    "section": "APL functionality",
    "text": "APL functionality\nDefining a tensor (or ‚Äúarrays,‚Äù using their own terminology), a.\na ‚É™ 3 5 6\nMultiplying by a scalar\na ‚®â 3\nElement-wise division\nb ‚É™ 7 8 9\na √∑ b\nNumpy was influenced by APL, PyTorch was influenced by numpy.\nOne thing that differs is that scalars are just ‚Äú1-rank‚Äù tensors in numpy, whereas they have special scalars have special semantics in numpy.",
    "crumbs": [
      "Diving Deeper"
    ]
  },
  {
    "objectID": "coco_a.html",
    "href": "coco_a.html",
    "title": "COCO: Coloration",
    "section": "",
    "text": "First, let‚Äôs download the data.\n\nwget http://images.cocodataset.org/zips/train2017.zip -O data/train2017.zip\nunzip data/train2017.zip\nrm data/train2017.zip\n\nNow, we want to remove all black and white images, because we want to predict the color data. We‚Äôll use a simple heuristic to determine if the image is black and white.\n\nsource\n\nto_img\n\n to_img (im)\n\nConvert PIL image to numpy\n\nsource\n\n\nblack_and_white\n\n black_and_white (im, viz=False, thresh=0.01)\n\nInfer whether image is black and white by seeing how much the original image and a converted black-and-white version differ from one another\n\nfp = \"data/train2017\"\nfps = [str(fi) for fi in Path(fp).glob(\"**/*.jpg\")]\n\nWe can see this working as follows:\n\nblack_and_white(Image.open(fps[0]), viz=True)\n\nTrue\n\n\n\n\n\n\n\n\n\n\nblack_and_white(Image.open(fps[1]), viz=True)\n\nFalse\n\n\n\n\n\n\n\n\n\n\nfps_color = []\nwith multiprocessing.Pool() as p:\n    iter_ = zip(p.imap(black_and_white, fps), fps)\n    for b_w, fp in tqdm(iter_, total=len(fps)):\n        if not b_w:\n            fps_color.append(fp)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118287/118287 [04:46&lt;00:00, 412.95it/s]\n\n\n\nfps_color = set(fps_color)\nfor fp in Path(fp).glob(\"**/*.jpg\"):\n    if fp not in fps_color:\n        fp.unlink()\n\n\nsource\n\n\ncoco_2017_trn\n\n coco_2017_trn (fps=None, n=None, remove_bw=True)\n\nCombine the image preprocessing logic and return a huggingface dataset\n\nds = coco_2017_trn(fps_color)\n\n\n\n\nCPU times: user 5.98 s, sys: 1.09 s, total: 7.07 s\nWall time: 9.98 s\n\n\n\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['image_fp', 'image'],\n        num_rows: 101938\n    })\n    test: Dataset({\n        features: ['image_fp', 'image'],\n        num_rows: 11327\n    })\n})\n\n\n\nsource\n\n\ncrop_to_box\n\n crop_to_box (img:&lt;module'PIL.Image'from'/opt/hostedtoolcache/Python/3.10.\n              14/x64/lib/python3.10/site-packages/PIL/Image.py'&gt;)\n\n\nsource\n\n\npreprocess_super_rez\n\n preprocess_super_rez (examples, pipe, extra_blur=False)\n\n\nrows = ds[\"train\"][:6]\nfig, axes = plt.subplots(4, 5, figsize=(10, 8))\nfor ax in axes.flatten():\n    ax.set_xticks([])\n    ax.set_yticks([])\ntrn = preprocess_super_rez(rows, pipe=trn_preprocess_super_rez)\ntst = preprocess_super_rez(rows, pipe=tst_preprocess_super_rez)\nfor im_trn_hi, im_trn_lo, im_test, im_org, ax_col in zip(\n    trn[\"image_high_rez\"],\n    trn[\"image_low_rez\"],\n    tst[\"image_high_rez\"],\n    rows[\"image\"],\n    axes.T,\n):\n    for ax, im in zip(ax_col, (im_trn_lo, im_trn_hi, im_test)):\n        ax.imshow(denorm(im).permute(1, 2, 0))\n    ax_col[3].imshow(im_org)\naxes[0, 0].set(title=\"Train (Low Rez)\")\naxes[1, 0].set(title=\"Train (Hi Rez)\")\naxes[2, 0].set(title=\"Test\")\naxes[3, 0].set(title=\"Original\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_coco_dataset\n\n get_coco_dataset (fac, trn, tst, fp='data/train2017', bs=512, n=None,\n                   columns=['image_low_rez', 'image_high_rez'])\n\n\ndls = get_coco_dataset_super_rez(n=100)\n\n\n\n\n\n\n\nCPU times: user 842 ms, sys: 188 ms, total: 1.03 s\nWall time: 858 ms\n\n\nWe also want to do colorization\n\nsource\n\n\npreprocess_colorization\n\n preprocess_colorization (examples, pipe)\n\n\ndls = get_coco_dataset_colorization(n=100)\n\n\n\n\n\n\n\nCPU times: user 959 ms, sys: 162 ms, total: 1.12 s\nWall time: 897 ms\n\n\n\nxb, yb = dls.peek()\n\n\ndenorm(xb).max()\n\ntensor(0.9961)\n\n\n\nshow_images(denorm(xb[:6, ...]), imsize=(1.6))\n\n\n\n\n\n\n\n\n\nshow_images(denorm(yb[:6, ...]), imsize=(1.6))",
    "crumbs": [
      "COCO: Coloration"
    ]
  },
  {
    "objectID": "colorization.html",
    "href": "colorization.html",
    "title": "Colorization",
    "section": "",
    "text": "dls = get_coco_dataset_super_rez(bs=128)\n\n\n\n\n\n\n\nCPU times: user 3min 25s, sys: 21.5 s, total: 3min 47s\nWall time: 2min\n\n\nLet‚Äôs start with super-resolution, to see how this compares to Imagenet200 and get a sense of the compute requirements.\n\nxb, yb = dls.peek()\n\n\nshow_images(denorm(xb[:6, ...]), imsize=(1.6))\n\n\n\n\n\n\n\n\n\nshow_images(denorm(yb[:6, ...]), imsize=(1.6))\n\n\n\n\n\n\n\n\n\nloss_fn = partial(F.mse_loss, reduction=\"mean\")\n\n\nlr_find(\n    Unet.kaiming(),\n    dls,\n    gamma=1.1,\n    start_lr=1e-5,\n    loss_fn=loss_fn,\n)\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n    \n      \n      8.53% [71/832 00:36&lt;06:34 0.813]\n    \n    \n\n\n\n\n\n\n\n\n\n\nun = train(\n    Unet.kaiming(),\n    dls,\n    lr=1e-3,\n    loss_fn=loss_fn,\n    n_epochs=25,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.118\n0\ntrain\n\n\n0.191\n0\neval\n\n\n0.124\n1\ntrain\n\n\n0.079\n1\neval\n\n\n0.063\n2\ntrain\n\n\n0.046\n2\neval\n\n\n0.040\n3\ntrain\n\n\n0.034\n3\neval\n\n\n0.030\n4\ntrain\n\n\n0.027\n4\neval\n\n\n0.024\n5\ntrain\n\n\n0.023\n5\neval\n\n\n0.020\n6\ntrain\n\n\n0.020\n6\neval\n\n\n0.018\n7\ntrain\n\n\n0.019\n7\neval\n\n\n0.017\n8\ntrain\n\n\n0.018\n8\neval\n\n\n0.016\n9\ntrain\n\n\n0.017\n9\neval\n\n\n0.015\n10\ntrain\n\n\n0.016\n10\neval\n\n\n0.014\n11\ntrain\n\n\n0.016\n11\neval\n\n\n0.014\n12\ntrain\n\n\n0.015\n12\neval\n\n\n0.013\n13\ntrain\n\n\n0.015\n13\neval\n\n\n0.013\n14\ntrain\n\n\n0.015\n14\neval\n\n\n0.013\n15\ntrain\n\n\n0.014\n15\neval\n\n\n0.012\n16\ntrain\n\n\n0.014\n16\neval\n\n\n0.012\n17\ntrain\n\n\n0.014\n17\neval\n\n\n0.012\n18\ntrain\n\n\n0.014\n18\neval\n\n\n0.012\n19\ntrain\n\n\n0.014\n19\neval\n\n\n0.012\n20\ntrain\n\n\n0.014\n20\neval\n\n\n0.012\n21\ntrain\n\n\n0.014\n21\neval\n\n\n0.012\n22\ntrain\n\n\n0.014\n22\neval\n\n\n0.012\n23\ntrain\n\n\n0.014\n23\neval\n\n\n0.012\n24\ntrain\n\n\n0.014\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 2h 17min 53s, sys: 7min 55s, total: 2h 25min 48s\nWall time: 2h 57min 45s\n\n\n\nwith torch.no_grad():\n    yp = un(xb.to(def_device)).cpu()\nviz(xb, yb, yp)\n\n\n\n\n\n\n\n\nLooks great!\nNow that we can confidently say that the model does something, we can start to tweak it to do colorization.\nThe first step is use the data and model as-is and see how well it does.\n\ndls = get_coco_dataset_colorization(bs=128)\n\n\n\n\n\n\n\nCPU times: user 3min 35s, sys: 27.6 s, total: 4min 3s\nWall time: 2min 35s\n\n\n\nlr_find(\n    Unet.kaiming(),\n    dls,\n    gamma=1.1,\n    start_lr=1e-5,\n    loss_fn=loss_fn,\n)\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n    \n      \n      7.21% [60/832 00:30&lt;06:36 1.693]\n    \n    \n\n\n\n\n\n\n\n\n\n\nun = train(\n    Unet.kaiming(),\n    dls,\n    lr=1e-3,\n    loss_fn=loss_fn,\n    n_epochs=25,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n2.717\n0\ntrain\n\n\n0.309\n0\neval\n\n\n0.214\n1\ntrain\n\n\n0.155\n1\neval\n\n\n0.136\n2\ntrain\n\n\n0.119\n2\neval\n\n\n0.115\n3\ntrain\n\n\n0.107\n3\neval\n\n\n0.107\n4\ntrain\n\n\n0.106\n4\neval\n\n\n0.102\n5\ntrain\n\n\n0.101\n5\neval\n\n\n0.098\n6\ntrain\n\n\n0.098\n6\neval\n\n\n0.096\n7\ntrain\n\n\n0.096\n7\neval\n\n\n0.094\n8\ntrain\n\n\n0.093\n8\neval\n\n\n0.092\n9\ntrain\n\n\n0.096\n9\neval\n\n\n0.091\n10\ntrain\n\n\n0.098\n10\neval\n\n\n0.090\n11\ntrain\n\n\n0.091\n11\neval\n\n\n0.088\n12\ntrain\n\n\n0.090\n12\neval\n\n\n0.087\n13\ntrain\n\n\n0.095\n13\neval\n\n\n0.086\n14\ntrain\n\n\n0.098\n14\neval\n\n\n0.085\n15\ntrain\n\n\n0.093\n15\neval\n\n\n0.083\n16\ntrain\n\n\n0.091\n16\neval\n\n\n0.082\n17\ntrain\n\n\n0.088\n17\neval\n\n\n0.081\n18\ntrain\n\n\n0.084\n18\neval\n\n\n0.080\n19\ntrain\n\n\n0.084\n19\neval\n\n\n0.078\n20\ntrain\n\n\n0.083\n20\neval\n\n\n0.077\n21\ntrain\n\n\n0.082\n21\neval\n\n\n0.077\n22\ntrain\n\n\n0.081\n22\neval\n\n\n0.076\n23\ntrain\n\n\n0.081\n23\neval\n\n\n0.076\n24\ntrain\n\n\n0.081\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 2h 18min 23s, sys: 8min 6s, total: 2h 26min 29s\nWall time: 2h 59min\n\n\n\nxb, yb = dls.peek()\nxb = xb[3:, ...]\nyb = yb[3:, ...]\nwith torch.no_grad():\n    yp = un(xb.to(def_device)).cpu()\nviz(xb, yb, yp)\n\n\n\n\n\n\n\n\nThat is the most impressive result I‚Äôve ever had with machine learing :D\nAlright, here‚Äôs my crazy idea. I think the signal is a bit sparse. What if we were able to train each of the upsampler layers with a downsampled version of the input.\n\nsource\n\nTinyUnetWithMultiresolutionOutputs\n\n TinyUnetWithMultiresolutionOutputs (nfs:list[int]=(32, 64, 128, 256, 512,\n                                     1024), n_blocks=(3, 2, 2, 1, 1))\n\nU-net with outputs at multiple resolutions, enabling training that target these lower resolutions\n\nsource\n\n\nColorizationLearner\n\n ColorizationLearner ()\n\nTraining specific behaviors for the Learner\n\nsource\n\n\ntrain2\n\n train2 (model, dls, lr=0.004, n_epochs=25,\n         extra_cbs=[&lt;slowai.learner.MetricsCB object at 0x7faa41d8f430&gt;],\n         loss_fn=&lt;function mse_loss&gt;)\n\nTrain the Unet with multiple resolution outputs\n\nun = train2(\n    TinyUnetWithMultiresolutionOutputs.kaiming(),\n    dls,\n    lr=1e-3,\n    loss_fn=loss_fn,\n    n_epochs=25,\n)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n10.890\n0\ntrain\n\n\n0.376\n0\neval\n\n\n1.486\n1\ntrain\n\n\n0.180\n1\neval\n\n\n0.920\n2\ntrain\n\n\n0.133\n2\neval\n\n\n0.741\n3\ntrain\n\n\n0.123\n3\neval\n\n\n0.670\n4\ntrain\n\n\n0.109\n4\neval\n\n\n0.633\n5\ntrain\n\n\n0.115\n5\neval\n\n\n0.607\n6\ntrain\n\n\n0.107\n6\neval\n\n\n0.591\n7\ntrain\n\n\n0.098\n7\neval\n\n\n0.577\n8\ntrain\n\n\n0.098\n8\neval\n\n\n0.567\n9\ntrain\n\n\n0.095\n9\neval\n\n\n0.559\n10\ntrain\n\n\n0.098\n10\neval\n\n\n0.550\n11\ntrain\n\n\n0.093\n11\neval\n\n\n0.547\n12\ntrain\n\n\n0.093\n12\neval\n\n\n0.540\n13\ntrain\n\n\n0.103\n13\neval\n\n\n0.534\n14\ntrain\n\n\n0.093\n14\neval\n\n\n0.521\n15\ntrain\n\n\n0.098\n15\neval\n\n\n0.518\n16\ntrain\n\n\n0.091\n16\neval\n\n\n0.510\n17\ntrain\n\n\n0.087\n17\neval\n\n\n0.502\n18\ntrain\n\n\n0.089\n18\neval\n\n\n0.493\n19\ntrain\n\n\n0.087\n19\neval\n\n\n0.488\n20\ntrain\n\n\n0.087\n20\neval\n\n\n0.480\n21\ntrain\n\n\n0.086\n21\neval\n\n\n0.473\n22\ntrain\n\n\n0.086\n22\neval\n\n\n0.467\n23\ntrain\n\n\n0.084\n23\neval\n\n\n0.466\n24\ntrain\n\n\n0.084\n24\neval\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 1h 58min 5s, sys: 8min 2s, total: 2h 6min 7s\nWall time: 2h 56min 59s\n\n\n\nxb, yb = dls.peek()\n\n\nxb = xb[12:, ...]\nyb = yb[12:, ...]\nwith torch.no_grad():\n    yp, _ = un(xb.to(def_device))\nyp = yp.cpu()\nviz(xb, yb, yp)",
    "crumbs": [
      "Colorization"
    ]
  },
  {
    "objectID": "minibatch_training.html",
    "href": "minibatch_training.html",
    "title": "Minibatch training",
    "section": "",
    "text": "Adapted from:",
    "crumbs": [
      "Minibatch training"
    ]
  },
  {
    "objectID": "minibatch_training.html#cross-entropy-loss",
    "href": "minibatch_training.html#cross-entropy-loss",
    "title": "Minibatch training",
    "section": "Cross entropy loss",
    "text": "Cross entropy loss\nContinuing the simple model from the previous notebook, we need to implement a formally apropriate loss function. Regression is inapropriate for categorical outputs because it implies that different categories are different ‚Äúdistances‚Äù from eachother depending on their ordinal.\nThe proper output shall be a probability for each categories and the loss function is known as Cross Entropy loss.\n\nIn information theory, the cross-entropy between two probability distributions \\(p\\) and \\(q\\) over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution \\(q\\), rather than the true distribution \\(p\\).\n\nhttps://en.wikipedia.org/wiki/Cross-entropy\nThis works by:\n\nThe model outputs an unnormalized logit for each category (\\(\\vec{z}\\))\nThe softmax of the output (i.e., expotentiating and dividing by the sum of the expotentiated values) is taken\n\n\\[p_{y_i}=\\sigma_{\\vec{z}}(z_i)=\\frac{e^{z_i}}{\\Sigma e_{z_j}}\\]\n\nThe entropy is computed between each softmax and its corresponding label\n\n\\[\n-\\Sigma_{i=1}^{N} \\left[ y_i log( p_{y_i} ) + ( 1 - y_i ) log( 1 - p_{y_i} ) ) \\right]\n\\]\nWhere \\(y_i \\in \\{0,1\\}\\), such that for a single label output distribution, this simplifies to\n\\[\n-log( p_{y_i} )\n\\]\nMore information here.\n\n# Number of predictions\nN = 5\n\n# Assign some random prediction logits to demonstrate the operation of log-softmax\nprd = torch.rand(N, 10)\n\n\ndef log_softmax_naive(x):\n    softmax = x.exp() / x.exp().sum(axis=-1, keepdim=True)\n    return softmax.log()\n\n\nlsm_prd = log_softmax_naive(prd)\nlsm_prd\n\ntensor([[-2.1, -2.1, -2.6, -2.0, -2.6, -2.4, -2.7, -2.2, -2.0, -2.8],\n        [-2.0, -2.4, -2.1, -2.4, -2.2, -2.5, -2.1, -2.4, -2.7, -2.3],\n        [-2.4, -2.2, -2.4, -1.8, -2.6, -2.4, -2.3, -2.5, -2.1, -2.7],\n        [-2.0, -2.9, -2.1, -2.4, -2.6, -2.1, -2.4, -2.0, -2.4, -2.6],\n        [-2.3, -2.6, -2.2, -2.0, -2.1, -2.7, -2.1, -2.3, -2.2, -2.7]])\n\n\nIn generally, \\(log\\)s are handy because these additions are more numerically stable than products. We can take advantage of this because we have a division within a log:\n\\[\n\\begin{align*}\nlog(p_{y_i}) &= log(\\frac{e^{z_i}}{\\Sigma e_{z_j}}) \\\\\n             &= log(e^{z_i}) - log({\\Sigma e_{z_j}}) \\\\\n             &= z_i - log({\\Sigma e_{z_j}})\n\\end{align*}\n\\]\n\ndef log_softmax_less_naive(x):\n    return x - x.exp().sum(axis=-1, keepdim=True).log()\n\n\nassert torch.isclose(lsm_prd, log_softmax_less_naive(prd)).all()\n\nOne more trick. These sums can get larger, and we can deal with smaller, more stable sums using the LogSumExp trick.\nLet \\(a=max(\\vec{v})\\). Then, \\[\n\\begin{align*}\n\\sum e_{z_j-a} &= e^{z_i-a} + \\dots + e^{z_j-a} \\\\\n                 &= \\frac{e^{z_i} + \\dots + e^{z_j}}{e^{a\\vec{I}}} \\\\\n                 &= \\frac{ \\sum e^{z_j} }{e^{a}}\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\sum e^{z_j} &= e^a  \\left( \\sum e_{z_j-a} \\right) \\\\\nlog \\left( \\sum e_{z_j} \\right) &= log \\left( e^a \\cdot \\left( \\sum e_{z_j-a} \\right) \\right) \\\\\n                                &= log(e^a) + log \\left( \\sum e_{z_j-a} \\right) \\\\\n                                &= a + log \\left( \\sum e_{z_j-a} \\right)\n\\end{align*}\n\\]\n\ndef logsumexp(x):\n    # Since we're using a matrix instead of a vector, we take the row-wise max\n    # to vectorize across all rows\n    a = x.max(dim=-1).values\n    # We also covert `a` into a column vector to be broadcast the same value\n    # across each column\n    return a + (x - a[:, None]).exp().sum(-1).log()\n\n\nassert (logsumexp(prd) == prd.logsumexp(axis=1)).all()\n\n\ndef log_softmax(x):\n    a = x.max(dim=-1).values[:, None]\n    # This gives us the log-sum-exponent term, alternately (x-a).logsumexp(...)\n    lse = a + (x - a).exp().sum(axis=-1, keepdim=True).log()\n    # We subtract this from x to give the final log softmax\n    return x - lse\n\n\nassert torch.isclose(log_softmax(prd), lsm_prd).all()\n\nNow, for some target \\(x\\), the prediction \\(p(x)\\) is given by \\[\n-\\Sigma_i^N x_i \\cdot log ( p(x_i ) )\n\\]\nBut since the \\(x\\)‚Äôs are one-hot encoded, this is simply \\(-log(p(x_{target}))\\). We can index into this target by composing a slice of (row_index, target_index) pairs like so:\n\ntgt = torch.randint(0, 9, size=(N,))\ntgt, prd.shape, prd[range(N), tgt]\n\n(tensor([3, 0, 1, 0, 7]),\n torch.Size([5, 10]),\n tensor([1.0, 0.9, 0.4, 1.0, 0.6]))\n\n\nAlternately,\n\ndef nll(inp, tgt):\n    \"\"\"mean negative log likelihood loss\"\"\"\n    (n,) = tgt.shape\n    return -inp[range(n), tgt].mean()\n\nThis is equivalent to F.nll_loss.\n\nnll(log_softmax(prd), tgt), F.nll_loss(F.log_softmax(prd, dim=-1), tgt)\n\n(tensor(2.1), tensor(2.1))",
    "crumbs": [
      "Minibatch training"
    ]
  },
  {
    "objectID": "minibatch_training.html#training-the-model",
    "href": "minibatch_training.html#training-the-model",
    "title": "Minibatch training",
    "section": "Training the model",
    "text": "Training the model\nHere, we‚Äôll take what we have implemented by hand and substitute the PyTorch equivalents.\n\nüíø Set up the data\n\ndm = MNISTDataModule()\ndm.setup()\nX_trn, y_trn = dm.as_matrix(\"trn\")\nX_trn = rearrange(X_trn, \"n w h -&gt; n (w h)\")\nbs = 128\nn, m = X_trn.shape\nnh = 50  # num. hidden dimensions\nn_output_categories = y_trn.max().item() + 1\nbs, m, n, nh, n_output_categories\n\n(128, 784, 60000, 50, 10)\n\n\n\n\nüó∫Ô∏è Define the model\n\nclass Model(torch.nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [\n            torch.nn.Linear(n_in, nh),\n            torch.nn.ReLU(),\n            torch.nn.Linear(nh, n_out),\n        ]\n\n    def __call__(self, x):\n        for l in self.layers:\n            x = l(x)\n        return x\n\n\nmodel = Model(m, nh, n_output_categories)\n\n\n\nüßê Do a single prediction\n\nxb = X_trn[:bs, :]\nyb = y_trn[:bs]\npreds = model(xb)\npreds, preds.shape\npreds.argmax(axis=1), yb\n\n(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]),\n tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n         1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n         9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n         1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n         7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n         2, 0, 2, 7, 1, 8, 6, 4]))\n\n\n\nF.cross_entropy(preds, yb)\n\ntensor(2.3, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\naccuracy = (preds.argmax(axis=1) == yb).sum() / bs\nf\"{accuracy:.2%}\"\n\n'15.62%'\n\n\n\n\nüèÉ Train in a loop\n\nepochs = 3\nlr = 0.5\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        mask = slice(i, min(n, i + bs))\n        xb = X_trn[mask]\n        yb = y_trn[mask]\n        preds = model(xb)\n        loss = F.cross_entropy(preds, yb)\n        loss.backward()\n        if i == 0:\n            (bs,) = yb.shape\n            accuracy = (preds.argmax(axis=1) == yb).sum() / bs\n            print(f\"{epoch=}: loss={loss.item():.2f}, accuracy={accuracy.item():.2%}\")\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, \"weight\"):  # i.e., trainable\n                    l.weight -= l.weight.grad * lr\n                    l.bias -= l.bias.grad * lr\n                    l.weight.grad.zero_()\n                    l.bias.grad.zero_()\n\nepoch=0: loss=2.31, accuracy=15.62%\nepoch=1: loss=0.14, accuracy=96.09%\nepoch=2: loss=0.10, accuracy=96.88%\n\n\nAt this point, Jeremy refactors the training loop to:\n\nincorporate a module/parameter registry to make it cleaner to update the weights\nreimplemented the models in the previous notebook as torch.nn.Module‚Äôs\nimplements an optimizer class that stores the parameters and updates them based on the gradient computed by torch itself\nreplaces the optimizer with the torch.optim equivalent\nrefactored the data loader with the apropriate torch primitives\n\nI‚Äôm skipping all this because I‚Äôm pretty solid with the PyTorch fundamentals already.\nA few nice tips\n\nIn fastcore, How do I take the *args, **kwargs of a constructor and populate the object state?\n\n\nclass Foo:\n    def __init__(self, bar, baz=\"quz\"):\n        fc.store_attr()\n\n\nFoo(\"qux\").baz\n\n'quz'\n\n\nThis is a bit like a dataclass.__post_init__.\n\nHow do you control the generation of indecies to sample?\n\n\ntorch.utils.data.Sampler classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a Sampler could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.\n\nhttps://pytorch.org/docs/stable/data.html",
    "crumbs": [
      "Minibatch training"
    ]
  }
]