<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="In this module, we go over the fundamentals of Stable Diffusion, including a discussion of the mathematics and the component parts">

<title>slowai - Course overview</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="slowai - Course overview">
<meta property="og:description" content="In this module, we go over the fundamentals of Stable Diffusion, including a discussion of the mathematics and the component parts">
<meta property="og:image" content="https://jeremy.github.io/slowai/00_overview_files/figure-html/cell-4-output-2.png">
<meta property="og:site-name" content="slowai">
<meta property="og:image:height" content="156">
<meta property="og:image:width" content="484">
<meta name="twitter:title" content="slowai - Course overview">
<meta name="twitter:description" content="In this module, we go over the fundamentals of Stable Diffusion, including a discussion of the mathematics and the component parts">
<meta name="twitter:image" content="https://jeremy.github.io/slowai/00_overview_files/figure-html/cell-4-output-2.png">
<meta name="twitter:image-height" content="156">
<meta name="twitter:image-width" content="484">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">slowai</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jeremy/slowai" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./overview.html">Course overview</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SlowAI</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./overview.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Course overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diving_deeper.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diving Deeper</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diving_deeper_homework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Diving Deeper, homework</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Clustering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Calculus and Backprop</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./minibatch_training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Minibatch training</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data and visualizations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./convs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./autoencoders.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Autoencoders</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Learner</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./activations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Activation Statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./initializations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Initializations</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation">Motivation</a>
  <ul>
  <li><a href="#show_image" id="toc-show_image" class="nav-link" data-scroll-target="#show_image">show_image</a></li>
  <li><a href="#show_images" id="toc-show_images" class="nav-link" data-scroll-target="#show_images">show_images</a></li>
  </ul></li>
  <li><a href="#an-aside-the-mathematics" id="toc-an-aside-the-mathematics" class="nav-link" data-scroll-target="#an-aside-the-mathematics">An aside, the mathematics</a></li>
  <li><a href="#model-components" id="toc-model-components" class="nav-link" data-scroll-target="#model-components">Model components</a>
  <ul>
  <li><a href="#variational-autoencoder" id="toc-variational-autoencoder" class="nav-link" data-scroll-target="#variational-autoencoder">Variational Autoencoder</a></li>
  <li><a href="#image_from_url" id="toc-image_from_url" class="nav-link" data-scroll-target="#image_from_url">image_from_url</a></li>
  <li><a href="#decompress" id="toc-decompress" class="nav-link" data-scroll-target="#decompress">decompress</a></li>
  <li><a href="#compress" id="toc-compress" class="nav-link" data-scroll-target="#compress">compress</a></li>
  <li><a href="#u-net" id="toc-u-net" class="nav-link" data-scroll-target="#u-net">U-net</a></li>
  <li><a href="#clip" id="toc-clip" class="nav-link" data-scroll-target="#clip">CLIP</a></li>
  </ul></li>
  <li><a href="#putting-it-together" id="toc-putting-it-together" class="nav-link" data-scroll-target="#putting-it-together">Putting it together</a>
  <ul>
  <li><a href="#stablediffusion" id="toc-stablediffusion" class="nav-link" data-scroll-target="#stablediffusion">StableDiffusion</a></li>
  </ul></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">Sampling</a></li>
  <li><a href="#hacking" id="toc-hacking" class="nav-link" data-scroll-target="#hacking">Hacking</a>
  <ul>
  <li><a href="#optimizing-pixels-directly" id="toc-optimizing-pixels-directly" class="nav-link" data-scroll-target="#optimizing-pixels-directly">Optimizing pixels directly</a></li>
  <li><a href="#tinkering-with-the-prompt-embedding" id="toc-tinkering-with-the-prompt-embedding" class="nav-link" data-scroll-target="#tinkering-with-the-prompt-embedding">Tinkering with the prompt embedding</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jeremy/slowai/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Course overview</h1>
</div>

<div>
  <div class="description">
    In this module, we go over the fundamentals of Stable Diffusion, including a discussion of the mathematics and the component parts
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>Adapted from: - <a href="https://course.fast.ai/Lessons/lesson9.html">https://course.fast.ai/Lessons/lesson9.html</a> - <a href="https://www.youtube.com/watch?v=844LY0vYQhc">https://www.youtube.com/watch?v=844LY0vYQhc</a> - <a href="https://youtu.be/mYpjmM7O-30">https://youtu.be/mYpjmM7O-30</a></p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Say we want to generate images. Ideally, we would have a probability distribution of the pixels, <span class="math inline">\(P\)</span>. Let’s say that the probability distribution is for each <span class="math inline">\(28^2\)</span> pixels. Note tha teach pixel is variables of the <strong>probability density function</strong>, or PDF for short).</p>
<p>We don’t have this, but say we had the derivative of this PDF. Recall that mutlivariate calculus is concerned with partial derivatives. For example, the partial derivatives of <span class="math inline">\(f(x,y)=x^2 + y^2\)</span> are:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial f(x,y)}{\partial x} &amp;= 2x \\
\frac{\partial f(x,y)}{\partial y} &amp;= 2y \\
\end{align}
\]</span></p>
<p>In the case of an image, imagine the partial derivative of the function <span class="math inline">\(P\)</span> with respect to a <em>single pixel</em>:</p>
<p><span class="math display">\[
\frac{\partial P(X)}{\partial X_{i,j}} = \frac{P(X)-P(X + \partial X_{i,j})}{\partial X_{i,j}}
\]</span></p>
<p><strong>We can use this idea to generate images.</strong></p>
<p>We could start from a blurry image and “march upwards” in the direction of the partial derivative.</p>
<hr>
<p><a href="https://github.com/jeremy/slowai/blob/main/slowai/overview.py#L42" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="show_image" class="level3">
<h3 class="anchored" data-anchor-id="show_image">show_image</h3>
<blockquote class="blockquote">
<pre><code> show_image (img, title=None, K=2)</code></pre>
</blockquote>
<hr>
<p><a href="https://github.com/jeremy/slowai/blob/main/slowai/overview.py#L31" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="show_images" class="level3">
<h3 class="anchored" data-anchor-id="show_images">show_images</h3>
<blockquote class="blockquote">
<pre><code> show_images (imgs, titles=None, K=2)</code></pre>
</blockquote>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> load_dataset(<span class="st">"mnist"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> np.array(mnist[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">"image"</span>])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> normalize(img)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>img_blurry <span class="op">=</span> img <span class="op">+</span> np.random.normal(size<span class="op">=</span>img.size).reshape(<span class="dv">28</span>, <span class="dv">28</span>) <span class="op">*</span> <span class="fl">0.2</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>img_very_blurry <span class="op">=</span> img <span class="op">+</span> np.random.normal(size<span class="op">=</span>img.size).reshape(<span class="dv">28</span>, <span class="dv">28</span>) <span class="op">*</span> <span class="fl">0.8</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>imgs <span class="op">=</span> [img, img_blurry, img_very_blurry]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>show_images(imgs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Found cached dataset mnist (/Users/jeremiahfisher/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 104.60it/s]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="00_overview_files/figure-html/cell-4-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>An algorithm could look something like this:</p>
<ul>
<li>For all pixel values <span class="math inline">\(X_{i,j} \in X\)</span>, evaluate the partial derivative <span class="math inline">\(\frac{P(X)-P(X + \partial X_{i,j})}{\partial X_{i,j}}\)</span> or <span class="math inline">\(\frac{\partial P(X)}{\partial X_{i,j}}\)</span>. This can also be expressed as <span class="math inline">\(\nabla_X P\)</span></li>
<li>For some hyperparameter constant <span class="math inline">\(C\)</span> and for all pixel values <span class="math inline">\(X_{i,j} \in X\)</span>, <span class="math inline">\(X_{i,j} := X_{i,j} + C \frac{\partial P(X)}{\partial X_{i,j}}\)</span> or, equivalently, <span class="math inline">\(X := X + C \cdot \nabla_X P\)</span></li>
<li>Repeat until satisfied</li>
</ul>
<p>In PyTorch, this would look something like:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> get_image()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Somewhat unusually, you would incorporate the image as a model</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># parameter in order to get auto-differentiation</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_nabla_X_of_P_model(X<span class="op">=</span>X)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_timesteps):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    p_grad <span class="op">=</span> model.forward(X)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    p_grad.backward()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    model.X <span class="op">+=</span> C <span class="op">*</span> model.X.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In fact, we don’t have <span class="math inline">\(P(X)\)</span> or <span class="math inline">\(\nabla_X P(X)\)</span> in real life. But we can solve a related problem.</p>
<p>Notice that <span class="math inline">\(\nabla_X P(X)\)</span> provides a direction from blurrier to sharper images. We can train a neural network to de-blur by adding the blur ourselves. The input-output pair would be <span class="math inline">\(\langle image + \epsilon, \epsilon \rangle\)</span> where <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \Sigma)\)</span> and <span class="math inline">\(\Sigma \in \mathbb{R}^{28^2 \times 28^2}\)</span></p>
</section>
</section>
<section id="an-aside-the-mathematics" class="level2">
<h2 class="anchored" data-anchor-id="an-aside-the-mathematics">An aside, the mathematics</h2>
<p>In mathematical parlance, we seek to fit a set of reverse Markov transitions to maximize the likelihood of the model on the training data. Or “minimizing the variational upper bound on the negative log likelihood.” Not sure what that mean exactly, but I’ll be targetting the mathematical level of myself 6 months ago.</p>
<p>First, we define some terms:</p>
<ul>
<li><span class="math inline">\(X^{(0)}\)</span> is the input data distribution; for example, MNIST digits</li>
<li><span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> are probability density functions</li>
<li><span class="math inline">\(\beta_t\)</span> is the “noise value” at time t</li>
</ul>
<p>We can look to physics for inspiration in AI. This paper draws from the thermodynamics to imagine the opposite of a diffusion process: that is, evolving for a high-entropy distribution (like a noisy image) to a clear one.</p>
<p>In the diffusion process, we have a <strong>“forward” </strong>Gaussian Markov process called <span class="math inline">\(Q\)</span> that governs the transition to a noisier distribution. In nature, this is a Guassian distribution: <span class="math display">\[
Q(X^{(t)} | X^{(t-1)}) = \mathcal{N}(X^{(t-1)}\sqrt{1-\beta_t}, I\beta_t)
\]</span> Note that at <span class="math inline">\(t=0\)</span>, we haven’t added any noise and <span class="math inline">\(B_0=0\)</span>. In fact, there is a simple expected value:</p>
<p><span class="math display">\[
\begin{align}
E\left[\mathcal{N}(X^{(t-1)}\sqrt{1-\beta_t}, I\beta_t)\right]
&amp;= E\left[\mathcal{N}(X^{(t-1)}\sqrt{1-(0)}, I(0)\right]\\
&amp;= E\left[\mathcal{N}(X^{(t-1)}, 0)\right] \\
&amp;= X^{(t-1)}
\end{align}
\]</span></p>
<p>Furthermore, note that adding noise is a simple process and we generally work with the “analytic” conditional distribution where we all the noise all at once. We’ll go over this function later.</p>
<p>The important process to consider, however, is the “backward” Guassian Markov process called <span class="math inline">\(P\)</span></p>
<p><span class="math display">\[
Q(X^{(t)} | X^{(t-1)}) = \mathcal{N}(\square(X^{(t-1)}), \triangle(X^{(t-1)}))
\]</span></p>
<p>We want a function that maximizes the log-likelihood of this probability distribution on the data. Mathematically, this would involve the integral over the parameters and likelihood. This is mathematically intractable.</p>
<p>(An aside, we use log-likelihood instead of likelihood because it increases monotonically and sums are more numerically stable on computers than products.)</p>
<p>Instead of solving the integral directly, we use the <strong>E</strong>vidence <strong>L</strong>ower <strong>Bo</strong>und. This is a score function of the model that balances maximizing the likelihood of the data under the model with the complexity of the model. <span class="math display">\[
ELBO = E\left[log(P_\theta(X))\right] - KL(q(\theta) || p(\theta))
\]</span> The expectation maximization term calculates the probability of the data using the PDF <span class="math inline">\(P\)</span>. The likelier the data under the model, the lower the loss. Simple.</p>
<p>In general, KL divergence measures the difference between distributions. In our case, it is a loss that should be minimized between the expected (or “variational”) weight distribution (<span class="math inline">\(p\)</span>) and the actual (“true posterior”) distribution (<span class="math inline">\(q\)</span>) of the model weights themselves. Basically, we want to see the same distributions in the reverse process that we would see in the forward process.</p>
<p>The <span class="math inline">\(||\)</span> is a notation for the computation: <span class="math display">\[
KL(q || p) = \int q(\theta)log\left( \frac{q(\theta)}{{p(\theta)}} \right) d\theta
\]</span> Let’s break this down.</p>
<ul>
<li>The log ratio of the point values of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> at <span class="math inline">\(\theta\)</span> is a measure of the difference at that point
<ul>
<li>If the probability distributions give the same likelihood for <span class="math inline">\(\theta\)</span>, the resulting value is 0</li>
<li>As <span class="math inline">\(q(\theta)\)</span> goes 0 and <span class="math inline">\(p(\theta)\)</span> goes to 1, the limit of the resulting value diverges to negative infinity</li>
<li>As <span class="math inline">\(q(\theta)\)</span> goes 1 and <span class="math inline">\(p(\theta)\)</span> goes to 0, the limit diverges to positive infinity</li>
</ul></li>
<li><span class="math inline">\(q(\theta)\)</span> is a weight for the integrand</li>
</ul>
<p>So, in sum, the KL diverenge is a sum of the difference in probability weighted by the probability values of the variational distribution. If the distributions are similar, the KL divergence is small; otherwise, they are large.</p>
<p>See: https://www.assemblyai.com/blog/content/media/2022/05/KL_Divergence.mp4</p>
<p>Interestingly, Ho et al fixes the variance schedule. In turns out, this makes it so that the <strong>covariance matrix is constant, the KL divergence term goes to 0 and we only need to estimate the mean</strong>, i.e.: fit a function to <span class="math inline">\(\square(X^{(t-1)})\)</span>.</p>
<p>We can train a model to do so with MSE loss.</p>
<p>This course will demonstrate how to do so from scratch.</p>
<p>Let’s begin by running the algorithm.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> StableDiffusionPipeline.from_pretrained(<span class="st">"CompVis/stable-diffusion-v1-4"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a simple noising scheduler for the initial draft</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>pipe.scheduler <span class="op">=</span> LMSDiscreteScheduler(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    beta_start<span class="op">=</span><span class="fl">0.00085</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    beta_end<span class="op">=</span><span class="fl">0.012</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    beta_schedule<span class="op">=</span><span class="st">"scaled_linear"</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    num_train_timesteps<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipe.to(TORCH_DEVICE)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>pipe.enable_attention_slicing()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a photo of a giraffe in Paris"</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>pipe(prompt).images[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:10&lt;00:00,  1.41s/it]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<p><img src="00_overview_files/figure-html/cell-5-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Now that we see what Stable Diffusion is capable of, we note its three components:</p>
<ul>
<li><strong>Variational Autoencoder</strong></li>
<li><strong>CLIP</strong></li>
<li><strong>Unet</strong></li>
</ul>
</section>
<section id="model-components" class="level2">
<h2 class="anchored" data-anchor-id="model-components">Model components</h2>
<section id="variational-autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="variational-autoencoder">Variational Autoencoder</h3>
<p>Stable Diffusion is a <strong>latent diffusion model.</strong> That means that the model manipulates vectors within the latent space manifold of another model. In this case, that model is a Variational Autoencoder.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>vae <span class="op">=</span> pipe.vae</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Variational autoencoders are trained to compress vector information into a normal distribution manifold and decompress it with minimal reconstruction loss.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="00_overview_files/figure-html/00c3ad25-b005-4bec-b7e8-4365d4b4366b-1-5cd90238-2686-4048-b3f6-907fd79d2f79.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>We can visualize the information from a trained VAE.</p>
<hr>
<p><a href="https://github.com/jeremy/slowai/blob/main/slowai/overview.py#L55" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="image_from_url" class="level3">
<h3 class="anchored" data-anchor-id="image_from_url">image_from_url</h3>
<blockquote class="blockquote">
<pre><code> image_from_url (url:str)</code></pre>
</blockquote>
<hr>
<p><a href="https://github.com/jeremy/slowai/blob/main/slowai/overview.py#L80" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="decompress" class="level3">
<h3 class="anchored" data-anchor-id="decompress">decompress</h3>
<blockquote class="blockquote">
<pre><code> decompress (latents:torch.Tensor, vae:torch.nn.modules.module.Module,
             as_pil=True, no_grad=True)</code></pre>
</blockquote>
<p>Project latents into pixel space</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>latents</td>
<td>Tensor</td>
<td></td>
<td>VAE latents</td>
</tr>
<tr class="even">
<td>vae</td>
<td>Module</td>
<td></td>
<td>VAE</td>
</tr>
<tr class="odd">
<td>as_pil</td>
<td>bool</td>
<td>True</td>
<td>Return a PIL image</td>
</tr>
<tr class="even">
<td>no_grad</td>
<td>bool</td>
<td>True</td>
<td>Discard forward gradientss</td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://github.com/jeremy/slowai/blob/main/slowai/overview.py#L67" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="compress" class="level3">
<h3 class="anchored" data-anchor-id="compress">compress</h3>
<blockquote class="blockquote">
<pre><code> compress (img:PIL.Image.Image, vae:torch.nn.modules.module.Module)</code></pre>
</blockquote>
<p>Project pixels into latent space</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>img</td>
<td>Image</td>
<td>Input image</td>
</tr>
<tr class="even">
<td>vae</td>
<td>Module</td>
<td>VAE</td>
</tr>
</tbody>
</table>
<div class="cell" data-scrolled="true" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>input_ <span class="op">=</span> image_from_url(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"https://www.perfectduluthday.com/wp-content/uploads/2021/12/Weird_Al_Yankovic_profile.jpg"</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>latents <span class="op">=</span> compress(input_, vae)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> decompress(latents, vae)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>show_images([input_, <span class="op">*</span>latents.squeeze().cpu(), output])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>n_params_img <span class="op">=</span> np.product(input_.shape)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>n_params_latents <span class="op">=</span> np.product(latents.shape)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="ss">f"Compression rate: </span><span class="sc">{</span>n_params_img<span class="op">/</span>n_params_latents<span class="sc">:.2f}</span><span class="ss">x"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>'Compression rate: 49.13x'</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="00_overview_files/figure-html/cell-10-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We get 50x compression with no loss in detail.</p>
<p>Note that, since we’re dealing with noise, that noise in the latent space <strong>does not correspond to Gaussian noise in the pixel space</strong>.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> torch.randn(latents.shape, device<span class="op">=</span>latents.device)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>noised_latents <span class="op">=</span> latents <span class="op">*</span> <span class="fl">0.6</span> <span class="op">+</span> noise <span class="op">*</span> <span class="fl">0.4</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> decompress(noised_latents, vae)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>img</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<p><img src="00_overview_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="u-net" class="level3">
<h3 class="anchored" data-anchor-id="u-net">U-net</h3>
<p>U-nets are the model used for denoising</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>unet <span class="op">=</span> pipe.unet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll get much more into the architecture of the U-net later. For now, know that it is a model with a strong translational equivariance property.</p>
<p>There are two inputs to the model: the prompt and the time.</p>
<p>The time is an index of how noisy the image is. This helps the model to determine how much noise to remove, because the amount of noise added at any given step is non-linear.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> pipe.scheduler</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>scheduler</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>LMSDiscreteScheduler {
  "_class_name": "LMSDiscreteScheduler",
  "_diffusers_version": "0.17.1",
  "beta_end": 0.012,
  "beta_schedule": "scaled_linear",
  "beta_start": 0.00085,
  "num_train_timesteps": 1000,
  "prediction_type": "epsilon",
  "trained_betas": null,
  "use_karras_sigmas": false
}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> timesteps <span class="op">=</span> scheduler.timesteps.to(torch.uint8).cpu().numpy()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> scheduler.betas.cpu().numpy()[timesteps]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>ax.scatter(X, Y, marker<span class="op">=</span><span class="st">"+"</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">"Time"</span>, ylabel<span class="op">=</span><span class="vs">r"$\beta$"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>[Text(0.5, 0, 'Time'), Text(0, 0.5, '$\\beta$')]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="00_overview_files/figure-html/cell-14-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="clip" class="level3">
<h3 class="anchored" data-anchor-id="clip">CLIP</h3>
<p>The other thing we could add to make the problem of de-noising easier is to indicate the image class (1, 2, 3, etc). For a simple class distribution, we could just one hot-encode it. The input-output pair would then be <span class="math inline">\(\langle \left( image + \epsilon, t, class \right), \epsilon \rangle\)</span>.</p>
<p>But we cannot one-hot encode the distribution of images on the internet à la Stable Diffusion. Therefore, we need a more sophisticated encoder: CLIP (<strong>C</strong>onstrastively <strong>L</strong>earned <strong>I</strong>mage <strong>P</strong>airs). This works on the idea that the dot-product between image encoding and text encoding of the <em>same thing</em> should be large, while the image encoding and text encoding for <em>different things</em> should be small.</p>
<p>We learn this with a neural network contrastively. For a given batch, <span class="math inline">\(B\)</span>, of (image, language) pairs from html <code>alt</code> tags:</p>
<ul>
<li><p>Compute the encoding <span class="math inline">\(f_{image}(I_i)\)</span> and <span class="math inline">\(f_{language}(L_i)\)</span> for all <span class="math inline">\(i \in |B|\)</span></p></li>
<li><p>Compute the sum <span class="math inline">\(\text{correctly paried loss} := \Sigma_i^{|B|} f_{image}(I_i) \cdot f_{language}(L_i)\)</span></p></li>
<li><p>Compute the sum</p></li>
</ul>
<p><span class="math display">\[
\text{incorrectly paired loss} := \Sigma_i^{|B|} \Sigma_j^{|B|}  \begin{cases}
    f_{image}(I_i) \cdot f_{language}(L_j) &amp; i \neq j\\
    0 &amp; i = j
  \end{cases}
\]</span></p>
<ul>
<li>Final loss = <code>incorrectly paired loss</code> - <code>correctly paired loss</code>. Note that want the overall loss to be small or negative, so we take the negative of the sum of the correctly paired dot products. This pushes the vectors for correctly paired language image to be in the same subspace, and incorrectly paired counterparts into different subspaces.</li>
</ul>
<p><img src="https://github.com/jeremyadamsfisher/SlowAI/blob/main/images/CLIP.jpg?raw=1" class="img-fluid"></p>
<p>The <code>transformers</code> library has a module for this</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> pipe.tokenizer</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>text_encoder <span class="op">=</span> pipe.text_encoder</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor([[49406,   320,  1125,   539,   320, 22826,   530,  3445, 49407]])</code></pre>
</div>
</div>
<p>We can go backwards like so:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> tokens[<span class="dv">0</span>, :<span class="dv">8</span>]:</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>decoder<span class="sc">.</span>get(<span class="bu">int</span>(t))<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>49406: &lt;|startoftext|&gt;
320: a&lt;/w&gt;
1125: photo&lt;/w&gt;
539: of&lt;/w&gt;
320: a&lt;/w&gt;
22826: giraffe&lt;/w&gt;
530: in&lt;/w&gt;
3445: paris&lt;/w&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    text_embeddings <span class="op">=</span> text_encoder(tokens.to(TORCH_DEVICE)).last_hidden_state</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>text_embeddings.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>torch.Size([1, 9, 768])</code></pre>
</div>
</div>
<p>In fact, this isn’t used to directly denoise the image during inference. We use a hack called <strong>C</strong>lassifier <strong>F</strong>ree <strong>G</strong>uidance (CFG), where – in addition to the latents – the model is prompted with a null prompt (an unconditional prompt) and and the original prompt.</p>
<p>See more here: https://www.youtube.com/watch?v=344w5h24-h8</p>
</section>
</section>
<section id="putting-it-together" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-together">Putting it together</h2>
<p>With these components in mind, we can start to put them together.</p>
<hr>
<p><a href="https://github.com/jeremy/slowai/blob/main/slowai/overview.py#L103" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="stablediffusion" class="level3">
<h3 class="anchored" data-anchor-id="stablediffusion">StableDiffusion</h3>
<blockquote class="blockquote">
<pre><code> StableDiffusion
                  (tokenizer:transformers.models.clip.tokenization_clip.CL
                  IPTokenizer, text_encoder:transformers.models.clip.model
                  ing_clip.CLIPTextModel, scheduler:Any, unet:diffusers.mo
                  dels.unet_2d_condition.UNet2DConditionModel, vae:diffuse
                  rs.models.autoencoders.autoencoder_kl.AutoencoderKL)</code></pre>
</blockquote>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> StableDiffusion(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    text_encoder<span class="op">=</span>text_encoder,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    scheduler<span class="op">=</span>scheduler,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    unet<span class="op">=</span>unet,</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    vae<span class="op">=</span>vae,</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>sd(prompt, n_inference_steps<span class="op">=</span><span class="dv">30</span>, as_pil<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40&lt;00:00,  1.35s/it]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="00_overview_files/figure-html/cell-19-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="sampling" class="level2">
<h2 class="anchored" data-anchor-id="sampling">Sampling</h2>
<p>There are many ways to sample from Stable Diffusion. The original sampling was a simple numerical differential equation solver (known as the linear multistep solver or LMS): where we take the steps that the gradient suggests with the magnitude of the solver.</p>
<p>It should be noted that this is similar to solving for neural network weights. Therefore, we take a step accourding to the optimizer and wait for convergence.</p>
<p>We can also use the tricks associated with successful neural network training. It is well-understood that the derivative isn’t always a good indicator of the loss curvature. We can improve the quality of the right direction by incorporating previous computations of the gradient (i.e., momentum).</p>
<p>Read more about that here: https://stable-diffusion-art.com/samplers/</p>
</section>
<section id="hacking" class="level2">
<h2 class="anchored" data-anchor-id="hacking">Hacking</h2>
<section id="optimizing-pixels-directly" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-pixels-directly">Optimizing pixels directly</h3>
<p>We are not constrained to using the algorithms as presented to us. This is all just calculus, so we can optimize with respect to arbirary loss functions.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> StableDiffusionWithArbitraryLoss(StableDiffusion):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    loss_f: Callable</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    k: <span class="bu">float</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    periodicity: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> denoise(<span class="va">self</span>, prompt_embedding, l, t, guidance_scale, i):</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="va">self</span>.periodicity <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> i <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate noise as per usual</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>            noise_pred <span class="op">=</span> <span class="va">self</span>.pred_noise(prompt_embedding, l, t, guidance_scale)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a copy of the latents that keeps track of the gradients</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> l.detach().requires_grad_()</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Take a step all the way towards a predicted x0 and use this to</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute the loss</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>            l_x0 <span class="op">=</span> <span class="va">self</span>.scheduler.step(noise_pred, t, l).pred_original_sample</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>            image_x0 <span class="op">=</span> decompress(l_x0, <span class="va">self</span>.vae, as_pil<span class="op">=</span><span class="va">False</span>, no_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>.loss_f(image_x0) <span class="op">*</span> <span class="va">self</span>.k</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: arbritrary loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the loss gradient with respect to the latents and take</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># a step in that direction</span></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>            (grad,) <span class="op">=</span> torch.autograd.grad(loss, l)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> <span class="va">self</span>.scheduler.sigmas[i]</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> l.detach() <span class="op">-</span> grad <span class="op">*</span> sigma<span class="op">**</span><span class="dv">2</span></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>                noise_pred <span class="op">=</span> <span class="va">self</span>.pred_noise(prompt_embedding, l, t, guidance_scale)</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> <span class="va">self</span>.scheduler.step(noise_pred, t, l).prev_sample</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> l</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blue_loss(images):</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.<span class="bu">abs</span>(images[:, <span class="dv">2</span>] <span class="op">-</span> <span class="fl">0.9</span>).mean()</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>StableDiffusionWithArbitraryLoss(</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>    text_encoder<span class="op">=</span>text_encoder,</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>    scheduler<span class="op">=</span>scheduler,</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>    unet<span class="op">=</span>unet,</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>    vae<span class="op">=</span>vae,</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>    loss_f<span class="op">=</span>blue_loss,</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span><span class="dv">75</span>,</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>    periodicity<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>)(</span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a photo of an octopus, national geographic, dlsr"</span>,</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>    n_inference_steps<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>    as_pil<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code> 17%|████████████████████████                                                                                                                        | 5/30 [00:06&lt;00:33,  1.33s/it]/Users/jeremiahfisher/miniforge3/envs/slowai/lib/python3.9/site-packages/torch/autograd/__init__.py:303: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 33%|███████████████████████████████████████████████▋                                                                                               | 10/30 [00:40&lt;01:21,  4.09s/it] 50%|███████████████████████████████████████████████████████████████████████▌                                                                       | 15/30 [01:18&lt;01:09,  4.65s/it] 67%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 20/30 [02:02&lt;00:53,  5.37s/it] 83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 25/30 [02:42&lt;00:24,  4.96s/it]100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [03:27&lt;00:00,  6.91s/it]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>5: arbritrary loss: 40.17461013793945
10: arbritrary loss: 27.197010040283203
15: arbritrary loss: 21.870607376098633
20: arbritrary loss: 21.79729652404785
25: arbritrary loss: 21.87038230895996</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<p><img src="00_overview_files/figure-html/cell-20-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Look! Less blue than you would expect for an underwater scene. Cool 😎</p>
</section>
<section id="tinkering-with-the-prompt-embedding" class="level3">
<h3 class="anchored" data-anchor-id="tinkering-with-the-prompt-embedding">Tinkering with the prompt embedding</h3>
<p>We are also not constrained to the text embeddings directly from CLIP. We can take embeddings from different tokens and hack them apart.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"an adorable photo of a puppy"</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>tokens_a <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> tokens_a[<span class="dv">0</span>, ...]:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>decoder<span class="sc">.</span>get(<span class="bu">int</span>(t))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>text_embeddings_a <span class="op">=</span> text_encoder(tokens_a.to(TORCH_DEVICE)).last_hidden_state</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>text_embeddings_a.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>49406: &lt;|startoftext|&gt;
550: an&lt;/w&gt;
6298: adorable&lt;/w&gt;
1125: photo&lt;/w&gt;
539: of&lt;/w&gt;
320: a&lt;/w&gt;
6829: puppy&lt;/w&gt;
49407: &lt;|endoftext|&gt;</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>torch.Size([1, 8, 768])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"a adorable photo of a koala"</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>tokens_b <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> tokens_b[<span class="dv">0</span>, ...]:</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>decoder<span class="sc">.</span>get(<span class="bu">int</span>(t))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    text_embeddings_b <span class="op">=</span> text_encoder(tokens_b.to(TORCH_DEVICE)).last_hidden_state</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>text_embeddings_b.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>49406: &lt;|startoftext|&gt;
320: a&lt;/w&gt;
6298: adorable&lt;/w&gt;
1125: photo&lt;/w&gt;
539: of&lt;/w&gt;
320: a&lt;/w&gt;
36654: koala&lt;/w&gt;
49407: &lt;|endoftext|&gt;</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>torch.Size([1, 8, 768])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>combined_text_embeddings <span class="op">=</span> (text_embeddings_a <span class="op">+</span> text_embeddings_b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PuppyKoala(StableDiffusion):</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> embed_prompt(<span class="va">self</span>, _: <span class="bu">str</span>) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>        <span class="kw">global</span> combined_text_embeddings</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        _, max_length, _ <span class="op">=</span> combined_text_embeddings.shape</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        uncond_input <span class="op">=</span> <span class="va">self</span>.tokenizer(</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>            [<span class="st">""</span>],</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>            max_length<span class="op">=</span>max_length,</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>            return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>            uncond_embeddings <span class="op">=</span> <span class="va">self</span>.text_encoder(</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>                uncond_input.input_ids.to(TORCH_DEVICE)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>            uncond_embeddings <span class="op">=</span> uncond_embeddings[<span class="dv">0</span>]</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([uncond_embeddings, combined_text_embeddings])</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> PuppyKoala(</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>    text_encoder<span class="op">=</span>text_encoder,</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    scheduler<span class="op">=</span>scheduler,</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    unet<span class="op">=</span>unet,</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    vae<span class="op">=</span>vae,</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>sd(prompt, n_inference_steps<span class="op">=</span><span class="dv">30</span>, as_pil<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:58&lt;00:00,  1.96s/it]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="24">
<p><img src="00_overview_files/figure-html/cell-23-output-2.png" class="img-fluid"></p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>