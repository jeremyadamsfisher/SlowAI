[
  {
    "objectID": "diving_deeper_homework.html",
    "href": "diving_deeper_homework.html",
    "title": "Diving Deeper, homework",
    "section": "",
    "text": "Negative prompts are an extension of the Classifier Free Guidance Module. Recall this is part of the pred_noise method of StableDiffusion\n\nStableDiffusion.pred_noise?\n\nSignature: StableDiffusion.pred_noise(self, prompt_embedding, l, t, guidance_scale)\nDocstring: &lt;no docstring&gt;\nFile:      ~/Desktop/SlowAI/nbs/slowai/overview.py\nType:      function\n\n\nLet‚Äôs define a helper method to load StableDiffusion, as in the ‚ÄúOverview‚Äù notebook\n\nsource\n\nget_stable_diffusion\n\n get_stable_diffusion (cls=&lt;class 'slowai.overview.StableDiffusion'&gt;)\n\n\nsource\n\n\nget_simple_pipe\n\n get_simple_pipe ()\n\n\nsd = get_stable_diffusion()\n\n\nsd(\n    prompt=\"a photo of a giraffe in Paris\",\n    guidance_scale=7.5,\n    as_pil=True,\n)\n\n  0%|          | 0/30 [00:00&lt;?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:04&lt;00:00,  7.38it/s]\n\n\n\n\n\nprompt_embedding is a tensor four-rank tensor of batch_size x seq_len x channels, where the batch size is 2 because its the concatenated unconditional prompt and the conditional prompt.\n\nsd.embed_prompt(\"a photo of a giraffe in paris\").shape\n\ntorch.Size([2, 77, 768])\n\n\nWe want to add the negative prompt and run this through the denoising unet at the same time. This should make the batch size into 3.\n\nsource\n\n\nStableDiffusionWithNegativePromptA\n\n StableDiffusionWithNegativePromptA\n                                     (tokenizer:transformers.models.clip.t\n                                     okenization_clip.CLIPTokenizer, text_\n                                     encoder:transformers.models.clip.mode\n                                     ling_clip.CLIPTextModel,\n                                     scheduler:Any, unet:diffusers.models.\n                                     unet_2d_condition.UNet2DConditionMode\n                                     l, vae:diffusers.models.autoencoders.\n                                     autoencoder_kl.AutoencoderKL)\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptA)\nembedding = sd.embed_prompt(\"a photo of a giraffe in paris\", \"blurry\")\nembedding.shape\n\ntorch.Size([3, 77, 768])\n\n\nNow, we need to pretty much totally rewrite the denoising method to incorporate this negative guidance.\n\nsource\n\n\nStableDiffusionWithNegativePromptB\n\n StableDiffusionWithNegativePromptB\n                                     (tokenizer:transformers.models.clip.t\n                                     okenization_clip.CLIPTokenizer, text_\n                                     encoder:transformers.models.clip.mode\n                                     ling_clip.CLIPTextModel,\n                                     scheduler:Any, unet:diffusers.models.\n                                     unet_2d_condition.UNet2DConditionMode\n                                     l, vae:diffusers.models.autoencoders.\n                                     autoencoder_kl.AutoencoderKL)\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptB)\nembedding = sd.embed_prompt(\"a photo of a giraffe in paris\", \"blurry\")\nl = sd.init_latents()\nepsilon = sd.pred_noise(embedding, l, t=0, guidance_scale_pos=7.5, guidance_scale_neg=2)\nepsilon.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nFinally, we incorporate the negative prompt into the class API.\n\nsource\n\n\nStableDiffusionWithNegativePromptC\n\n StableDiffusionWithNegativePromptC\n                                     (tokenizer:transformers.models.clip.t\n                                     okenization_clip.CLIPTokenizer, text_\n                                     encoder:transformers.models.clip.mode\n                                     ling_clip.CLIPTextModel,\n                                     scheduler:Any, unet:diffusers.models.\n                                     unet_2d_condition.UNet2DConditionMode\n                                     l, vae:diffusers.models.autoencoders.\n                                     autoencoder_kl.AutoencoderKL)\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptC)\nsd(\n    prompt=\"a photo of a labrador dog\",\n    negative_prompt=\"park, greenery, plants, flowers\",\n    guidance_scale=7.5,\n    neg_guidance_scale=5,\n    as_pil=True,\n)\n\n  0%|          | 0/30 [00:00&lt;?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06&lt;00:00,  4.98it/s]\n\n\n\n\n\n\nsd = get_stable_diffusion(StableDiffusionWithNegativePromptC)\nsd(\n    prompt=\"a photo of a labrador dog in a park\",\n    negative_prompt=\"greenery, plants, flowers\",\n    guidance_scale=7.5,\n    neg_guidance_scale=5,\n    as_pil=True,\n)\n\n  0%|          | 0/30 [00:00&lt;?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06&lt;00:00,  4.97it/s]"
  },
  {
    "objectID": "minibatch_training.html",
    "href": "minibatch_training.html",
    "title": "Minibatch training",
    "section": "",
    "text": "Adapted from:"
  },
  {
    "objectID": "minibatch_training.html#cross-entropy-loss",
    "href": "minibatch_training.html#cross-entropy-loss",
    "title": "Minibatch training",
    "section": "Cross entropy loss",
    "text": "Cross entropy loss\nContinuing the simple model from the previous notebook, we need to implement a formally apropriate loss function. Regression is inapropriate for categorical outputs because it implies that different categories are different ‚Äúdistances‚Äù from eachother depending on their ordinal.\nThe proper output shall be a probability for each categories and the loss function is known as Cross Entropy loss.\n\nIn information theory, the cross-entropy between two probability distributions \\(p\\) and \\(q\\) over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution \\(q\\), rather than the true distribution \\(p\\).\n\nhttps://en.wikipedia.org/wiki/Cross-entropy\nThis works by:\n\nThe model outputs an unnormalized logit for each category (\\(\\vec{z}\\))\nThe softmax of the output (i.e., expotentiating and dividing by the sum of the expotentiated values) is taken\n\n\\[p_{y_i}=\\sigma_{\\vec{z}}(z_i)=\\frac{e^{z_i}}{\\Sigma e_{z_j}}\\]\n\nThe entropy is computed between each softmax and its corresponding label\n\n\\[\n-\\Sigma_{i=1}^{N} \\left[ y_i log( p_{y_i} ) + ( 1 - y_i ) log( 1 - p_{y_i} ) ) \\right]\n\\]\nWhere \\(y_i \\in \\{0,1\\}\\), such that for a single label output distribution, this simplifies to\n\\[\n-log( p_{y_i} )\n\\]\nMore information here.\n\n# Number of predictions\nN = 5\n\n# Assign some random prediction logits to demonstrate the operation of log-softmax\nprd = torch.rand(N, 10)\n\n\ndef log_softmax_naive(x):\n    softmax = x.exp() / x.exp().sum(axis=-1, keepdim=True)\n    return softmax.log()\n\n\nlsm_prd = log_softmax_naive(prd)\nlsm_prd\n\ntensor([[-2.1, -2.1, -2.6, -2.0, -2.6, -2.4, -2.7, -2.2, -2.0, -2.8],\n        [-2.0, -2.4, -2.1, -2.4, -2.2, -2.5, -2.1, -2.4, -2.7, -2.3],\n        [-2.4, -2.2, -2.4, -1.8, -2.6, -2.4, -2.3, -2.5, -2.1, -2.7],\n        [-2.0, -2.9, -2.1, -2.4, -2.6, -2.1, -2.4, -2.0, -2.4, -2.6],\n        [-2.3, -2.6, -2.2, -2.0, -2.1, -2.7, -2.1, -2.3, -2.2, -2.7]])\n\n\nIn generally, \\(log\\)s are handy because these additions are more numerically stable than products. We can take advantage of this because we have a division within a log:\n\\[\n\\begin{align*}\nlog(p_{y_i}) &= log(\\frac{e^{z_i}}{\\Sigma e_{z_j}}) \\\\\n             &= log(e^{z_i}) - log({\\Sigma e_{z_j}}) \\\\\n             &= z_i - log({\\Sigma e_{z_j}})\n\\end{align*}\n\\]\n\ndef log_softmax_less_naive(x):\n    return x - x.exp().sum(axis=-1, keepdim=True).log()\n\n\nassert torch.isclose(lsm_prd, log_softmax_less_naive(prd)).all()\n\nOne more trick. These sums can get larger, and we can deal with smaller, more stable sums using the LogSumExp trick.\nLet \\(a=max(\\vec{v})\\). Then, \\[\n\\begin{align*}\n\\sum e_{z_j-a} &= e^{z_i-a} + \\dots + e^{z_j-a} \\\\\n                 &= \\frac{e^{z_i} + \\dots + e^{z_j}}{e^{a\\vec{I}}} \\\\\n                 &= \\frac{ \\sum e^{z_j} }{e^{a}}\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\sum e^{z_j} &= e^a  \\left( \\sum e_{z_j-a} \\right) \\\\\nlog \\left( \\sum e_{z_j} \\right) &= log \\left( e^a \\cdot \\left( \\sum e_{z_j-a} \\right) \\right) \\\\\n                                &= log(e^a) + log \\left( \\sum e_{z_j-a} \\right) \\\\\n                                &= a + log \\left( \\sum e_{z_j-a} \\right)\n\\end{align*}\n\\]\n\ndef logsumexp(x):\n    # Since we're using a matrix instead of a vector, we take the row-wise max\n    # to vectorize across all rows\n    a = x.max(dim=-1).values\n    # We also covert `a` into a column vector to be broadcast the same value\n    # across each column\n    return a + (x - a[:, None]).exp().sum(-1).log()\n\n\nassert (logsumexp(prd) == prd.logsumexp(axis=1)).all()\n\n\ndef log_softmax(x):\n    a = x.max(dim=-1).values[:, None]\n    # This gives us the log-sum-exponent term, alternately (x-a).logsumexp(...)\n    lse = a + (x - a).exp().sum(axis=-1, keepdim=True).log()\n    # We subtract this from x to give the final log softmax\n    return x - lse\n\n\nassert torch.isclose(log_softmax(prd), lsm_prd).all()\n\nNow, for some target \\(x\\), the prediction \\(p(x)\\) is given by \\[\n-\\Sigma_i^N x_i \\cdot log ( p(x_i ) )\n\\]\nBut since the \\(x\\)‚Äôs are one-hot encoded, this is simply \\(-log(p(x_{target}))\\). We can index into this target by composing a slice of (row_index, target_index) pairs like so:\n\ntgt = torch.randint(0, 9, size=(N,))\ntgt, prd.shape, prd[range(N), tgt]\n\n(tensor([3, 0, 1, 0, 7]),\n torch.Size([5, 10]),\n tensor([1.0, 0.9, 0.4, 1.0, 0.6]))\n\n\nAlternately,\n\ndef nll(inp, tgt):\n    \"\"\"mean negative log likelihood loss\"\"\"\n    (n,) = tgt.shape\n    return -inp[range(n), tgt].mean()\n\nThis is equivalent to F.nll_loss.\n\nnll(log_softmax(prd), tgt), F.nll_loss(F.log_softmax(prd, dim=-1), tgt)\n\n(tensor(2.1), tensor(2.1))"
  },
  {
    "objectID": "minibatch_training.html#training-the-model",
    "href": "minibatch_training.html#training-the-model",
    "title": "Minibatch training",
    "section": "Training the model",
    "text": "Training the model\nHere, we‚Äôll take what we have implemented by hand and substitute the PyTorch equivalents.\n\nüíø Set up the data\n\ndm = MNISTDataModule()\ndm.setup()\nX_trn, y_trn = dm.as_matrix(\"trn\")\nX_trn = rearrange(X_trn, \"n w h -&gt; n (w h)\")\nbs = 128\nn, m = X_trn.shape\nnh = 50  # num. hidden dimensions\nn_output_categories = y_trn.max().item() + 1\nbs, m, n, nh, n_output_categories\n\n(128, 784, 60000, 50, 10)\n\n\n\n\nüó∫Ô∏è Define the model\n\nclass Model(torch.nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [\n            torch.nn.Linear(n_in, nh),\n            torch.nn.ReLU(),\n            torch.nn.Linear(nh, n_out),\n        ]\n\n    def __call__(self, x):\n        for l in self.layers:\n            x = l(x)\n        return x\n\n\nmodel = Model(m, nh, n_output_categories)\n\n\n\nüßê Do a single prediction\n\nxb = X_trn[:bs, :]\nyb = y_trn[:bs]\npreds = model(xb)\npreds, preds.shape\npreds.argmax(axis=1), yb\n\n(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]),\n tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n         1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n         9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n         1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n         7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n         2, 0, 2, 7, 1, 8, 6, 4]))\n\n\n\nF.cross_entropy(preds, yb)\n\ntensor(2.3, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\naccuracy = (preds.argmax(axis=1) == yb).sum() / bs\nf\"{accuracy:.2%}\"\n\n'15.62%'\n\n\n\n\nüèÉ Train in a loop\n\nepochs = 3\nlr = 0.5\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        mask = slice(i, min(n, i + bs))\n        xb = X_trn[mask]\n        yb = y_trn[mask]\n        preds = model(xb)\n        loss = F.cross_entropy(preds, yb)\n        loss.backward()\n        if i == 0:\n            (bs,) = yb.shape\n            accuracy = (preds.argmax(axis=1) == yb).sum() / bs\n            print(f\"{epoch=}: loss={loss.item():.2f}, accuracy={accuracy.item():.2%}\")\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, \"weight\"):  # i.e., trainable\n                    l.weight -= l.weight.grad * lr\n                    l.bias -= l.bias.grad * lr\n                    l.weight.grad.zero_()\n                    l.bias.grad.zero_()\n\nepoch=0: loss=2.31, accuracy=15.62%\nepoch=1: loss=0.14, accuracy=96.09%\nepoch=2: loss=0.10, accuracy=96.88%\n\n\nAt this point, Jeremy refactors the training loop to: - incorporate a module/parameter registry to make it cleaner to update the weights - reimplemented the models in the previous notebook as torch.nn.Module‚Äôs - implements an optimizer class that stores the parameters and updates them based on the gradient computed by torch itself - replaces the optimizer with the torch.optim equivalent - refactored the data loader with the apropriate torch primitives\nI‚Äôm skipping all this because I‚Äôm pretty solid with the PyTorch fundamentals already.\nA few nice tips\n\nIn fastcore, How do I take the *args, **kwargs of a constructor and populate the object state?\n\n\nclass Foo:\n    def __init__(self, bar, baz=\"quz\"):\n        fc.store_attr()\n\n\nFoo(\"qux\").baz\n\n'quz'\n\n\nThis is a bit like a dataclass.__post_init__.\n\nHow do you control the generation of indecies to sample?\n\n\ntorch.utils.data.Sampler classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a Sampler could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.\n\nhttps://pytorch.org/docs/stable/data.html"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Data and visualizations",
    "section": "",
    "text": "Adapted from:\nThis is, again, a topic I already know a lot about so I‚Äôm going through this notebook as quickly as possible. Probably nothing here of interest to anyone üò≠"
  },
  {
    "objectID": "datasets.html#huggingface-datasets",
    "href": "datasets.html#huggingface-datasets",
    "title": "Data and visualizations",
    "section": "Huggingface Datasets",
    "text": "Huggingface Datasets\n\ndsd = load_dataset(\"fashion_mnist\")\ndsd\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\n\nx, y = dsd.column_names[\"train\"]\nx, y\n\n('image', 'label')\n\n\n\ntrn = dsd[\"train\"]\nexample = trn[0]\nxb = example[x]\nyb = example[y]\nxb\n\n\n\n\n\ny_feats = trn.features[y]\ny_feats.int2str(example[y])\n\n'Ankle boot'\n\n\n\ndef collate_fn(b):\n    return dict(\n        x=torch.stack([T.to_tensor(example[x]) for example in b]),\n        y=torch.tensor([example[y] for example in b]),\n    )\n\n\ndl = DataLoader(trn, collate_fn=collate_fn, batch_size=16)\nout = next(iter(dl))\nout[\"x\"].shape, out[\"y\"]\n\n(torch.Size([16, 1, 28, 28]),\n tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9]))\n\n\nThis is the only tip I thought was interesting from the first hour. You can use this to avoid a return.\n\nsource\n\ninplace\n\n inplace (f)\n\n\n@inplace\ndef transformi(batch):\n    batch[\"image\"] = [T.to_tensor(example) for example in batch[\"image\"]]\n\n\nt_trn = trn.with_transform(transformi)\nt_trn[0][\"image\"].shape, t_trn[0][\"label\"]\n\n(torch.Size([1, 28, 28]), 9)"
  },
  {
    "objectID": "datasets.html#plotting",
    "href": "datasets.html#plotting",
    "title": "Data and visualizations",
    "section": "Plotting",
    "text": "Plotting\n\nb = t_trn[0]\nxb = b[\"image\"]\nplt.imshow(xb.squeeze())\n\n&lt;matplotlib.image.AxesImage at 0x7f4649481450&gt;\n\n\n\n\n\nWe‚Äôll copy-and-paste some of the plotting convenience functions\n\nsource\n\nsubplots\n\n subplots (nrows:int=1, ncols:int=1, figsize:tuple=None, imsize:int=3,\n           suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None, **kwargs)\n\nA figure and set of subplots to display images of imsize inches\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\nkwargs\n\n\n\n\n\n\ndelegates is a nice feature that creates the correct documentation for kwargs\n\nsubplots?\n\n\nSignature:\nsubplots(\n    nrows: int = 1,\n    ncols: int = 1,\n    figsize: tuple = None,\n    imsize: int = 3,\n    suptitle: str = None,\n    *,\n    sharex: \"bool | Literal['none', 'all', 'row', 'col']\" = False,\n    sharey: \"bool | Literal['none', 'all', 'row', 'col']\" = False,\n    squeeze: 'bool' = True,\n    width_ratios: 'Sequence[float] | None' = None,\n    height_ratios: 'Sequence[float] | None' = None,\n    subplot_kw: 'dict[str, Any] | None' = None,\n    gridspec_kw: 'dict[str, Any] | None' = None,\n    **kwargs,\n)\nDocstring: A figure and set of subplots to display images of `imsize` inches\nFile:      /tmp/ipykernel_22296/3433216597.py\nType:      function\n\n\n\nI‚Äôm straight up just stealing these from the FastAI notebooks üòà\n\nsource\n\n\nshow_images\n\n show_images (ims:list, nrows:Optional[int]=None,\n              ncols:Optional[int]=None, titles:Optional[list]=None,\n              figsize:tuple=None, imsize:int=3, suptitle:str=None,\n              sharex:\"bool|Literal['none','all','row','col']\"=False,\n              sharey:\"bool|Literal['none','all','row','col']\"=False,\n              squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n              height_ratios:Sequence[float]|None=None,\n              subplot_kw:dict[str,Any]|None=None,\n              gridspec_kw:dict[str,Any]|None=None)\n\nShow all images ims as subplots with rows using titles\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nims\nlist\n\nImages to show\n\n\nnrows\nOptional\nNone\nNumber of rows in grid\n\n\nncols\nOptional\nNone\nNumber of columns in grid (auto-calculated if None)\n\n\ntitles\nOptional\nNone\nOptional list of titles for each image\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Other Parameters\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\nsource\n\n\nshow_image\n\n show_image (im, ax=None, figsize=None, title=None, noframe=True,\n             cmap=None, norm=None, aspect=None, interpolation=None,\n             alpha=None, vmin=None, vmax=None, origin=None, extent=None,\n             interpolation_stage=None, filternorm=True, filterrad=4.0,\n             resample=None, url=None, data=None)\n\nShow a PIL or PyTorch image on ax.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nim\n\n\n\n\n\nax\nNoneType\nNone\n\n\n\nfigsize\nNoneType\nNone\n\n\n\ntitle\nNoneType\nNone\n\n\n\nnoframe\nbool\nTrue\n\n\n\ncmap\nNoneType\nNone\nThe Colormap instance or registered colormap name used to map scalar datato colors.This parameter is ignored if X is RGB(A).\n\n\nnorm\nNoneType\nNone\nThe normalization method used to scale scalar data to the [0, 1] rangebefore mapping to colors using cmap. By default, a linear scaling isused, mapping the lowest value to 0 and the highest to 1.If given, this can be one of the following:- An instance of .Normalize or one of its subclasses (see :ref:colormapnorms).- A scale name, i.e.¬†one of ‚Äúlinear‚Äù, ‚Äúlog‚Äù, ‚Äúsymlog‚Äù, ‚Äúlogit‚Äù, etc. For a list of available scales, call matplotlib.scale.get_scale_names(). In that case, a suitable .Normalize subclass is dynamically generated and instantiated.This parameter is ignored if X is RGB(A).\n\n\naspect\nNoneType\nNone\nThe aspect ratio of the Axes. This parameter is particularlyrelevant for images since it determines whether data pixels aresquare.This parameter is a shortcut for explicitly calling.Axes.set_aspect. See there for further details.- ‚Äòequal‚Äô: Ensures an aspect ratio of 1. Pixels will be square (unless pixel sizes are explicitly made non-square in data coordinates using extent).- ‚Äòauto‚Äô: The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels.Normally, None (the default) means to use :rc:image.aspect. However, ifthe image uses a transform that does not contain the axes data transform,then None means to not modify the axes aspect at all (in that case, directlycall .Axes.set_aspect if desired).\n\n\ninterpolation\nNoneType\nNone\nThe interpolation method used.Supported values are ‚Äònone‚Äô, ‚Äòantialiased‚Äô, ‚Äònearest‚Äô, ‚Äòbilinear‚Äô,‚Äòbicubic‚Äô, ‚Äòspline16‚Äô, ‚Äòspline36‚Äô, ‚Äòhanning‚Äô, ‚Äòhamming‚Äô, ‚Äòhermite‚Äô,‚Äòkaiser‚Äô, ‚Äòquadric‚Äô, ‚Äòcatrom‚Äô, ‚Äògaussian‚Äô, ‚Äòbessel‚Äô, ‚Äòmitchell‚Äô,‚Äòsinc‚Äô, ‚Äòlanczos‚Äô, ‚Äòblackman‚Äô.The data X is resampled to the pixel size of the image on thefigure canvas, using the interpolation method to either up- ordownsample the data.If interpolation is ‚Äònone‚Äô, then for the ps, pdf, and svgbackends no down- or upsampling occurs, and the image data ispassed to the backend as a native image. Note that different ps,pdf, and svg viewers may display these raw pixels differently. Onother backends, ‚Äònone‚Äô is the same as ‚Äònearest‚Äô.If interpolation is the default ‚Äòantialiased‚Äô, then ‚Äònearest‚Äôinterpolation is used if the image is upsampled by more than afactor of three (i.e.¬†the number of display pixels is at leastthree times the size of the data array). If the upsampling rate issmaller than 3, or the image is downsampled, then ‚Äòhanning‚Äôinterpolation is used to act as an anti-aliasing filter, unless theimage happens to be upsampled by exactly a factor of two or one.See:doc:/gallery/images_contours_and_fields/interpolation_methodsfor an overview of the supported interpolation methods, and:doc:/gallery/images_contours_and_fields/image_antialiasing fora discussion of image antialiasing.Some interpolation methods require an additional radius parameter,which can be set by filterrad. Additionally, the antigrain imageresize filter is controlled by the parameter filternorm.\n\n\nalpha\nNoneType\nNone\nThe alpha blending value, between 0 (transparent) and 1 (opaque).If alpha is an array, the alpha blending values are applied pixelby pixel, and alpha must have the same shape as X.\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\nPlace the [0, 0] index of the array in the upper left or lowerleft corner of the Axes. The convention (the default) ‚Äòupper‚Äô istypically used for matrices and images.Note that the vertical axis points upward for ‚Äòlower‚Äôbut downward for ‚Äòupper‚Äô.See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\nextent\nNoneType\nNone\nThe bounding box in data coordinates that the image will fill.These values may be unitful and match the units of the Axes.The image is stretched individually along x and y to fill the box.The default extent is determined by the following conditions.Pixels have unit size in data coordinates. Their centers are oninteger coordinates, and their center coordinates range from 0 tocolumns-1 horizontally and from 0 to rows-1 vertically.Note that the direction of the vertical axis and thus the defaultvalues for top and bottom depend on origin:- For origin == 'upper' the default is (-0.5, numcols-0.5, numrows-0.5, -0.5).- For origin == 'lower' the default is (-0.5, numcols-0.5, -0.5, numrows-0.5).See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\ninterpolation_stage\nNoneType\nNone\nIf ‚Äòdata‚Äô, interpolationis carried out on the data provided by the user. If ‚Äòrgba‚Äô, theinterpolation is carried out after the colormapping has beenapplied (visual interpolation).\n\n\nfilternorm\nbool\nTrue\nA parameter for the antigrain image resize filter (see theantigrain documentation). If filternorm is set, the filternormalizes integer values and corrects the rounding errors. Itdoesn‚Äôt do anything with the source floating point values, itcorrects only integers according to the rule of 1.0 which meansthat any sum of pixel weights must be equal to 1.0. So, thefilter function must produce a graph of the proper shape.\n\n\nfilterrad\nfloat\n4.0\nThe filter radius for filters that have a radius parameter, i.e.when interpolation is one of: ‚Äòsinc‚Äô, ‚Äòlanczos‚Äô or ‚Äòblackman‚Äô.\n\n\nresample\nNoneType\nNone\nWhen True, use a full resampling method. When False, onlyresample when the output image is larger than the input image.\n\n\nurl\nNoneType\nNone\nSet the url of the created .AxesImage. See .Artist.set_url.\n\n\ndata\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nget_grid\n\n get_grid (n:int, nrows:int=None, ncols:int=None, title:str=None,\n           weight:str='bold', size:int=14, figsize:tuple=None,\n           imsize:int=3, suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None)\n\nReturn a grid of n axes, rows by cols\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n\nNumber of axes\n\n\nnrows\nint\nNone\nNumber of rows, defaulting to int(math.sqrt(n))\n\n\nncols\nint\nNone\nNumber of columns, defaulting to ceil(n/rows)\n\n\ntitle\nstr\nNone\nIf passed, title set to the figure\n\n\nweight\nstr\nbold\nTitle font weight\n\n\nsize\nint\n14\nTitle font size\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\n\n\nsource\n\n\nsubplots\n\n subplots (nrows:int=1, ncols:int=1, figsize:tuple=None, imsize:int=3,\n           suptitle:str=None,\n           sharex:\"bool|Literal['none','all','row','col']\"=False,\n           sharey:\"bool|Literal['none','all','row','col']\"=False,\n           squeeze:bool=True, width_ratios:Sequence[float]|None=None,\n           height_ratios:Sequence[float]|None=None,\n           subplot_kw:dict[str,Any]|None=None,\n           gridspec_kw:dict[str,Any]|None=None, **kwargs)\n\nA figure and set of subplots to display images of imsize inches\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsharey\nbool | Literal[‚Äònone‚Äô, ‚Äòall‚Äô, ‚Äòrow‚Äô, ‚Äòcol‚Äô]\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nSequence[float] | None\nNone\n\n\n\nheight_ratios\nSequence[float] | None\nNone\n\n\n\nsubplot_kw\ndict[str, Any] | None\nNone\n\n\n\ngridspec_kw\ndict[str, Any] | None\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nb = t_trn[:16]\nshow_images(b[\"image\"], figsize=(6, 6))"
  },
  {
    "objectID": "activations.html",
    "href": "activations.html",
    "title": "Activation Statistics",
    "section": "",
    "text": "Adapted from:\nWe need to have a way of looking inside models and diagnosing issues.\nsource"
  },
  {
    "objectID": "activations.html#baseline",
    "href": "activations.html#baseline",
    "title": "Activation Statistics",
    "section": "Baseline",
    "text": "Baseline\nLet‚Äôs look at a fashion MNIST classification problem.\n\nsource\n\nConv2dWithReLU\n\n Conv2dWithReLU (*args, stride:Union[int,Tuple[int,int]]=1,\n                 padding:Union[str,int,Tuple[int,int]]=0,\n                 dilation:Union[int,Tuple[int,int]]=1, groups:int=1,\n                 bias:bool=True, padding_mode:str='zeros', device=None,\n                 dtype=None)\n\nConvolutional neural network with a built in activation\n\nsource\n\n\nCNN\n\n CNN ()\n\nSix layer convolutional neural network\nGenerally, we want a high learning rate to come up with generalizable algorithms. Let‚Äôs start with the relatively high 0.6.\n\ndef train(model, extra_cbs=None):\n    cbs = [\n        MetricsCB(MulticlassAccuracy(num_classes=10)),\n        DeviceCB(),\n        ProgressCB(plot=True),\n    ]\n    if extra_cbs:\n        cbs.extend(extra_cbs)\n    learn = TrainLearner(\n        model,\n        fashion_mnist(),\n        F.cross_entropy,\n        lr=0.6,\n        cbs=cbs,\n    ).fit()\n    return learn\n\n\ntrain(model=CNN())\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.160\n2.911\n0\ntrain\n\n\n0.100\n2.305\n0\neval\n\n\n\n\n\n\n\n\nLet‚Äôs look at the underlying activations"
  },
  {
    "objectID": "activations.html#hooks",
    "href": "activations.html#hooks",
    "title": "Activation Statistics",
    "section": "Hooks",
    "text": "Hooks\nJeremy‚Äôs implementation is kind of a mess so I did a bit of refactoring. Hooks are just another kind of callback in the PyTorch universe, so we can adopt our Callback conventions.\n\nsource\n\nHook\n\n Hook (m, f)\n\nWrapper for a PyTorch hook, facilitating adding instance state\n\nsource\n\n\nHooksCallback\n\n HooksCallback (hook_f, mods=None, mod_filter=&lt;function noop&gt;,\n                on_train=True, on_valid=False)\n\nContainer for hooks with clean up and and options to target certain modules\nThat being implemented, we can subclass these for adding hook behaviors.\n\nsource\n\n\nStoreModuleStats\n\n StoreModuleStats (m, on_train=True, on_valid=False)\n\nA hook for storing the activation statistics\n\nsource\n\n\nStoreModuleStatsCB\n\n StoreModuleStatsCB (mods=None, mod_filter=&lt;function noop&gt;, on_train=True,\n                     on_valid=False)\n\nCallback for plotting the layer-wise activation statistics\nNow, we can rerun while keeping track of the activation stats\n\nmodel = CNN()\ntrain(\n    model=model,\n    extra_cbs=[StoreModuleStatsCB(mods=model.layers)],\n)\n\n\n\n\n\n\n\n\nMulticlassAccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.166\n2.275\n0\ntrain\n\n\n0.250\n1.999\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJeremy makes the point that his network isn‚Äôt training because the weights are close to 0, which makes them ‚Äúdead units.‚Äù\n‚ö†Ô∏è Generally, the mean should be 0 and the standard deviation should be close to 1.\nUltimately, Jeremy recommends simply abandoning any training run where the activation variance increases and crashes."
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Course overview",
    "section": "",
    "text": "Adapted from:"
  },
  {
    "objectID": "overview.html#motivation",
    "href": "overview.html#motivation",
    "title": "Course overview",
    "section": "Motivation",
    "text": "Motivation\nSay we want to generate images. Ideally, we would have a probability distribution of the pixels, \\(P\\). Let‚Äôs say that the probability distribution is for each \\(28^2\\) pixels. Note tha teach pixel is variables of the probability density function, or PDF for short).\nWe don‚Äôt have this, but say we had the derivative of this PDF. Recall that mutlivariate calculus is concerned with partial derivatives. For example, the partial derivatives of \\(f(x,y)=x^2 + y^2\\) are:\n\\[\n\\begin{align}\n\\frac{\\partial f(x,y)}{\\partial x} &= 2x \\\\\n\\frac{\\partial f(x,y)}{\\partial y} &= 2y \\\\\n\\end{align}\n\\]\nIn the case of an image, imagine the partial derivative of the function \\(P\\) with respect to a single pixel:\n\\[\n\\frac{\\partial P(X)}{\\partial X_{i,j}} = \\frac{P(X)-P(X + \\partial X_{i,j})}{\\partial X_{i,j}}\n\\]\nWe can use this idea to generate images.\nWe could start from a blurry image and ‚Äúmarch upwards‚Äù in the direction of the partial derivative.\n\nsource\n\nshow_image\n\n show_image (img, title=None, K=2)\n\n\nsource\n\n\nshow_images\n\n show_images (imgs, titles=None, K=2)\n\n\nmnist = load_dataset(\"mnist\")\nimg = np.array(mnist[\"train\"][0][\"image\"])\nimg = normalize(img)\nimg_blurry = img + np.random.normal(size=img.size).reshape(28, 28) * 0.2\nimg_very_blurry = img + np.random.normal(size=img.size).reshape(28, 28) * 0.8\nimgs = [img, img_blurry, img_very_blurry]\nshow_images(imgs)\n\nFound cached dataset mnist (/Users/jeremiahfisher/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 104.60it/s]\n\n\n\n\n\nAn algorithm could look something like this:\n\nFor all pixel values \\(X_{i,j} \\in X\\), evaluate the partial derivative \\(\\frac{P(X)-P(X + \\partial X_{i,j})}{\\partial X_{i,j}}\\) or \\(\\frac{\\partial P(X)}{\\partial X_{i,j}}\\). This can also be expressed as \\(\\nabla_X P\\)\nFor some hyperparameter constant \\(C\\) and for all pixel values \\(X_{i,j} \\in X\\), \\(X_{i,j} := X_{i,j} + C \\frac{\\partial P(X)}{\\partial X_{i,j}}\\) or, equivalently, \\(X := X + C \\cdot \\nabla_X P\\)\nRepeat until satisfied\n\nIn PyTorch, this would look something like:\nX = get_image()\n# Somewhat unusually, you would incorporate the image as a model\n# parameter in order to get auto-differentiation\nmodel = get_nabla_X_of_P_model(X=X)\nfor _ in range(n_timesteps):\n    p_grad = model.forward(X)\n    p_grad.backward()\n    model.X += C * model.X.grad\nIn fact, we don‚Äôt have \\(P(X)\\) or \\(\\nabla_X P(X)\\) in real life. But we can solve a related problem.\nNotice that \\(\\nabla_X P(X)\\) provides a direction from blurrier to sharper images. We can train a neural network to de-blur by adding the blur ourselves. The input-output pair would be \\(\\langle image + \\epsilon, \\epsilon \\rangle\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\) and \\(\\Sigma \\in \\mathbb{R}^{28^2 \\times 28^2}\\)"
  },
  {
    "objectID": "overview.html#an-aside-the-mathematics",
    "href": "overview.html#an-aside-the-mathematics",
    "title": "Course overview",
    "section": "An aside, the mathematics",
    "text": "An aside, the mathematics\nIn mathematical parlance, we seek to fit a set of reverse Markov transitions to maximize the likelihood of the model on the training data. Or ‚Äúminimizing the variational upper bound on the negative log likelihood.‚Äù Not sure what that mean exactly, but I‚Äôll be targetting the mathematical level of myself 6 months ago.\nFirst, we define some terms:\n\n\\(X^{(0)}\\) is the input data distribution; for example, MNIST digits\n\\(Q\\) and \\(P\\) are probability density functions\n\\(\\beta_t\\) is the ‚Äúnoise value‚Äù at time t\n\nWe can look to physics for inspiration in AI. This paper draws from the thermodynamics to imagine the opposite of a diffusion process: that is, evolving for a high-entropy distribution (like a noisy image) to a clear one.\nIn the diffusion process, we have a ‚Äúforward‚Äù Gaussian Markov process called \\(Q\\) that governs the transition to a noisier distribution. In nature, this is a Guassian distribution: \\[\nQ(X^{(t)} | X^{(t-1)}) = \\mathcal{N}(X^{(t-1)}\\sqrt{1-\\beta_t}, I\\beta_t)\n\\] Note that at \\(t=0\\), we haven‚Äôt added any noise and \\(B_0=0\\). In fact, there is a simple expected value:\n\\[\n\\begin{align}\nE\\left[\\mathcal{N}(X^{(t-1)}\\sqrt{1-\\beta_t}, I\\beta_t)\\right]\n&= E\\left[\\mathcal{N}(X^{(t-1)}\\sqrt{1-(0)}, I(0)\\right]\\\\\n&= E\\left[\\mathcal{N}(X^{(t-1)}, 0)\\right] \\\\\n&= X^{(t-1)}\n\\end{align}\n\\]\nFurthermore, note that adding noise is a simple process and we generally work with the ‚Äúanalytic‚Äù conditional distribution where we all the noise all at once. We‚Äôll go over this function later.\nThe important process to consider, however, is the ‚Äúbackward‚Äù Guassian Markov process called \\(P\\)\n\\[\nQ(X^{(t)} | X^{(t-1)}) = \\mathcal{N}(\\square(X^{(t-1)}), \\triangle(X^{(t-1)}))\n\\]\nWe want a function that maximizes the log-likelihood of this probability distribution on the data. Mathematically, this would involve the integral over the parameters and likelihood. This is mathematically intractable.\n(An aside, we use log-likelihood instead of likelihood because it increases monotonically and sums are more numerically stable on computers than products.)\nInstead of solving the integral directly, we use the Evidence Lower Bound. This is a score function of the model that balances maximizing the likelihood of the data under the model with the complexity of the model. \\[\nELBO = E\\left[log(P_\\theta(X))\\right] - KL(q(\\theta) || p(\\theta))\n\\] The expectation maximization term calculates the probability of the data using the PDF \\(P\\). The likelier the data under the model, the lower the loss. Simple.\nIn general, KL divergence measures the difference between distributions. In our case, it is a loss that should be minimized between the expected (or ‚Äúvariational‚Äù) weight distribution (\\(p\\)) and the actual (‚Äútrue posterior‚Äù) distribution (\\(q\\)) of the model weights themselves. Basically, we want to see the same distributions in the reverse process that we would see in the forward process.\nThe \\(||\\) is a notation for the computation: \\[\nKL(q || p) = \\int q(\\theta)log\\left( \\frac{q(\\theta)}{{p(\\theta)}} \\right) d\\theta\n\\] Let‚Äôs break this down.\n\nThe log ratio of the point values of \\(p\\) and \\(q\\) at \\(\\theta\\) is a measure of the difference at that point\n\nIf the probability distributions give the same likelihood for \\(\\theta\\), the resulting value is 0\nAs \\(q(\\theta)\\) goes 0 and \\(p(\\theta)\\) goes to 1, the limit of the resulting value diverges to negative infinity\nAs \\(q(\\theta)\\) goes 1 and \\(p(\\theta)\\) goes to 0, the limit diverges to positive infinity\n\n\\(q(\\theta)\\) is a weight for the integrand\n\nSo, in sum, the KL diverenge is a sum of the difference in probability weighted by the probability values of the variational distribution. If the distributions are similar, the KL divergence is small; otherwise, they are large.\nSee: https://www.assemblyai.com/blog/content/media/2022/05/KL_Divergence.mp4\nInterestingly, Ho et al fixes the variance schedule. In turns out, this makes it so that the covariance matrix is constant, the KL divergence term goes to 0 and we only need to estimate the mean.\nWe can train a model to do so with MSE loss.\nThis course will demonstrate how to do so from scratch.\nLet‚Äôs begin by running the algorithm.\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n# Use a simple noising scheduler for the initial draft\npipe.scheduler = LMSDiscreteScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    num_train_timesteps=1000,\n)\npipe = pipe.to(TORCH_DEVICE)\npipe.enable_attention_slicing()\nprompt = \"a photo of a giraffe in Paris\"\npipe(prompt).images[0]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:10&lt;00:00,  1.41s/it]\n\n\n\n\n\nNow that we see what Stable Diffusion is capable of, we note its three components:\n\nVariational Autoencoder\nCLIP\nUnet"
  },
  {
    "objectID": "overview.html#model-components",
    "href": "overview.html#model-components",
    "title": "Course overview",
    "section": "Model components",
    "text": "Model components\n\nVariational Autoencoder\nStable Diffusion is a latent diffusion model. That means that the model manipulates vectors within the latent space manifold of another model. In this case, that model is a Variational Autoencoder.\n\nvae = pipe.vae\n\nVariational autoencoders are trained to compress vector information into a normal distribution manifold and decompress it with minimal reconstruction loss.\n\n\n\nimage.png\n\n\nWe can visualize the information from a trained VAE.\n\nsource\n\n\nimage_from_url\n\n image_from_url (url:str)\n\n\nsource\n\n\ndecompress\n\n decompress (latents:torch.Tensor, vae:torch.nn.modules.module.Module,\n             as_pil=True, no_grad=True)\n\nProject latents into pixel space\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatents\nTensor\n\nVAE latents\n\n\nvae\nModule\n\nVAE\n\n\nas_pil\nbool\nTrue\nReturn a PIL image\n\n\nno_grad\nbool\nTrue\nDiscard forward gradientss\n\n\n\n\nsource\n\n\ncompress\n\n compress (img:PIL.Image.Image, vae:torch.nn.modules.module.Module)\n\nProject pixels into latent space\n\n\n\n\nType\nDetails\n\n\n\n\nimg\nImage\nInput image\n\n\nvae\nModule\nVAE\n\n\n\n\ninput_ = image_from_url(\n    \"https://www.perfectduluthday.com/wp-content/uploads/2021/12/Weird_Al_Yankovic_profile.jpg\"\n)\nlatents = compress(input_, vae)\noutput = decompress(latents, vae)\nshow_images([input_, *latents.squeeze().cpu(), output])\nn_params_img = np.product(input_.shape)\nn_params_latents = np.product(latents.shape)\nf\"Compression rate: {n_params_img/n_params_latents:.2f}x\"\n\n'Compression rate: 49.13x'\n\n\n\n\n\nWe get 50x compression with no loss in detail.\nNote that, since we‚Äôre dealing with noise, that noise in the latent space does not correspond to Gaussian noise in the pixel space.\n\nnoise = torch.randn(latents.shape, device=latents.device)\nnoised_latents = latents * 0.6 + noise * 0.4\nimg = decompress(noised_latents, vae)\nimg\n\n\n\n\n\n\nU-net\nU-nets are the model used for denoising\n\nunet = pipe.unet\n\nWe‚Äôll get much more into the architecture of the U-net later. For now, know that it is a model with a strong translational equivariance property.\nThere are two inputs to the model: the prompt and the time.\nThe time is an index of how noisy the image is. This helps the model to determine how much noise to remove, because the amount of noise added at any given step is non-linear.\n\nscheduler = pipe.scheduler\nscheduler\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.17.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null,\n  \"use_karras_sigmas\": false\n}\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nX = timesteps = scheduler.timesteps.to(torch.uint8).cpu().numpy()\nY = scheduler.betas.cpu().numpy()[timesteps]\nax.scatter(X, Y, marker=\"+\")\nax.set(xlabel=\"Time\", ylabel=r\"$\\beta$\")\n\n[Text(0.5, 0, 'Time'), Text(0, 0.5, '$\\\\beta$')]\n\n\n\n\n\n\n\nCLIP\nThe other thing we could add to make the problem of de-noising easier is to indicate the image class (1, 2, 3, etc). For a simple class distribution, we could just one hot-encode it. The input-output pair would then be \\(\\langle \\left( image + \\epsilon, t, class \\right), \\epsilon \\rangle\\).\nBut we cannot one-hot encode the distribution of images on the internet √† la Stable Diffusion. Therefore, we need a more sophisticated encoder: CLIP (Constrastively Learned Image Pairs). This works on the idea that the dot-product between image encoding and text encoding of the same thing should be large, while the image encoding and text encoding for different things should be small.\nWe learn this with a neural network contrastively. For a given batch, \\(B\\), of (image, language) pairs from html alt tags:\n\nCompute the encoding \\(f_{image}(I_i)\\) and \\(f_{language}(L_i)\\) for all \\(i \\in |B|\\)\nCompute the sum \\(\\text{correctly paried loss} := \\Sigma_i^{|B|} f_{image}(I_i) \\cdot f_{language}(L_i)\\)\nCompute the sum\n\n\\[\n\\text{incorrectly paired loss} := \\Sigma_i^{|B|} \\Sigma_j^{|B|}  \\begin{cases}\n    f_{image}(I_i) \\cdot f_{language}(L_j) & i \\neq j\\\\\n    0 & i = j\n  \\end{cases}\n\\]\n\nFinal loss = incorrectly paired loss - correctly paired loss. Note that want the overall loss to be small or negative, so we take the negative of the sum of the correctly paired dot products. This pushes the vectors for correctly paired language image to be in the same subspace, and incorrectly paired counterparts into different subspaces.\n\n\nThe transformers library has a module for this\n\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\ntokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\ntokens\n\ntensor([[49406,   320,  1125,   539,   320, 22826,   530,  3445, 49407]])\n\n\nWe can go backwards like so:\n\nfor t in tokens[0, :8]:\n    print(f\"{t}: {tokenizer.decoder.get(int(t))}\")\n\n49406: &lt;|startoftext|&gt;\n320: a&lt;/w&gt;\n1125: photo&lt;/w&gt;\n539: of&lt;/w&gt;\n320: a&lt;/w&gt;\n22826: giraffe&lt;/w&gt;\n530: in&lt;/w&gt;\n3445: paris&lt;/w&gt;\n\n\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(tokens.to(TORCH_DEVICE)).last_hidden_state\ntext_embeddings.shape\n\ntorch.Size([1, 9, 768])\n\n\nIn fact, this isn‚Äôt used to directly denoise the image during inference. We use a hack called Classifier Free Guidance (CFG), where ‚Äì in addition to the latents ‚Äì the model is prompted with a null prompt (an unconditional prompt) and and the original prompt.\nSee more here: https://www.youtube.com/watch?v=344w5h24-h8"
  },
  {
    "objectID": "overview.html#putting-it-together",
    "href": "overview.html#putting-it-together",
    "title": "Course overview",
    "section": "Putting it together",
    "text": "Putting it together\nWith these components in mind, we can start to put them together.\n\nsource\n\nStableDiffusion\n\n StableDiffusion\n                  (tokenizer:transformers.models.clip.tokenization_clip.CL\n                  IPTokenizer, text_encoder:transformers.models.clip.model\n                  ing_clip.CLIPTextModel, scheduler:Any, unet:diffusers.mo\n                  dels.unet_2d_condition.UNet2DConditionModel, vae:diffuse\n                  rs.models.autoencoders.autoencoder_kl.AutoencoderKL)\n\n\nsd = StableDiffusion(\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler,\n    unet=unet,\n    vae=vae,\n)\nsd(prompt, n_inference_steps=30, as_pil=True)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:40&lt;00:00,  1.35s/it]"
  },
  {
    "objectID": "overview.html#sampling",
    "href": "overview.html#sampling",
    "title": "Course overview",
    "section": "Sampling",
    "text": "Sampling\nThere are many ways to sample from Stable Diffusion. The original sampling was a simple numerical differential equation solver (known as the linear multistep solver or LMS): where we take the steps that the gradient suggests with the magnitude of the solver.\nIt should be noted that this is similar to solving for neural network weights. Therefore, we take a step accourding to the optimizer and wait for convergence.\nWe can also use the tricks associated with successful neural network training. It is well-understood that the derivative isn‚Äôt always a good indicator of the loss curvature. We can improve the quality of the right direction by incorporating previous computations of the gradient (i.e., momentum).\nRead more about that here: https://stable-diffusion-art.com/samplers/"
  },
  {
    "objectID": "overview.html#hacking",
    "href": "overview.html#hacking",
    "title": "Course overview",
    "section": "Hacking",
    "text": "Hacking\n\nOptimizing pixels directly\nWe are not constrained to using the algorithms as presented to us. This is all just calculus, so we can optimize with respect to arbirary loss functions.\n\n@dataclass\nclass StableDiffusionWithArbitraryLoss(StableDiffusion):\n    loss_f: Callable\n    k: float\n    periodicity: int = 5\n\n    def denoise(self, prompt_embedding, l, t, guidance_scale, i):\n        if i % self.periodicity == 0 and i != 0:\n            # Calculate noise as per usual\n            noise_pred = self.pred_noise(prompt_embedding, l, t, guidance_scale)\n\n            # Create a copy of the latents that keeps track of the gradients\n            l = l.detach().requires_grad_()\n\n            # Take a step all the way towards a predicted x0 and use this to\n            # compute the loss\n            l_x0 = self.scheduler.step(noise_pred, t, l).pred_original_sample\n            image_x0 = decompress(l_x0, self.vae, as_pil=False, no_grad=False)\n            loss = self.loss_f(image_x0) * self.k\n            print(f\"{i}: arbritrary loss: {loss}\")\n\n            # Compute the loss gradient with respect to the latents and take\n            # a step in that direction\n            (grad,) = torch.autograd.grad(loss, l)\n            sigma = self.scheduler.sigmas[i]\n            l = l.detach() - grad * sigma**2\n        else:\n            with torch.no_grad():\n                noise_pred = self.pred_noise(prompt_embedding, l, t, guidance_scale)\n\n        l = self.scheduler.step(noise_pred, t, l).prev_sample\n\n        return l\n\n\ndef blue_loss(images):\n    return torch.abs(images[:, 2] - 0.9).mean()\n\n\nStableDiffusionWithArbitraryLoss(\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler,\n    unet=unet,\n    vae=vae,\n    loss_f=blue_loss,\n    k=75,\n    periodicity=5,\n)(\n    \"a photo of an octopus, national geographic, dlsr\",\n    n_inference_steps=30,\n    as_pil=True,\n)\n\n 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                        | 5/30 [00:06&lt;00:33,  1.33s/it]/Users/jeremiahfisher/miniforge3/envs/slowai/lib/python3.9/site-packages/torch/autograd/__init__.py:303: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                               | 10/30 [00:40&lt;01:21,  4.09s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                       | 15/30 [01:18&lt;01:09,  4.65s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                               | 20/30 [02:02&lt;00:53,  5.37s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 25/30 [02:42&lt;00:24,  4.96s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [03:27&lt;00:00,  6.91s/it]\n\n\n5: arbritrary loss: 40.17461013793945\n10: arbritrary loss: 27.197010040283203\n15: arbritrary loss: 21.870607376098633\n20: arbritrary loss: 21.79729652404785\n25: arbritrary loss: 21.87038230895996\n\n\n\n\n\nLook! Less blue than you would expect for an underwater scene. Cool üòé\n\n\nTinkering with the prompt embedding\nWe are also not constrained to the text embeddings directly from CLIP. We can take embeddings from different tokens and hack them apart.\n\nprompt = \"an adorable photo of a puppy\"\ntokens_a = tokenizer(prompt, return_tensors=\"pt\").input_ids\nfor t in tokens_a[0, ...]:\n    print(f\"{t}: {tokenizer.decoder.get(int(t))}\")\ntext_embeddings_a = text_encoder(tokens_a.to(TORCH_DEVICE)).last_hidden_state\ntext_embeddings_a.shape\n\n49406: &lt;|startoftext|&gt;\n550: an&lt;/w&gt;\n6298: adorable&lt;/w&gt;\n1125: photo&lt;/w&gt;\n539: of&lt;/w&gt;\n320: a&lt;/w&gt;\n6829: puppy&lt;/w&gt;\n49407: &lt;|endoftext|&gt;\n\n\ntorch.Size([1, 8, 768])\n\n\n\nprompt = \"a adorable photo of a koala\"\ntokens_b = tokenizer(prompt, return_tensors=\"pt\").input_ids\nfor t in tokens_b[0, ...]:\n    print(f\"{t}: {tokenizer.decoder.get(int(t))}\")\nwith torch.no_grad():\n    text_embeddings_b = text_encoder(tokens_b.to(TORCH_DEVICE)).last_hidden_state\ntext_embeddings_b.shape\n\n49406: &lt;|startoftext|&gt;\n320: a&lt;/w&gt;\n6298: adorable&lt;/w&gt;\n1125: photo&lt;/w&gt;\n539: of&lt;/w&gt;\n320: a&lt;/w&gt;\n36654: koala&lt;/w&gt;\n49407: &lt;|endoftext|&gt;\n\n\ntorch.Size([1, 8, 768])\n\n\n\ncombined_text_embeddings = (text_embeddings_a + text_embeddings_b) / 2\n\n\nclass PuppyKoala(StableDiffusion):\n    def embed_prompt(self, _: str) -&gt; torch.tensor:\n        global combined_text_embeddings\n        _, max_length, _ = combined_text_embeddings.shape\n        uncond_input = self.tokenizer(\n            [\"\"],\n            padding=\"max_length\",\n            max_length=max_length,\n            return_tensors=\"pt\",\n        )\n        with torch.no_grad():\n            uncond_embeddings = self.text_encoder(\n                uncond_input.input_ids.to(TORCH_DEVICE)\n            )\n            uncond_embeddings = uncond_embeddings[0]\n        return torch.cat([uncond_embeddings, combined_text_embeddings])\n\n\nsd = PuppyKoala(\n    tokenizer=tokenizer,\n    text_encoder=text_encoder,\n    scheduler=scheduler,\n    unet=unet,\n    vae=vae,\n)\n\nsd(prompt, n_inference_steps=30, as_pil=True)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:58&lt;00:00,  1.96s/it]"
  },
  {
    "objectID": "convs.html",
    "href": "convs.html",
    "title": "Convolutions",
    "section": "",
    "text": "Adapted from:\ndsd = load_dataset(\"mnist\")\ntrn, tst = dsd[\"train\"], dsd[\"test\"]\nxb = T.to_tensor(trn[0][\"image\"])[None, ...]\nxb.shape\n\ntorch.Size([1, 1, 28, 28])\nThis is a nice resource.\nConvolutions encode position equivariance like the probability that there is a bird at any particular location in an image. It‚Äôs implemented as a sliding matrix multiplication, followed by a sum like so:\nkernel = tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n\n\ndef apply_kernel(row, col, img, kernel):\n    # Normally, you would need to check if the kernel has\n    # an odd or even size\n    width, height = kernel.shape\n    receptive_field = img[\n        row - (height // 2) : row + (height // 2) + 1,\n        col - (width // 2) : col + (width // 2) + 1,\n    ]\n    return (receptive_field * kernel).sum()\n\n\n*_, w, h = xb.shape\nprocessed = [\n    [apply_kernel(i, j, xb.squeeze(), kernel) for j in range(1, w - 1)]\n    for i in range(1, h - 1)\n]\nprocessed = tensor(processed)\n\nshow_images(\n    [kernel, xb.squeeze(), processed],\n    titles=[\"Kernel\", \"Original\", \"Original + Kernel\"],\n    figsize=(8, 8),\n);\nYou can see it is very easy to think of interesting features of images that are just convolutional filters.\nOne clever way of implementing this is the im2col algorithm that represents a convolution as a matrix multiplication and take advantage of highly efficient algorithms for matrix multiplications. It does so by unrolling the input matrix matrix such that every receptive field is a contiguous block of values, with a correspondingly unrolled contiguous block of convolutional filter values. The values of the convolutional matrix can change, but they are shared.\nThis has a corresponding function in PyTorch.\nF.unfold?\n\n\nSignature:\nF.unfold(\n    input: torch.Tensor,\n    kernel_size: None,\n    dilation: None = 1,\n    padding: None = 0,\n    stride: None = 1,\n) -&gt; torch.Tensor\nDocstring:\nExtracts sliding local blocks from a batched input tensor.\n.. warning::\n    Currently, only 4-D input tensors (batched image-like tensors) are\n    supported.\n.. warning::\n    More than one element of the unfolded tensor may refer to a single\n    memory location. As a result, in-place operations (especially ones that\n    are vectorized) may result in incorrect behavior. If you need to write\n    to the tensor, please clone it first.\nSee :class:`torch.nn.Unfold` for details\nFile:      ~/micromamba/envs/slowai/lib/python3.11/site-packages/torch/nn/functional.py\nType:      function\n# Need to add a channel dimension and a batch dimension\nxb_unfolded = F.unfold(xb, kernel_size=(3, 3)).float()\nkernel_unfolded = kernel.view(-1).float()\n\n# im2col\nxb_processed = kernel_unfolded @ xb_unfolded\n\n# Reshape\nxb_processed = xb_processed.view(26, 26)\nshow_image(xb_processed);\nThis unfold trick is about the same efficiency as the built-in convolution layer.\nFor better performance, we can apply a bunch of convolutions simultaneously.\ndiag1_edge = tensor([[0, -1, 1], [-1, 1, 0], [1, 0, 0]]).float()\ndiag2_edge = tensor([[1, -1, 0], [0, 1, -1], [0, 0, 1]]).float()\nleft_edge = tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]).float()\n\nedge_kernels = torch.stack([left_edge, diag1_edge, diag2_edge])\nedge_kernels.shape\n\ntorch.Size([3, 3, 3])\nbatch_features = F.conv2d(xb, edge_kernels[:, None, ...])\nshow_images([batch_features[0, i] for i in range(3)])\nThe parameters to a convolutional layer are the stride and padding, which allow us to manipulate the size of the feature map."
  },
  {
    "objectID": "convs.html#creating-a-cnn",
    "href": "convs.html#creating-a-cnn",
    "title": "Convolutions",
    "section": "Creating a CNN",
    "text": "Creating a CNN\nFor classifying digits, we cannot solely use convolutional layers because it gives 10 outputs per pixel!\n\nnh, n_outputs = 30, 10\n\npartial_model = torch.nn.Sequential(\n    torch.nn.Conv2d(1, 30, kernel_size=3, padding=1),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(30, 10, kernel_size=3, padding=1),\n)\n\npartial_model(xb).shape\n\ntorch.Size([1, 10, 28, 28])\n\n\nInstead, we‚Äôll add a lot of layers and chew up the extra feature map dimensions until we have a 1x1x10 feature map.\nWe‚Äôll add a convenience function to create conv layers with optional activations.\n\nsource\n\nconv\n\n conv (ni, nf, ks=3, stride=2, act=True)\n\nWe also need some annoying device stuff.\n\nsource\n\n\nto_device\n\n to_device (x, device='cpu')\n\nNow, define the model‚Ä¶\nNote here that the default stride is 2, such that the feature map is downsampled by 2 each layer.\n\nsource\n\n\nget_model\n\n get_model ()\n\nAnd we can train! üèãÔ∏è\n\nsource\n\n\nget_dls_from_dataset_dict\n\n get_dls_from_dataset_dict (dsd, collate_fn=&lt;function default_collate&gt;,\n                            bs=32)\n\n\nsource\n\n\nfit\n\n fit (epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False)\n\n\nsource\n\n\naccuracy\n\n accuracy (y, y_pred)\n\n\nsource\n\n\nfashion_mnist\n\n fashion_mnist (bs=256)\n\n\nsource\n\n\nfashion_collate\n\n fashion_collate (examples)\n\n\nmodel = get_model()\nwith fashion_mnist() as dls:\n    fit(6, model, F.cross_entropy, optim.SGD(model.parameters(), lr=0.1), *dls)\n\nepoch=0, validation loss=1.041, validation accuracy=0.62\nepoch=1, validation loss=0.767, validation accuracy=0.73\nepoch=2, validation loss=0.623, validation accuracy=0.78\nepoch=3, validation loss=0.561, validation accuracy=0.80\nepoch=4, validation loss=0.515, validation accuracy=0.81\nepoch=5, validation loss=0.483, validation accuracy=0.82\n\n\nThis gives us comparable accuracy to the linear model, which had 39,760 parameters. In contrast‚Ä¶\n\nsum(p.numel() for p in model.parameters())\n\n5274\n\n\nMore tips:\n\nChewing up the feature map until we have one logit per dimension is not the only way to summarize the features before a classification/regression head. We can also use a dense layer or global average pooling\nThe receptive field of a convolution grows through the network, as the convolutions are functions of convolutions. Really nice illustration of that here"
  },
  {
    "objectID": "initializations.html",
    "href": "initializations.html",
    "title": "Initializations",
    "section": "",
    "text": "Adapted from:\nset_seed(42)\nplt.style.use(\"ggplot\")"
  },
  {
    "objectID": "initializations.html#baseline",
    "href": "initializations.html#baseline",
    "title": "Initializations",
    "section": "Baseline",
    "text": "Baseline\nLet‚Äôs look at a fashion MNIST classification problem."
  },
  {
    "objectID": "diving_deeper.html",
    "href": "diving_deeper.html",
    "title": "Diving Deeper",
    "section": "",
    "text": "Adapted from:"
  },
  {
    "objectID": "diving_deeper.html#matrix-notation-and-playing-with-data",
    "href": "diving_deeper.html#matrix-notation-and-playing-with-data",
    "title": "Diving Deeper",
    "section": "Matrix notation and playing with data",
    "text": "Matrix notation and playing with data\n\nGet the data\n\nsource\n\n\ndownload_mnist\n\n download_mnist (path_gz=Path('data/mnist.pkl.gz'))\n\n\nsource\n\n\ndownload_file\n\n download_file (url, destination)\n\n\ndata_fp = download_mnist()\ndata_fp\n\nPosixPath('data/mnist.pkl.gz')\n\n\n\nwith gzip.open(data_fp, \"rb\") as f:\n    data = pickle.load(f, encoding=\"latin-1\")\n    ((X_trn, y_trn), (X_vld, y_vld), _) = data\nX_trn.shape\n\nNote that \\(784 = 28^2\\)\n\nfig, ax = plt.subplots(1, 1)\nax.imshow(X_trn[0].reshape(28, 28))\n\n&lt;matplotlib.image.AxesImage at 0x172b266a0&gt;\n\n\n\n\n\n\n\nWriting a matrix class\n\n@dataclass\nclass Matrix:\n    xs: List[List[float]]\n\n    def __getitem__(self, idxs):\n        x, y = idxs\n        return self.xs[x][y]\n\nThis is implemented by PyTorch (along with all the auto-differentiation stuff)\n\ntorch.Tensor?\n\n\nInit signature: torch.Tensor(self, /, *args, **kwargs)\nDocstring:      &lt;no docstring&gt;\nFile:           ~/miniforge3/envs/slowai/lib/python3.9/site-packages/torch/__init__.py\nType:           _TensorMeta\nSubclasses:     Parameter, UninitializedBuffer, FakeTensor, MaskedTensor"
  },
  {
    "objectID": "diving_deeper.html#apl-functionality",
    "href": "diving_deeper.html#apl-functionality",
    "title": "Diving Deeper",
    "section": "APL functionality",
    "text": "APL functionality\nDefining a tensor (or ‚Äúarrays,‚Äù using their own terminology), a.\na ‚É™ 3 5 6\nMultiplying by a scalar\na ‚®â 3\nElement-wise division\nb ‚É™ 7 8 9\na √∑ b\nNumpy was influenced by APL, PyTorch was influenced by numpy.\nOne thing that differs is that scalars are just ‚Äú1-rank‚Äù tensors in numpy, whereas they have special scalars have special semantics in numpy."
  },
  {
    "objectID": "learner.html",
    "href": "learner.html",
    "title": "Learner",
    "section": "",
    "text": "Adapted from:\nAt this point, Jeremy points out that copying and pasting code leads to bottlenecks in modeling velocity. We need to start to build a framework to:"
  },
  {
    "objectID": "learner.html#data",
    "href": "learner.html#data",
    "title": "Learner",
    "section": "Data",
    "text": "Data\nWe‚Äôll start with a wrapper around datasets to make it simpler to work with raw PyTorch.\n\nsource\n\nDataLoaders\n\n DataLoaders (splits, nworkers:int=0, bs=32, collate_fn=&lt;function\n              default_collate&gt;, tdir=&lt;TemporaryDirectory\n              '/tmp/tmprf8dt6h0'&gt;)\n\nWrapper around huggingface datasets to facilitate raw pytorch work\n\ndls = DataLoaders.from_hf(\"fashion_mnist\")\n\nWe should also add some helpers to facilitate processing images.\n\nsource\n\n\ntensorize_images\n\n tensorize_images (dls, feature='image', normalize=True)\n\nTensorize and normalize the image feature\n\nsource\n\n\nbatchify\n\n batchify (f)\n\nConvert a function that processes a single feature to processing a list of features\n\ndls = tensorize_images(dls)\nxb = dls.peek()[\"image\"]\nshow_images(xb[:8, ...], figsize=(8, 4))\n\n\n\n\nNotice that this unit-normalized\n\nplt.hist(xb.view(-1))\n\n(array([13418.,   607.,   687.,  1014.,  1057.,  1076.,  1408.,  2054.,\n         2393.,  1374.]),\n array([-0.82342112, -0.54524243, -0.26706368,  0.01111503,  0.28929374,\n         0.56747246,  0.84565115,  1.12382984,  1.40200865,  1.68018734,\n         1.95836604]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "learner.html#learner-and-callbacks",
    "href": "learner.html#learner-and-callbacks",
    "title": "Learner",
    "section": "Learner and callbacks",
    "text": "Learner and callbacks\nNext, we‚Äôll add a learner with callbacks. Recall, this was our earlier fit function:\n\nfit??\n\n\nSignature: fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False):\n    progress = tqdm if tqdm_ else lambda x: x\n    for epoch in range(epochs):\n        model.train()\n        for batch in progress(train_dl):\n            xb, yb = map(to_device, batch)\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        model.eval()\n        with torch.no_grad():\n            tot_loss, tot_acc, count = 0.0, 0.0, 0\n            for batch in progress(valid_dl):\n                xb, yb = map(to_device, batch)\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred, yb).item() * n\n                tot_acc += accuracy(pred, yb).item() * n\n        print(\n            f\"{epoch=}, validation loss={tot_loss / count:.3f}, validation accuracy={tot_acc / count:.2f}\"\n        )\n    return tot_loss / count, tot_acc / count\nFile:      ~/Desktop/SlowAI/nbs/slowai/convs.py\nType:      function\n\n\n\nTo add callbacks, we need a few clever Exception control flow signals\n\nsource\n\nCancelEpochException\nSkip to the next epoch\n\nsource\n\n\nCancelBatchException\nSkip to the next batch\n\nsource\n\n\nCancelFitException\nExit fit context\nThen, we define the learner and callback classes\n\nsource\n\n\nCallback\n\n Callback ()\n\nModify the training behavior\n\nsource\n\n\nwith_cbs\n\n with_cbs (nm)\n\nRun the callbacks lifecycle at the apropriate time\n\nsource\n\n\nLearner\n\n Learner (model, dls, loss_func=&lt;function mse_loss&gt;, lr=0.1, cbs=None,\n          opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nFlexible training loop\nThis learner delegates all aspects of model training to callbacks, so something like this is neccesary.\n\nsource\n\n\nTrainCB\n\n TrainCB ()\n\nTraining specific behaviors for the Learner\nNow that we have the basic scaffolding, we‚Äôll add metrics. Updating and storing state will be handled by torchmetrics, but we‚Äôll define a callback to orchestrate the torchmetrics instances.\n\nsource\n\n\nMetricsCB\n\n MetricsCB (*ms, **metrics)\n\nUpdate and print metrics\nFinally, we can define a Trainer callback specifically for the autoencoder objective.\n\nclass TrainAutoencoderCB(TrainCB):\n    \"\"\"Modify the training loop for the ELBO objective\"\"\"\n\n    def predict(self, learn):\n        xb, *_ = learn.batch\n        learn.preds = learn.model(xb)\n\n    def get_loss(self, learn):\n        xb, *_ = learn.batch\n        learn.loss = learn.loss_func(learn.preds, xb)\n\nLet‚Äôs also define some additional useful callbacks and dataset helpers:\n\nsource\n\n\nProgressCB\n\n ProgressCB (plot=False)\n\nReport the progress\n\nsource\n\n\nafter\n\n after (callback_cls:Type[__main__.Callback])\n\nRun a callback after another callback\n\nsource\n\n\nDeviceCB\n\n DeviceCB (device='cpu')\n\nMove tensors and model to the CPU/GPU/etc\n\nsource\n\n\nto_cpu\n\n to_cpu (x)\n\n\nsource\n\n\nfashion_mnist\n\n fashion_mnist ()\n\nHelper to use fashion MNIST\nPutting it all together\n\nmodel = get_ae_model()\ndls = fashion_mnist()\ncbs = [\n    MetricsCB(),\n    DeviceCB(),\n    TrainAutoencoderCB(),\n    ProgressCB(plot=True),\n]\nlearn = Learner(\n    model,\n    dls,\n    F.mse_loss,\n    lr=0.01,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.939\n0\ntrain\n\n\n0.776\n0\neval\n\n\n0.747\n1\ntrain\n\n\n0.716\n1\neval\n\n\n\n\n\n\n\n\nCPU times: user 24.7 s, sys: 1.6 s, total: 26.3 s\nWall time: 28.7 s\n\n\n\ndef viz(model, xb):\n    xb = xb.to(def_device)\n    pred = model(xb)\n    paired = []\n    for i in range(min(xb.shape[0], 8)):\n        paired.append(xb[i, ...])\n        paired.append(pred[i, ...])\n    show_images(paired, figsize=(8, 8))\n\n\nxbt, _ = dls.peek(\"test\")\nviz(model, xbt)\n\n\n\n\nStill not good, but less code!\nI don‚Äôt really like the idea of delegating the core training functions to callbacks, so we can just implement them here:\n\nsource\n\n\nTrainLearner\n\n TrainLearner (model, dls, loss_func=&lt;function mse_loss&gt;, lr=0.1,\n               cbs=None, opt_func=&lt;class 'torch.optim.sgd.SGD'&gt;)\n\nSane training loop\nThis works pretty similarly\n\nclass AutoencoderTrainer(TrainLearner):\n    def predict(self):\n        xb, *_ = self.batch\n        self.preds = self.model(xb)\n\n    def get_loss(self):\n        xb, *_ = self.batch\n        self.loss = self.loss_func(self.preds, xb)\n\n\ncbs = [MetricsCB(), DeviceCB(), ProgressCB(plot=True)]\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=0.01,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.941\n0\ntrain\n\n\n0.757\n0\neval\n\n\n0.737\n1\ntrain\n\n\n0.712\n1\neval\n\n\n\n\n\n\n\n\nCPU times: user 22.9 s, sys: 81 ms, total: 23 s\nWall time: 23 s\n\n\nCan we improve the reconstruction? Let‚Äôs implement a simple momentum.\n\nclass MomentumCB(Callback):\n    def __init__(self, momentum=0.85):\n        self.momentum = momentum\n\n    def zero_grad(self, learn):\n        with torch.no_grad():\n            for p in learn.model.parameters():\n                p.grad *= self.momentum\n\n\ncbs = [MetricsCB(), DeviceCB(), ProgressCB(plot=True), MomentumCB()]\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=0.01,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\nviz(model, xbt)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.232\n0\ntrain\n\n\n1.193\n0\neval\n\n\n1.175\n1\ntrain\n\n\n1.146\n1\neval\n\n\n\n\n\n\n\n\nCPU times: user 22.6 s, sys: 91.6 ms, total: 22.7 s\nWall time: 22.7 s\n\n\n\n\n\nNot especially impressive.\nWhat about using the automated learning rate finder?\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\nsource\n\n\nLRFinderCB\n\n LRFinderCB (gamma=1.3, max_mult=3)\n\nFind an apopriate learning rate by increasing it by a constant factor for each batch until the loss diverges\n\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=1e-5,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).lr_find()\n\n\n\n\n\n\n    \n      \n      20.00% [2/10 00:17&lt;01:10]\n    \n    \n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.274\n0\ntrain\n\n\n1.090\n1\ntrain\n\n\n\n\n\n    \n      \n      63.33% [19/30 00:05&lt;00:03 2.001]\n    \n    \n\n\n/home/jeremy/micromamba/envs/slowai/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered `nan` values in tensor. Will be removed.\n  warnings.warn(*args, **kwargs)  # noqa: B028\n\n\n\n\n\nIt looks like 1e-2 is a good learning rate.\n\ncbs = [MetricsCB(), DeviceCB(), ProgressCB(plot=True), MomentumCB()]\nlearn = AutoencoderTrainer(\n    get_ae_model(),\n    dls,\n    F.mse_loss,\n    lr=1e-2,\n    cbs=cbs,\n    opt_func=torch.optim.AdamW,\n).fit(2)\nviz(model, xbt)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.923\n0\ntrain\n\n\n0.640\n0\neval\n\n\n0.617\n1\ntrain\n\n\n0.598\n1\neval\n\n\n\n\n\n\n\n\n\n\n\nAgain, not especially impressive.\nWe‚Äôll write some tool to diagnose model issues in the next notebook."
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Calculus and Backprop",
    "section": "",
    "text": "Adapted from:\nThe better calculus pedagogy is the calculus of infintesimals, that is: assume \\(f'(x) = \\frac{f(x + \\Delta x)-f(x)}{\\Delta x}\\) like normal and ignore second order infinitesimals (i.e., infinitesimals of infinitesimals). By doing so, the main rules of arithmetic suddenly also apply to calculus. For example:"
  },
  {
    "objectID": "calculus.html#neural-networks",
    "href": "calculus.html#neural-networks",
    "title": "Calculus and Backprop",
    "section": "Neural Networks",
    "text": "Neural Networks\nHow does this relate to deep learning? Suppose we wanted to model this function with a neural network.\n\nx = np.linspace(0, 10, 100)\ny = (x - 2.5) ** 2\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(x, y);\n\n\n\n\nA single line wouldn‚Äôt be especially adequate for this. (The sum of lines is just a line.) But what about the sum of rectified lines?\n\ny1 = np.clip(-3 * (x - 2), a_min=0, a_max=None)\ny2 = np.clip(3 * (x - 3), a_min=0, a_max=None)\ny3 = np.clip(6 * (x - 5), a_min=0, a_max=None)\n\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(6, 3))\nax0.plot(x, y)\nax0.plot(x, y1)\nax0.plot(x, y2)\nax0.plot(x, y3)\nax1.plot(x, y)\nax1.plot(x, y1 + y2 + y3);\n\n\n\n\nThis is the idea of neural networks (except, instead of lines, we‚Äôre dealing with hyperplanes).\n\nBasic architecture\nLet‚Äôs consider a specific problem. Suppose we wanted to classify digits with a simple neural network.\n\nsource\n\n\nMNISTDataModule\n\n MNISTDataModule (bs=128)\n\nMNIST data Module\nIt‚Äôs a bit cleaner to deal with images as vectors for this exercise.\n\nX_trn = rearrange(X_trn, \"b h w -&gt; b (h w)\")\n\nSay we wanted to classify a digit as a ‚Äúseven‚Äù (or not) based on a single pixel. A trained linear model would find some coefficient and you would draw some line dividing sevens from non-sevens.\nOf course, this is pretty limiting. What if this surface had a lot of curvature?\n\nx = np.linspace(0, 1, 100)\ny_t = 1 - x**2\ny_pred = -0.95 * x + 1.1\nfig, ax = plt.subplots(figsize=(3, 3))\nax.plot(x, y_t, label=\"True\")\nax.plot(x, y_pred, label=\"Pred\")\nax.set(xlabel=\"Pixel Intensity\", ylabel=\"P(Seven)\")\nfig.legend();\n\n\n\n\nHow do fit the sum of rectified lines like before?\nTo make this more interesting, let‚Äôs consider using all pixels, for all images.\nLet‚Äôs define some parameters and helpers.\n\ndef relu(x):\n    return np.clip(x, a_min=0, a_max=None)\n\n\nn, m = X_trn.shape\nnh = 50  # num. hidden dimensions\nn, m\n\n(60000, 784)\n\n\nOur results are going to be non-sense here, but this gives us the right dimensions for everything.\n\nW0 = np.random.randn(m, nh)\nb0 = np.zeros(nh)\nW1 = np.random.randn(nh, 1)\nb1 = np.zeros(1)\n\nl0 = X_trn @ W0 + b0\nl1 = relu(l0)\ny_pred = l1 @ W1 + b1\ny_pred[:5]\n\narray([[ -9.87308795],\n       [-24.17324311],\n       [ 32.42885041],\n       [  6.0755439 ],\n       [-65.67629667]])\n\n\nLet‚Äôs compute an regression loss to get the model to predict the label. (This isn‚Äôt formally apropriate but it gives us the intuition.)\n\ny_pred.shape, y_true.shape\n\n((60000, 1), (60000,))\n\n\n\ndiff = y_pred.T - y_true\nmse = (diff**2).mean()\nmse\n\n1979.1768273281134\n\n\nEventually, at the end of the forward pass, we end up with a single number. We compute the loss for each example in the batch and take the batchwise mean or sum of the loss. Then, we use this along with the gradients with respect to each parameter to update the weights,\nLet‚Äôs calculate these derivates, one by one ü§©\n\n\nMean Squared Error\nFirst, we need to determine the gradient of the loss with respect to it‚Äôs input.\n\\[MSE = \\frac{ \\sum_{i=1}^{N} ( y^i-a^i )^2 }{N}\\]\nThis function composes an inner difference function and an outer square function. By the chain rule:\n\\[\n\\frac{d}{dx} f(g(x,y)) = f'(g(x,y)) g'(x,y)\n\\]\nLet \\(g(x,y) = x-y\\) and \\(f(x) = \\frac{x^2}{n}\\)\nThus, \\(\\frac{d}{dx} g(x,y)=\\frac{dx}{dx} - \\frac{dy}{dx}=1\\) and \\[\n\\begin{align*}\nf'(g(x, y)) & = f'((x-y)^2 / n) \\\\\n            & = f'((x^2 - 2xy + y^2)/n) \\\\\n            & = (2x - 2y) / n\n\\end{align*}\n\\]\nLet‚Äôs verify:\n\nassert n == 60000\nx, y = sympy.symbols(\"x y\")\nsympy.diff(((x - y) ** 2) / n, x)\n\n\\(\\displaystyle \\frac{x}{30000} - \\frac{y}{30000}\\)\n\n\nGreat. Now, to implement this in code, we need a way to store gradients on tensors themselves.\n\n@dataclass\nclass T:\n    \"\"\"Wrapper for numpy arrays to help store a gradient\"\"\"\n\n    value: Any\n    g: Any = None\n\n    def __getattr__(self, t):\n        return getattr(self.value, t)\n\n    def __getitem__(self, i):\n        return self.value[i]\n\n    @property\n    def v(self):\n        return self.value\n\nThen, we can implement it like so:\n\ndef mse_grad(y_pred: T, y_true: np.array):\n    diff = y_pred.squeeze() - y_true\n    y_pred_g = 2 * diff / n\n    y_pred.g = y_pred_g[:, None]\n\nContinuing on with the linear transformation layer, we‚Äôll review the mathematics then implement the code.\n\n\nLinear layer\nFor a linear layer, the gradient is derived like so:\nLet \\(L = loss(Y)\\) and \\(Y = WX+B\\) be a neural network. We want to compute the partial derivates of \\(L\\) with respect to \\(W\\), \\(B\\) and \\(X\\) to, ultimately, reduce \\(L\\).\n\nFor \\(X\\)‚Ä¶\nStarting with a single, \\(j\\)th parameter of the \\(i\\)th example, \\(x_j^i\\)\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial x_j^i} &= \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial x_j^i} \\\\\n                                  &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial x_j^i}\n\\end{align*}\n\\]\nNote that \\(\\frac{\\partial y^k}{\\partial x_j^i} = 0\\) if \\(k \\neq i\\) (i.e., the output of an example passed through the network is not a function of the input of a different example). Therefore,\n\\[\n\\begin{align*}\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial x_j^i}\n&= \\frac{\\partial L}{\\partial y^i} \\cdot \\frac{\\partial y^i}{\\partial x_j^i} \\\\\n&= \\frac{\\partial L}{\\partial y^i} \\cdot \\frac{\\partial w_j x_j^i +b}{\\partial x_j^i} \\\\\n&= \\frac{\\partial L}{\\partial y^i} w_j\n\\end{align*}\n\\]\nThe matrix of derivates for all \\(d\\) parameters (\\(j \\in \\{1, ..., d\\}\\)) and \\(N\\) examples (\\(i \\in \\{1, ..., N\\}\\)) is:\n\\[\n\\frac{\\partial L}{\\partial X} = \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y^1} w_1 & \\dots & \\frac{\\partial L}{\\partial y^1} w_d \\\\\n\\frac{\\partial L}{\\partial y^2} w_1 & \\dots & \\frac{\\partial L}{\\partial y^2} w_d \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial y^N} w_1 & \\dots & \\frac{\\partial L}{\\partial y^N} w_d \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y^1} \\\\\n\\frac{\\partial L}{\\partial y^2} \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial y^N} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1 & w_2 & \\dots & w_d\n\\end{bmatrix}\n\\]\nIn code: inp.g = out.g @ w.T\n\n\nFor \\(W\\)‚Ä¶\nStarting with a single parameter, \\(w_j\\):\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial w_j} &= \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial w_j} \\\\\n                                &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial w_j} \\\\\n                                &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial w_j x_j^k + b_j}{\\partial w_j} \\\\\n                                &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_j^k\n\\end{align*}\n\\]\nFor all parameters, \\(W\\):\n\\[\n\\frac{\\partial L}{\\partial W} = \\begin{bmatrix}\n\\frac{\\partial L}{\\partial w_{1}} \\\\\n\\frac{\\partial L}{\\partial w_{2}} \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial w_{d}}\n\\end{bmatrix} = \\begin{bmatrix}\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_1^k \\\\\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_2^k \\\\\n\\vdots \\\\\n\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} x_d^k\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y^1} x_1^1 &+ \\dots &+ \\frac{\\partial L}{\\partial y^N} x_1^N \\\\\n\\frac{\\partial L}{\\partial y^1} x_2^1 &+ \\dots &+ \\frac{\\partial L}{\\partial y^N} x_2^N \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial L}{\\partial y^1} x_d^1 &+ \\dots &+ \\frac{\\partial L}{\\partial y^N} x_d^N \\\\\n\\end{bmatrix} = \\begin{bmatrix}\nx_1^1 & \\dots & x^{N}_1 \\\\\nx_2^1 & \\dots & x^{N}_2 \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_d^1 & \\dots & x^{N}_d\n\\end{bmatrix}  \\begin{bmatrix}\n\\frac{\\partial L}{\\partial y_{1}} \\\\\n\\frac{\\partial L}{\\partial y_{2}} \\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial y_{N}} \\\\\n\\end{bmatrix} = X^T \\frac{\\partial L}{\\partial Y}\n\\]\nIn code: w.g = inp.v.T @ out.g\n\n\nFor \\(B\\)‚Ä¶\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial B} &= \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial B} \\\\\n                              &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} \\cdot \\frac{\\partial y^k}{\\partial B} \\\\\n                              &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k} (1) \\\\\n                               &= \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y^k}\n\\end{align*}\n\\]\nIn code b.g = out.g.sum(axis=0)\nYou can see that these derivation are similar to their single-variable counterparts (e.g.¬†\\(\\frac{dL}{dm} = \\frac{dL}{dy} \\cdot \\frac{dy}{dm} = \\frac{dL}{dy} \\cdot \\frac{dmx+b}{dm}=\\frac{dL}{dy}x\\)). However, the order of operations and the transposition is dictated by the matrix mathematics.\nIn code:\n\ndef lin_grad(inp, out, w, b):\n    inp.g = out.g @ w.T\n    w.g = inp.v.T @ out.g\n    b.g = out.g.sum(axis=0)\n\nMuch credit for my understanding goes to this blog bost.\n\n\n\nReLU\nFor ReLU, we pass the upstream gradient downstream for any dimensions that contributed to the upstream signal. Note that this is an element-wise operation because the layer operates only on specific elements and has no global behavior.\n\ndef relu_grad(inp, out):\n    inp.g = (inp.value &gt; 0).astype(float) * out.g\n\nPutting it all together:\n\n# \"Tensorize\" the weights, biases and outputs\ntensors = (y_pred, W1, b1, l1, W0, b0, l0, X_trn)\nty_pred, tW1, tb1, tl1, tW0, tb0, tl0, tX_trn = map(T, tensors)\nmse_grad(ty_pred, y_true)\nty_pred.g[:5]\n\narray([[-0.00049577],\n       [-0.00080577],\n       [ 0.00094763],\n       [ 0.00016918],\n       [-0.00248921]])\n\n\n\nmse_grad(ty_pred, y_true)\nlin_grad(tl1, ty_pred, tW1, tb1)\nrelu_grad(tl0, tl1)\nlin_grad(tX_trn, tl0, tW0, tb0)\n\nVerify with PyTorch\n\n# Port layers\npt_lin0 = nn.Linear(m, nh)\ndtype = pt_lin0.weight.data.dtype\npt_lin0.weight.data = torch.from_numpy(tW0.v.T).to(dtype)\npt_lin0.bias.data = torch.from_numpy(tb0.v).to(dtype)\npt_lin1 = nn.Linear(nh, 1)\npt_lin1.weight.data = torch.from_numpy(tW1.T).to(dtype)\npt_lin1.bias.data = torch.from_numpy(tb1.v).to(dtype)\n\n# Forward pass\nlogits = pt_lin0(torch.from_numpy(X_trn).to(dtype))\nlogits = F.relu(logits)\nlogits = pt_lin1(logits)\nloss = F.mse_loss(\n    logits.squeeze(),\n    torch.from_numpy(y_true).float(),\n)\n\n# Backward pass\nloss.backward()\n\nfor w, b, layer in [\n    (tW0, tb0, pt_lin0),\n    (tW1, tb1, pt_lin1),\n]:\n    assert torch.isclose(\n        torch.from_numpy(w.g.T).float(),\n        layer.weight.grad,\n        atol=1e-4,\n    ).all()\n    assert torch.isclose(\n        torch.from_numpy(b.g.T).float(),\n        layer.bias.grad,\n        atol=1e-4,\n    ).all()\n\nLet‚Äôs refactor these as classes.\n\nclass Module:\n    def __call__(self, *x):\n        self.inp = x\n        self.out = self.forward(*x)\n        if isinstance(self.out, (np.ndarray,)):\n            self.out = T(self.out)\n        return self.out\n\n\nclass ReLu(Module):\n    def forward(self, x: T):\n        return relu(x)\n\n    def backward(self):\n        (x,) = self.inp\n        relu_grad(x, self.out)\n\n\nclass Linear(Module):\n    def __init__(self, h_in, h_out):\n        self.W = T(np.random.randn(h_in, h_out))\n        self.b = T(np.zeros(h_out))\n\n    def forward(self, x):\n        return T(x @ self.W.v + self.b.v)\n\n    def backward(self):\n        (x,) = self.inp\n        lin_grad(x, self.out, self.W, self.b)\n\n\nclass MSE(Module):\n    def forward(self, y_pred, y_true):\n        return ((y_pred.squeeze() - y_true) ** 2).mean()\n\n    def backward(self):\n        y_pred, y_true = self.inp\n        mse_grad(y_pred, y_true)\n\n\nclass MLP(Module):\n    def __init__(self, layers, criterion):\n        super().__init__()\n        self.layers = layers\n        self.criterion = criterion\n\n    def forward(self, x, y_pred):\n        for l in self.layers:\n            x = l(x)\n        self.criterion(x, y_pred)\n\n    def backward(self):\n        self.criterion.backward()\n        for l in reversed(self.layers):\n            l.backward()\n\n\nmodel = MLP([Linear(m, nh), ReLu(), Linear(nh, 1)], criterion=MSE())\n\n\nmodel(tX_trn, y_true)\nmodel.backward()\n\nThis is quite a bit cleaner!\nBy the rules of FastAI, we can now use the torch.nn.Module classes which is the equivalent in PyTorch."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Adapted from:\nimport math\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\nfrom torch import tensor\nfrom torch.distributions.multivariate_normal import MultivariateNormal\ntorch.manual_seed(42);"
  },
  {
    "objectID": "clustering.html#homework",
    "href": "clustering.html#homework",
    "title": "Clustering",
    "section": "Homework",
    "text": "Homework\nImplement DBSCAN\n\nX = data.clone()\n\n\nn, _ = X.shape\n\n\neps = 2.5\nmin_points = 25\n\n\nnorms = torch.pow(X[None, :] - X[:, None], 2).sum(axis=2).sqrt()\nnorms\n\ntensor([[ 0.0000,  4.6538,  4.8366,  ..., 48.0356, 51.1620, 45.6434],\n        [ 4.6538,  0.0000,  0.6129,  ..., 43.3835, 46.5297, 40.9945],\n        [ 4.8366,  0.6129,  0.0000,  ..., 43.2585, 46.4482, 40.8819],\n        ...,\n        [48.0356, 43.3835, 43.2585,  ...,  0.0000,  4.5282,  2.5374],\n        [51.1620, 46.5297, 46.4482,  ...,  4.5282,  0.0000,  5.9866],\n        [45.6434, 40.9945, 40.8819,  ...,  2.5374,  5.9866,  0.0000]])\n\n\n\ncluster_assignment = torch.zeros(n)\ncluster_assignment\n\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n\n\n\nfrom functools import lru_cache\nfrom typing import Optional\n\n\nclass DontReassignCluster(Exception):\n    ...\n\n\n@lru_cache\ndef core_point_neighbords(point_idx) -&gt; Optional:\n    dists = norms[point_idx, :]\n    neighboring_point_idxs = dists &lt; eps\n    if neighboring_point_idxs.sum() &gt; min_points:\n        return neighboring_point_idxs\n\n\ndef assign_cluster(cluster_id, point_idx):\n    if cluster_assignment[point_idx]:\n        raise DontReassignCluster\n\n    # First, assign point to cluster\n    cluster_assignment[point_idx] = cluster_id\n\n    # Then, if it is a core point, recursively assign the neighbors\n    dists = norms[point_idx, :]\n    neighboring_point_idxs = core_point_neighbords(point_idx)\n    if neighboring_point_idxs is not None:\n        for i, is_neighbor in enumerate(neighboring_point_idxs):\n            if is_neighbor:\n                try:\n                    assign_cluster(cluster_id, i)\n                except DontReassignCluster:\n                    continue\n\n\ncurrent_cluster_id = 1\n\nfor point_idx in range(n):\n    if core_point_neighbords(point_idx) is not None:\n        try:\n            assign_cluster(current_cluster_id, point_idx)\n        except DontReassignCluster:\n            continue\n        else:\n            current_cluster_id += 1\n\n\nfig, ax = plt.subplots(figsize=(4, 4))\nfor cluster in cluster_assignment.unique():\n    ax.scatter(\n        *X[cluster_assignment == cluster, :].T,\n        label=f\"Cluster {int(cluster)}\" if cluster else \"Noise\",\n    )\nax.legend();"
  },
  {
    "objectID": "autoencoders.html",
    "href": "autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "Adapted from:\n\nhttps://youtu.be/0Hi2r4CaHvk?si=GA9KaGAnGOlS_NJO&t=3568\n\nAutoencoders learn a bottleneck representation that can be ‚Äúreversed‚Äù to reconstruct the original image.\n\nTypically, they are not used on their own but are used to produce compressed representations.\nWe‚Äôve seen how a convolutional neural network can produce a simple representation of an image: that is, the categorical probability distribution over all the fashion classes. How do reverse this process to reconstruct the original image.\nTranspose or ‚ÄúStride \\(\\frac{1}{2}\\)‚Äù convolutions work, but this notebook focuses on the nearest neighbor upsampling. This upsamples the activations from the previous layer and applies a convolutional layer to restore detail.\n\nsource\n\ndeconv\n\n deconv (c_in, c_out, ks=3, act=True)\n\nWe need to modify the fit function because the loss function is no longer of the label.\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl, tqdm_=False):\n    \"\"\"Modified fit function for reconstruction tasks\"\"\"\n    progress = tqdm if tqdm_ else lambda x: x\n    for epoch in range(epochs):\n        model.train()\n        trn_loss, trn_count = 0.0, 0\n        for xb, _ in progress(train_dl):\n            xb = to_device(xb)\n            loss = loss_func(model(xb), xb)  # üëà\n            bs, *_ = xb.shape\n            trn_loss += loss.item() * bs\n            trn_count += bs\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tst_loss, tot_acc, tst_count = 0.0, 0.0, 0\n            for xb, _ in progress(valid_dl):\n                xb = to_device(xb)\n                pred = model(xb)\n                bs, *_ = xb.shape\n                tst_count += bs\n                tst_loss += loss_func(pred, xb).item() * bs\n\n        print(\n            f\"{epoch=}: trn_loss={trn_loss / trn_count:.3f}, tst_loss={tst_loss / tst_count:.3f}\"\n        )\n\n\nsource\n\n\nget_model\n\n get_model ()\n\n\nautoencoder = get_model()\nautoencoder\n\nSequential(\n  (0): ZeroPad2d((2, 2, 2, 2))\n  (1): Sequential(\n    (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (2): Sequential(\n    (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (3): Sequential(\n    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (4): Sequential(\n    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n    (1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): ReLU()\n  )\n  (5): Sequential(\n    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n    (1): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): ReLU()\n  )\n  (6): Sequential(\n    (0): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n    (1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (7): ZeroPad2d((-2, -2, -2, -2))\n  (8): Sigmoid()\n)\n\n\n\nwith fashion_mnist() as (_, tst_dl):\n    xb, _ = next(iter(tst_dl))\n\n\nassert xb.shape == autoencoder(xb.to(def_device)).shape\n\n\nmodel = get_model()\nwith fashion_mnist() as dls:\n    opt = optim.AdamW(model.parameters(), lr=0.01)\n    fit(10, model, F.mse_loss, opt, *dls)\n\nepoch=0: trn_loss=0.052, tst_loss=0.028\nepoch=1: trn_loss=0.024, tst_loss=0.021\nepoch=2: trn_loss=0.020, tst_loss=0.019\nepoch=3: trn_loss=0.019, tst_loss=0.018\nepoch=4: trn_loss=0.018, tst_loss=0.018\nepoch=5: trn_loss=0.018, tst_loss=0.018\nepoch=6: trn_loss=0.018, tst_loss=0.018\nepoch=7: trn_loss=0.017, tst_loss=0.017\nepoch=8: trn_loss=0.017, tst_loss=0.018\nepoch=9: trn_loss=0.017, tst_loss=0.017\n\n\n\npred = model(xb.to(def_device))\nshow_images([xb[0, ...].squeeze(), pred[0, ...].squeeze()])\n\n\n\n\nThat looks‚Ä¶not great.\nAt this point, Jeremy pauses to go over building a framework to iterate on this problem more quickly. Continued in the next notebook."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SlowAI",
    "section": "",
    "text": "pip install -e .\nThis repo represents my plodding, meticulous notetaking from FastAI‚Äôs Practical Deep Learning Part II."
  }
]